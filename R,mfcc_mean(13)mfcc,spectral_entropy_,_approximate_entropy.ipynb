{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb_c7qs8sr7f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import librosa.display\n",
        "from IPython.display import Audio\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from scipy.io import wavfile\n",
        "import scipy.signal\n",
        "from PIL import Image\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import noisereduce as nr\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.signal import lfilter, butter\n",
        "#import keras_tuner as kt\n",
        "#from kerastuner import HyperModel\n",
        "#from keras_tuner import RandomSearch"
      ],
      "metadata": {
        "id": "MA-B3T0EsxHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "#from hmmlearn import hmm\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import  VotingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import lightgbm as lgb\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.datasets import make_classification\n",
        "from scipy.stats import uniform, randint\n",
        "from keras import optimizers\n",
        "from keras import losses\n"
      ],
      "metadata": {
        "id": "DsyC1p-Zs0X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "hZC3mb6rs22O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM,GRU\n",
        "from keras import regularizers\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from keras.optimizers import Adam, SGD, RMSprop, Adadelta\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.layers import SimpleRNN\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from tensorflow.keras.layers import  Bidirectional,RepeatVector, TimeDistributed\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
        "\n",
        "from keras.layers import BatchNormalization"
      ],
      "metadata": {
        "id": "u_AGFSAGs6AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d uwrfkaggler/ravdess-emotional-speech-audio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0FmZcars89i",
        "outputId": "88ae0e4f-e8a2-4c0c-cc7a-b0a177ac3fa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "Downloading ravdess-emotional-speech-audio.zip to /content\n",
            "100% 428M/429M [00:03<00:00, 122MB/s]\n",
            "100% 429M/429M [00:03<00:00, 132MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/ravdess-emotional-speech-audio.zip', 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "lR4li5BdtMe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filename: ex-03-01-06-01-02-01-12.wav ,where third therm is Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised)\n",
        "paths = []\n",
        "labels = []\n",
        "for dirname, _, filenames in os.walk('/content/audio_speech_actors_01-24'):\n",
        "    for filename in filenames:\n",
        "        paths.append(os.path.join(dirname, filename))\n",
        "        parts = filename.split('-')\n",
        "\n",
        "        if len(parts) >= 3:\n",
        "            label = parts[2]  # Get the 3rd term\n",
        "            labels.append(label)\n",
        "        else:\n",
        "            print(f\"Skipping file with unexpected format: {filename}\")\n"
      ],
      "metadata": {
        "id": "iFVJ2Kjat1te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a RAVDESS dataframe\n",
        "df1 = pd.DataFrame()\n",
        "df1['speech'] = paths\n",
        "df1['label'] = labels\n",
        "df1.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "j976bYcBt6Sb",
        "outputId": "cb8470a1-f301-4401-fcab-6da8d9ede36d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              speech label\n",
              "0  /content/audio_speech_actors_01-24/Actor_05/03...    05\n",
              "1  /content/audio_speech_actors_01-24/Actor_05/03...    07\n",
              "2  /content/audio_speech_actors_01-24/Actor_05/03...    02\n",
              "3  /content/audio_speech_actors_01-24/Actor_05/03...    04\n",
              "4  /content/audio_speech_actors_01-24/Actor_05/03...    02"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-588c1c04-dc61-4f02-9468-f28d2f680ebc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>speech</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/audio_speech_actors_01-24/Actor_05/03...</td>\n",
              "      <td>05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/audio_speech_actors_01-24/Actor_05/03...</td>\n",
              "      <td>07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/audio_speech_actors_01-24/Actor_05/03...</td>\n",
              "      <td>02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/audio_speech_actors_01-24/Actor_05/03...</td>\n",
              "      <td>04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/audio_speech_actors_01-24/Actor_05/03...</td>\n",
              "      <td>02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-588c1c04-dc61-4f02-9468-f28d2f680ebc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-588c1c04-dc61-4f02-9468-f28d2f680ebc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-588c1c04-dc61-4f02-9468-f28d2f680ebc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-38ef1290-cbc9-4d0c-868b-3be93da37ad9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-38ef1290-cbc9-4d0c-868b-3be93da37ad9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-38ef1290-cbc9-4d0c-868b-3be93da37ad9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df1",
              "summary": "{\n  \"name\": \"df1\",\n  \"rows\": 1440,\n  \"fields\": [\n    {\n      \"column\": \"speech\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1440,\n        \"samples\": [\n          \"/content/audio_speech_actors_01-24/Actor_09/03-01-06-02-02-01-09.wav\",\n          \"/content/audio_speech_actors_01-24/Actor_07/03-01-07-01-01-01-07.wav\",\n          \"/content/audio_speech_actors_01-24/Actor_16/03-01-03-01-01-02-16.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"07\",\n          \"01\",\n          \"05\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "label_count = df1['label'].value_counts()\n",
        "label_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "My6BGee5t-bk",
        "outputId": "0efbdaf0-3d30-48db-b355-3d41bb68ef8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "05    192\n",
              "07    192\n",
              "02    192\n",
              "04    192\n",
              "06    192\n",
              "08    192\n",
              "03    192\n",
              "01     96\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>05</th>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>07</th>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>02</th>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>04</th>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>06</th>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>08</th>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>03</th>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>01</th>\n",
              "      <td>96</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature extraction"
      ],
      "metadata": {
        "id": "vbr6NrQ5uw16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install antropy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iu-HG4tT0b29",
        "outputId": "ffe7e88c-39f2-4f06-c620-fd8be0c1e246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting antropy\n",
            "  Downloading antropy-0.1.6.tar.gz (17 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from antropy) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from antropy) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from antropy) (1.5.2)\n",
            "Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.10/dist-packages (from antropy) (0.60.0)\n",
            "Collecting stochastic (from antropy)\n",
            "  Downloading stochastic-0.7.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57->antropy) (0.43.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->antropy) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->antropy) (3.5.0)\n",
            "Downloading stochastic-0.7.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antropy\n",
            "  Building wheel for antropy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antropy: filename=antropy-0.1.6-py3-none-any.whl size=16879 sha256=c1d3254375978e46fbacba6c31b4014fdb5931c55a47d1ad229dcf82733b72c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/22/06/e91d7bb213c7133d5e2eb34258623e1e19928d5f05e1ee6812\n",
            "Successfully built antropy\n",
            "Installing collected packages: stochastic, antropy\n",
            "Successfully installed antropy-0.1.6 stochastic-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import entropy\n",
        "import antropy as ant"
      ],
      "metadata": {
        "id": "k62HmJvW0bzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define preprocessing parameters\n",
        "TARGET_SAMPLE_RATE = 48000   # Desired sample rate for resampling\n",
        "FRAME_SIZE = 1200            # Frame size in samples (25 ms)\n",
        "HOP_LENGTH = 600             # Hop length in samples (50% overlap)\n",
        "MFCC_N_MFCC = 13            # Number of MFCC coefficients to extract\n",
        "PAD_LENGTH = 5 * TARGET_SAMPLE_RATE"
      ],
      "metadata": {
        "id": "v3Ti33Egu51c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess audio\n",
        "def preprocess_audio(file_path, target_sr=TARGET_SAMPLE_RATE, pad_length=PAD_LENGTH):\n",
        "    # Load the audio\n",
        "    y, sr = librosa.load(file_path, sr=target_sr)\n",
        "\n",
        "    # Step 1: Silence Removal (Using librosa.effects.trim)\n",
        "    y, _ = librosa.effects.trim(y)\n",
        "\n",
        "    # Step 2: Padding (if the audio is shorter than the target length)\n",
        "    if len(y) < pad_length:\n",
        "        padding = pad_length - len(y)\n",
        "        y = np.pad(y, (0, padding), 'constant')  # Zero padding at the end\n",
        "\n",
        "    # Step 3: Normalization (Peak normalization to [-1, 1] range)\n",
        "    y = librosa.util.normalize(y)\n",
        "\n",
        "    return y, sr"
      ],
      "metadata": {
        "id": "jLRNozWfu5qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute spectral entropy based on MFCC\n",
        "def spectral_entropy(mfcc):\n",
        "    # Normalize the MFCC to form a probability distribution\n",
        "    psd = np.square(mfcc)\n",
        "    psd_norm = psd / np.sum(psd, axis=0)\n",
        "\n",
        "    # Compute the spectral entropy (Shannon entropy)\n",
        "    entropy = -np.sum(psd_norm * np.log(psd_norm + 1e-12), axis=0)\n",
        "    entropy_mean = entropy.mean()  # Mean spectral entropy over time\n",
        "    return entropy_mean\n"
      ],
      "metadata": {
        "id": "28wP-u3uu5ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute approximate entropy\n",
        "def approximate_entropy(mfcc, m=2, r=0.2):\n",
        "    def _phi(m):\n",
        "        N = mfcc.shape[1] - m + 1\n",
        "        patterns = np.array([mfcc[:, i:i+m] for i in range(N)])\n",
        "        distances = np.sum(np.abs(patterns[:, np.newaxis] - patterns[np.newaxis, :]), axis=-1)\n",
        "        Cm = np.sum(np.max(distances, axis=2) <= r, axis=1) / N\n",
        "        return np.sum(np.log(Cm + 1e-12)) / N\n",
        "\n",
        "    # Approximate entropy calculation\n",
        "    return abs(_phi(m+1) - _phi(m))"
      ],
      "metadata": {
        "id": "Zyr38T6-u5bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract MFCC mean, spectral entropy, and approximate entropy\n",
        "def extract_features(audio, sr, frame_size=FRAME_SIZE, hop_length=HOP_LENGTH, n_mfcc=MFCC_N_MFCC):\n",
        "    # Extract MFCC features\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)\n",
        "\n",
        "    # Compute the mean of each MFCC coefficient over time\n",
        "    mfcc_means = mfcc.mean(axis=1)\n",
        "\n",
        "    # Compute spectral entropy using MFCC\n",
        "    spec_entropy = spectral_entropy(mfcc)\n",
        "\n",
        "    # Compute approximate entropy using MFCC\n",
        "    approx_entropy = approximate_entropy(mfcc)\n",
        "\n",
        "    return mfcc_means, spec_entropy, approx_entropy"
      ],
      "metadata": {
        "id": "unmVbCzEu5ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize list to store processed data\n",
        "processed_data = []\n",
        "\n",
        "# Assuming df1 is already defined and contains the audio paths and labels\n",
        "for index, row in df1.iterrows():\n",
        "    audio_path = row['speech']\n",
        "    label = row['label']\n",
        "\n",
        "    # Preprocess the audio\n",
        "    y, sr = preprocess_audio(audio_path)\n",
        "\n",
        "    # Extract features\n",
        "    mfcc_means, spec_entropy, approx_entropy = extract_features(y, sr)\n",
        "\n",
        "    # Combine all features into a single array\n",
        "    features = np.hstack([mfcc_means, spec_entropy, approx_entropy])\n",
        "\n",
        "    # Append the features and label to processed_data\n",
        "    processed_data.append((features, label))"
      ],
      "metadata": {
        "id": "6U1ww0VBvSWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert processed_data to DataFrame\n",
        "features_array, labels = zip(*processed_data)  # Unzip features and labels\n",
        "features_df = pd.DataFrame(features_array)  # Create DataFrame for features\n",
        "features_df['label'] = labels  # Add labels as a new column"
      ],
      "metadata": {
        "id": "vQKft3AkqZWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=features_df\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "YrgGuOa0yqZB",
        "outputId": "780faa6e-7971-4dce-f632-04efef4bc844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0          1          2          3          4          5  \\\n",
              "0 -482.506104  59.905594  -6.821089   2.734158   1.233579   8.958209   \n",
              "1 -434.612213  50.399967   0.631297  16.207701   2.889047   6.601787   \n",
              "2 -422.564453  66.020157   9.411767  19.945904  10.494566   7.528195   \n",
              "3 -441.973785  45.318970   8.326529  12.036758   5.382025   9.978813   \n",
              "4 -429.680267  63.371269  11.470847  25.317907  10.043150  10.997593   \n",
              "\n",
              "           6         7         8         9        10        11        12  \\\n",
              "0 -11.159545 -2.004997 -1.493389 -2.094221 -4.677904 -2.987441  0.164259   \n",
              "1  -5.060184  2.132876  0.980047  1.502715  1.947337  1.624204  4.511497   \n",
              "2   1.195160  3.965173  1.304804  0.231898  4.683016  1.790579  1.876973   \n",
              "3   0.852225  4.016699 -0.788007 -2.175442 -0.144637  3.107192  2.169956   \n",
              "4   0.675366  4.491855  0.114406 -2.783764  5.472361  4.758094  3.971247   \n",
              "\n",
              "         13        14 label  \n",
              "0  0.346328  0.030019    05  \n",
              "1  0.307142  0.052273    07  \n",
              "2  0.350052  0.008778    02  \n",
              "3  0.282367  0.022878    04  \n",
              "4  0.357398  0.008628    02  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c1c52d96-705a-4329-9e13-4151da22502d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-482.506104</td>\n",
              "      <td>59.905594</td>\n",
              "      <td>-6.821089</td>\n",
              "      <td>2.734158</td>\n",
              "      <td>1.233579</td>\n",
              "      <td>8.958209</td>\n",
              "      <td>-11.159545</td>\n",
              "      <td>-2.004997</td>\n",
              "      <td>-1.493389</td>\n",
              "      <td>-2.094221</td>\n",
              "      <td>-4.677904</td>\n",
              "      <td>-2.987441</td>\n",
              "      <td>0.164259</td>\n",
              "      <td>0.346328</td>\n",
              "      <td>0.030019</td>\n",
              "      <td>05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-434.612213</td>\n",
              "      <td>50.399967</td>\n",
              "      <td>0.631297</td>\n",
              "      <td>16.207701</td>\n",
              "      <td>2.889047</td>\n",
              "      <td>6.601787</td>\n",
              "      <td>-5.060184</td>\n",
              "      <td>2.132876</td>\n",
              "      <td>0.980047</td>\n",
              "      <td>1.502715</td>\n",
              "      <td>1.947337</td>\n",
              "      <td>1.624204</td>\n",
              "      <td>4.511497</td>\n",
              "      <td>0.307142</td>\n",
              "      <td>0.052273</td>\n",
              "      <td>07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-422.564453</td>\n",
              "      <td>66.020157</td>\n",
              "      <td>9.411767</td>\n",
              "      <td>19.945904</td>\n",
              "      <td>10.494566</td>\n",
              "      <td>7.528195</td>\n",
              "      <td>1.195160</td>\n",
              "      <td>3.965173</td>\n",
              "      <td>1.304804</td>\n",
              "      <td>0.231898</td>\n",
              "      <td>4.683016</td>\n",
              "      <td>1.790579</td>\n",
              "      <td>1.876973</td>\n",
              "      <td>0.350052</td>\n",
              "      <td>0.008778</td>\n",
              "      <td>02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-441.973785</td>\n",
              "      <td>45.318970</td>\n",
              "      <td>8.326529</td>\n",
              "      <td>12.036758</td>\n",
              "      <td>5.382025</td>\n",
              "      <td>9.978813</td>\n",
              "      <td>0.852225</td>\n",
              "      <td>4.016699</td>\n",
              "      <td>-0.788007</td>\n",
              "      <td>-2.175442</td>\n",
              "      <td>-0.144637</td>\n",
              "      <td>3.107192</td>\n",
              "      <td>2.169956</td>\n",
              "      <td>0.282367</td>\n",
              "      <td>0.022878</td>\n",
              "      <td>04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-429.680267</td>\n",
              "      <td>63.371269</td>\n",
              "      <td>11.470847</td>\n",
              "      <td>25.317907</td>\n",
              "      <td>10.043150</td>\n",
              "      <td>10.997593</td>\n",
              "      <td>0.675366</td>\n",
              "      <td>4.491855</td>\n",
              "      <td>0.114406</td>\n",
              "      <td>-2.783764</td>\n",
              "      <td>5.472361</td>\n",
              "      <td>4.758094</td>\n",
              "      <td>3.971247</td>\n",
              "      <td>0.357398</td>\n",
              "      <td>0.008628</td>\n",
              "      <td>02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1c52d96-705a-4329-9e13-4151da22502d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c1c52d96-705a-4329-9e13-4151da22502d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c1c52d96-705a-4329-9e13-4151da22502d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a53833e1-6aff-4294-a585-fb594f6bea61\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a53833e1-6aff-4294-a585-fb594f6bea61')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a53833e1-6aff-4294-a585-fb594f6bea61 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1440,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29.9704508011464,\n        \"min\": -517.4265747070312,\n        \"max\": -338.84454345703125,\n        \"num_unique_values\": 1439,\n        \"samples\": [\n          -452.7906799316406,\n          -460.79974365234375,\n          -423.9035949707031\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11.777831936440167,\n        \"min\": 14.11308765411377,\n        \"max\": 94.7603759765625,\n        \"num_unique_values\": 1439,\n        \"samples\": [\n          41.23746871948242,\n          48.74565124511719,\n          41.83320999145508\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.95513600966588,\n        \"min\": -37.774864196777344,\n        \"max\": 14.865141868591309,\n        \"num_unique_values\": 1439,\n        \"samples\": [\n          -0.13256683945655823,\n          1.50631582736969,\n          -5.372255802154541\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.120743040670965,\n        \"min\": -13.696608543395996,\n        \"max\": 30.477190017700195,\n        \"num_unique_values\": 1438,\n        \"samples\": [\n          19.467500686645508,\n          5.722509384155273,\n          12.610212326049805\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 4,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.157607105584111,\n        \"min\": -17.984195709228516,\n        \"max\": 14.542010307312012,\n        \"num_unique_values\": 1439,\n        \"samples\": [\n          3.28686261177063,\n          1.2198846340179443,\n          1.5703812837600708\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 5,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.341440207695377,\n        \"min\": -10.37924575805664,\n        \"max\": 22.38988494873047,\n        \"num_unique_values\": 1439,\n        \"samples\": [\n          10.188806533813477,\n          9.115045547485352,\n          3.097961902618408\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 6,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.7022428696347935,\n        \"min\": -24.288862228393555,\n        \"max\": 7.472330093383789,\n        \"num_unique_values\": 1439,\n        \"samples\": [\n          -4.698643207550049,\n          -8.083755493164062,\n          -6.817258834838867\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 7,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.8641042383496407,\n        \"min\": -12.683357238769531,\n        \"max\": 12.134343147277832,\n        \"num_unique_values\": 1439,\n        \"samples\": [\n          -1.233416199684143,\n          -0.5460568070411682,\n          -2.213940382003784\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 8,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.80545011052804,\n        \"min\": -18.15420913696289,\n        \"max\": 4.418431282043457,\n        \"num_unique_values\": 1439,\n        \"samples\": [\n          -2.4649546146392822,\n          -3.369614839553833,\n          -6.138519763946533\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 9,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.326898061317426,\n        \"min\": -18.26426887512207,\n        \"max\": 5.649686336517334,\n        \"num_unique_values\": 1439,\n        \"samples\": [\n          0.7506794333457947,\n          -0.9601463079452515,\n          -2.4562675952911377\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 10,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.9059208958592944,\n        \"min\": -14.339527130126953,\n        \"max\": 6.228267192840576,\n        \"num_unique_values\": 1439,\n        \"samples\": [\n          -5.108475685119629,\n          0.00114066107198596,\n          -1.2700647115707397\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 11,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.4703749185584933,\n        \"min\": -11.83915901184082,\n        \"max\": 13.496124267578125,\n        \"num_unique_values\": 1438,\n        \"samples\": [\n          1.8568713665008545,\n          1.7780133485794067,\n          -1.1491965055465698\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 12,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.957627927853475,\n        \"min\": -10.734885215759277,\n        \"max\": 9.86028003692627,\n        \"num_unique_values\": 1439,\n        \"samples\": [\n          -0.1120990589261055,\n          -0.5190662741661072,\n          -4.064906597137451\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 13,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06196447356304857,\n        \"min\": 0.19630932807922363,\n        \"max\": 0.6103787422180176,\n        \"num_unique_values\": 1439,\n        \"samples\": [\n          0.2495327740907669,\n          0.31764450669288635,\n          0.297696590423584\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 14,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025768676315155,\n        \"min\": 0.0025031302171178993,\n        \"max\": 0.1955623364899961,\n        \"num_unique_values\": 862,\n        \"samples\": [\n          0.022859185361420842,\n          0.026111640493317267,\n          0.008646897072578597\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"07\",\n          \"01\",\n          \"05\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=df.drop(columns=['label'])"
      ],
      "metadata": {
        "id": "p5TzvhSRyvoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yiw0nNYdy7lc",
        "outputId": "baa35838-f726-4b8b-e386-e7a0143a3094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1440, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=df['label']"
      ],
      "metadata": {
        "id": "wJb4xnmSy9Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x = scaler.fit_transform(x)\n"
      ],
      "metadata": {
        "id": "zq4KfErgzAIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vnQAwWj2Tt6",
        "outputId": "6d354f64-11c5-49f7-c509-6bfb12c9057b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc = OneHotEncoder()\n",
        "y = enc.fit_transform(df[['label']])"
      ],
      "metadata": {
        "id": "WWWCkWKZ1T18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = y.toarray()"
      ],
      "metadata": {
        "id": "i3pZOjYY1ljo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=1)"
      ],
      "metadata": {
        "id": "r7vljBX21n03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_sparse = tf.argmax(y_train, axis=1)\n",
        "y_test_sparse = tf.argmax(y_test, axis=1)"
      ],
      "metadata": {
        "id": "MezDuulm11Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Decision Tree Classifier\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "dt_model.fit(x_train, y_train_sparse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "By2KnIe62g2c",
        "outputId": "e41d127e-6a1d-4694-a82e-bfef33ebefa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;DecisionTreeClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(random_state=42)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "y_pred = dt_model.predict(x_test)\n"
      ],
      "metadata": {
        "id": "5kvAYcaR2lvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test_sparse, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_sparse, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_sparse, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnS3xeMd2sM7",
        "outputId": "0bbced23-51f0-40c5-a7f1-2e73bee549f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.32\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.13      0.22      0.17        18\n",
            "           1       0.39      0.44      0.42        34\n",
            "           2       0.30      0.29      0.29        38\n",
            "           3       0.27      0.23      0.25        44\n",
            "           4       0.39      0.28      0.32        40\n",
            "           5       0.28      0.30      0.29        33\n",
            "           6       0.42      0.32      0.36        44\n",
            "           7       0.33      0.43      0.37        37\n",
            "\n",
            "    accuracy                           0.32       288\n",
            "   macro avg       0.31      0.31      0.31       288\n",
            "weighted avg       0.33      0.32      0.32       288\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 4  3  1  2  2  2  3  1]\n",
            " [ 6 15  1  6  0  1  1  4]\n",
            " [ 1  2 11  7  1  6  3  7]\n",
            " [ 8  7  5 10  2  2  6  4]\n",
            " [ 4  0  2  4 11  8  5  6]\n",
            " [ 3  2  7  2  5 10  1  3]\n",
            " [ 1  6  4  4  4  3 14  8]\n",
            " [ 3  3  6  2  3  4  0 16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "rf_model.fit(x_train, y_train_sparse)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = rf_model.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test_sparse, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HqrZM2F23Ui",
        "outputId": "697110ff-f69d-4d55-d757-dde443e6cb6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LightGBM Classifier\n",
        "lgbm_model = lgb.LGBMClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "lgbm_model.fit(x_train, y_train_sparse)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = lgbm_model.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test_sparse, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_sparse, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_sparse, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t_BmqPQ3BuV",
        "outputId": "22c58a49-bc18-459f-e9c4-6232eb0224e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000929 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 1152, number of used features: 15\n",
            "[LightGBM] [Info] Start training from score -2.692546\n",
            "[LightGBM] [Info] Start training from score -1.986660\n",
            "[LightGBM] [Info] Start training from score -2.012302\n",
            "[LightGBM] [Info] Start training from score -2.052043\n",
            "[LightGBM] [Info] Start training from score -2.025374\n",
            "[LightGBM] [Info] Start training from score -1.980351\n",
            "[LightGBM] [Info] Start training from score -2.052043\n",
            "[LightGBM] [Info] Start training from score -2.005830\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Accuracy: 0.52\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.28      0.31        18\n",
            "           1       0.54      0.59      0.56        34\n",
            "           2       0.61      0.50      0.55        38\n",
            "           3       0.47      0.50      0.48        44\n",
            "           4       0.54      0.53      0.53        40\n",
            "           5       0.43      0.61      0.51        33\n",
            "           6       0.66      0.48      0.55        44\n",
            "           7       0.52      0.59      0.56        37\n",
            "\n",
            "    accuracy                           0.52       288\n",
            "   macro avg       0.52      0.51      0.51       288\n",
            "weighted avg       0.53      0.52      0.52       288\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 5  3  0  3  0  3  0  4]\n",
            " [ 3 20  2  5  0  2  1  1]\n",
            " [ 0  1 19  7  2  2  4  3]\n",
            " [ 2  5  3 22  3  6  3  0]\n",
            " [ 1  3  0  3 21  4  2  6]\n",
            " [ 0  1  1  4  2 20  1  4]\n",
            " [ 2  4  2  1  8  4 21  2]\n",
            " [ 1  0  4  2  3  5  0 22]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Support Vector Classifier\n",
        "svm_model = SVC(kernel='poly', random_state=42)"
      ],
      "metadata": {
        "id": "Jb5m5AXy3HKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model on the training data\n",
        "svm_model.fit(x_train, y_train_sparse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "TDOEc1oG3N9o",
        "outputId": "a05d03b2-841c-4d27-f0fd-6f059d5a1e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(kernel='poly', random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-2 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-2 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-2 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-2 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-2 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;poly&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(kernel=&#x27;poly&#x27;, random_state=42)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "y_pred = svm_model.predict(x_test)"
      ],
      "metadata": {
        "id": "YXPHzXgJ3QXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test_sparse, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_sparse, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_sparse, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6InntJA3SZD",
        "outputId": "62dc6b17-ee3c-45e1-a1da-40f663f7fc6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.40\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.06      0.11        18\n",
            "           1       0.50      0.41      0.45        34\n",
            "           2       0.52      0.42      0.46        38\n",
            "           3       0.30      0.18      0.23        44\n",
            "           4       0.65      0.33      0.43        40\n",
            "           5       0.52      0.33      0.41        33\n",
            "           6       0.62      0.45      0.53        44\n",
            "           7       0.25      0.86      0.39        37\n",
            "\n",
            "    accuracy                           0.40       288\n",
            "   macro avg       0.55      0.38      0.38       288\n",
            "weighted avg       0.51      0.40      0.39       288\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 1  2  1  1  0  1  0 12]\n",
            " [ 0 14  1  5  0  0  4 10]\n",
            " [ 0  1 16  6  0  3  0 12]\n",
            " [ 0  6  2  8  3  1  2 22]\n",
            " [ 0  3  4  2 13  1  4 13]\n",
            " [ 0  1  3  3  2 11  0 13]\n",
            " [ 0  1  1  2  2  4 20 14]\n",
            " [ 0  0  3  0  0  0  2 32]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MLP models**"
      ],
      "metadata": {
        "id": "V3nhsLx_JpJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Multi Layer Perceptropn\n",
        "# Create MLP model\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "model.add(Dense(52, input_dim=13, activation='elu'))  # Adjust input shape according to your dataset\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Dense(130,  activation='elu'))  # Adjust input shape according to your dataset\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(130, activation='elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(130, activation='elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(52, activation='elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(8, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "4OB5Wmd23UrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(x_train, y_train_sparse, batch_size=50, epochs=100, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "9SCISPeg3f_-",
        "outputId": "ea9358bd-ce54-4567-cc8c-8c56598a66ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 13, but received input with shape (None, 15)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 15), dtype=float32)\n  • training=True\n  • mask=None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-824df2743ade>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_sparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 }:\n\u001b[0;32m--> 227\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    228\u001b[0m                         \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                         \u001b[0;34mf\"incompatible with the layer: expected axis {axis} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 13, but received input with shape (None, 15)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 15), dtype=float32)\n  • training=True\n  • mask=None"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP parameter tunning\n"
      ],
      "metadata": {
        "id": "GSJKcIbu3xH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDFSr6Ya4mY1",
        "outputId": "72eb2cc2-a20a-4013-a2d5-8e26ddcf00a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt\n",
        "from kerastuner import HyperModel\n",
        "from keras_tuner import RandomSearch"
      ],
      "metadata": {
        "id": "um8vwlDX40SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1st"
      ],
      "metadata": {
        "id": "1rVxqXKLJ1RX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tNOYTbiDJ1N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the HyperModel class:1\n",
        "class MLPHyperModel(HyperModel):\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "\n",
        "        # Number of hidden layers\n",
        "        for i in range(hp.Int('num_layers', 1, 8)):\n",
        "            model.add(Dense(\n",
        "                units=hp.Int(f'units_{i}', min_value=52, max_value=390, step=13),\n",
        "                activation=hp.Choice(f'activation_{i}', ['relu', 'elu','tanh','softmax']),\n",
        "                input_dim=13 if i == 0 else None\n",
        "            ))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dropout(rate=hp.Float(f'dropout_{i}', 0.1, 0.5, step=0.1)))\n",
        "\n",
        "        # Output layer\n",
        "        model.add(Dense(8, activation='softmax'))\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-1, sampling='log')),\n",
        "            loss=hp.Choice('loss', ['sparse_categorical_crossentropy', 'categorical_crossentropy']),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model\n"
      ],
      "metadata": {
        "id": "V6PmEmGN3kHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tuner\n",
        "tuner = RandomSearch(\n",
        "    MLPHyperModel(),\n",
        "    objective='val_accuracy',\n",
        "    max_trials=3,\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='mlp_tuning'\n",
        ")"
      ],
      "metadata": {
        "id": "r_NOiKGr4YKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(x_train, y_train_sparse, epochs=100,batch_size=64, validation_data=(x_test, y_test_sparse))\n",
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKiduPnc5AO_",
        "outputId": "554eb566-f7d0-45c5-ae44-745729dec2c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3 Complete [00h 00m 34s]\n",
            "val_accuracy: 0.6805555820465088\n",
            "\n",
            "Best val_accuracy So Far: 0.6805555820465088\n",
            "Total elapsed time: 00h 01m 21s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the best hyperparameters\n",
        "best_hyperparameters = tuner.get_best_hyperparameters()[0]"
      ],
      "metadata": {
        "id": "lt69i_KQ7n3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "for hp_name in best_hyperparameters.values.keys():\n",
        "    print(f\"{hp_name}: {best_hyperparameters.get(hp_name)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30fnJUMD5Rlj",
        "outputId": "413827af-8019-41a5-b931-58146b24b6b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "num_layers: 3\n",
            "units_0: 364\n",
            "activation_0: relu\n",
            "dropout_0: 0.4\n",
            "learning_rate: 0.006019793254027166\n",
            "loss: sparse_categorical_crossentropy\n",
            "units_1: 377\n",
            "activation_1: elu\n",
            "dropout_1: 0.1\n",
            "units_2: 91\n",
            "activation_2: relu\n",
            "dropout_2: 0.1\n",
            "units_3: 221\n",
            "activation_3: relu\n",
            "dropout_3: 0.5\n",
            "units_4: 286\n",
            "activation_4: tanh\n",
            "dropout_4: 0.5\n",
            "units_5: 286\n",
            "activation_5: softmax\n",
            "dropout_5: 0.1\n",
            "units_6: 156\n",
            "activation_6: elu\n",
            "dropout_6: 0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    model = Sequential()\n",
        "\n",
        "    # Layer 0\n",
        "    model.add(Dense(364, activation='relu', input_shape=(13,)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # Layer 1\n",
        "    model.add(Dense(377, activation='elu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    # Layer 2\n",
        "    model.add(Dense(91, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    # Layer 3\n",
        "    model.add(Dense(221, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # Layer 4\n",
        "    model.add(Dense(286, activation='tanh'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # Layer 5\n",
        "    model.add(Dense(286, activation='softmax'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    # Layer 6 (Output layer)\n",
        "    model.add(Dense(156, activation='elu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = RMSprop(learning_rate=0.006019793254027166)\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "PsRUCKhSOMmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=200,batch_size=32, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kN-mLWwIOMi2",
        "outputId": "b90569fd-90cc-4459-9bf0-b87effb45aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 51ms/step - accuracy: 0.0189 - loss: 15.0847 - val_accuracy: 0.1319 - val_loss: 2.1610\n",
            "Epoch 2/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0933 - loss: 12.1662 - val_accuracy: 0.1319 - val_loss: 2.2524\n",
            "Epoch 3/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1262 - loss: 10.6082 - val_accuracy: 0.1562 - val_loss: 2.2592\n",
            "Epoch 4/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0857 - loss: 10.1090 - val_accuracy: 0.1597 - val_loss: 8.0742\n",
            "Epoch 5/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0916 - loss: 8.9078 - val_accuracy: 0.1736 - val_loss: 4.5027\n",
            "Epoch 6/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1129 - loss: 7.6706 - val_accuracy: 0.1354 - val_loss: 10.6071\n",
            "Epoch 7/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1225 - loss: 7.0562 - val_accuracy: 0.1458 - val_loss: 6.6042\n",
            "Epoch 8/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1011 - loss: 6.3886 - val_accuracy: 0.1424 - val_loss: 7.9319\n",
            "Epoch 9/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1250 - loss: 6.0076 - val_accuracy: 0.1458 - val_loss: 6.3512\n",
            "Epoch 10/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1172 - loss: 6.1875 - val_accuracy: 0.1389 - val_loss: 5.5755\n",
            "Epoch 11/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1322 - loss: 6.0008 - val_accuracy: 0.1285 - val_loss: 5.0499\n",
            "Epoch 12/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1249 - loss: 5.6871 - val_accuracy: 0.1250 - val_loss: 5.1353\n",
            "Epoch 13/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1102 - loss: 5.2428 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 14/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0967 - loss: 5.1682 - val_accuracy: 0.1285 - val_loss: 5.0499\n",
            "Epoch 15/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1024 - loss: 5.0983 - val_accuracy: 0.1250 - val_loss: 5.0499\n",
            "Epoch 16/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1269 - loss: 5.0687 - val_accuracy: 0.1285 - val_loss: 5.0499\n",
            "Epoch 17/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1035 - loss: 5.0499 - val_accuracy: 0.1285 - val_loss: 5.0499\n",
            "Epoch 18/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0936 - loss: 5.0576 - val_accuracy: 0.1354 - val_loss: 5.0499\n",
            "Epoch 19/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1028 - loss: 5.0494 - val_accuracy: 0.1458 - val_loss: 5.0499\n",
            "Epoch 20/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1150 - loss: 5.0499 - val_accuracy: 0.1597 - val_loss: 5.0499\n",
            "Epoch 21/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0868 - loss: 5.0499 - val_accuracy: 0.1424 - val_loss: 5.0499\n",
            "Epoch 22/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1066 - loss: 5.0499 - val_accuracy: 0.1424 - val_loss: 5.0499\n",
            "Epoch 23/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1080 - loss: 5.0487 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 24/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0968 - loss: 5.0477 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 25/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1209 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 26/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1266 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 27/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1022 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 28/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0963 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 29/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1008 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 30/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1271 - loss: 5.0642 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 31/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0911 - loss: 5.0494 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 32/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1206 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 33/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1226 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 34/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0856 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 35/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1116 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 36/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1233 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 37/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1037 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 38/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0955 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 39/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0981 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 40/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1095 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 41/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1084 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 42/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1012 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 43/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1129 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 44/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1143 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 45/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1049 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 46/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1079 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 47/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1066 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 48/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1015 - loss: 5.0514 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 49/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1047 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 50/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1000 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 51/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1073 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 52/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1136 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 53/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1140 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 54/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1225 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 55/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1014 - loss: 5.0638 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 56/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1066 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 57/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1097 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 58/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0877 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 59/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1108 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 60/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1022 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 61/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1190 - loss: 5.0538 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 62/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0954 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 63/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1267 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 64/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1205 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 65/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0924 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 66/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1218 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 67/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0880 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 68/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1107 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 69/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1159 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 70/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0834 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 71/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1147 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 72/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1254 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 73/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0972 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 74/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1050 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 75/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1055 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 76/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1070 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 77/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0949 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 78/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1076 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 79/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1004 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 80/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1283 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 81/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1068 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 82/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1169 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 83/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0952 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 84/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1185 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 85/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1274 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 86/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0940 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 87/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0924 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 88/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0985 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 89/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1022 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 90/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1184 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 91/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1134 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 92/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1207 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 93/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1077 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 94/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1099 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 95/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1117 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 96/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1059 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 97/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1094 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 98/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1132 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 99/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1082 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 100/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1102 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 101/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1054 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 102/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1161 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 103/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1032 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 104/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1335 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 105/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1020 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 106/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1207 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 107/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0995 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 108/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0952 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 109/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1186 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 110/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1131 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 111/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1139 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 112/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0981 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 113/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0975 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 114/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1248 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 115/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1058 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 116/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1135 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 117/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1242 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 118/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1016 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 119/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0955 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 120/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1154 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 121/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1226 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 122/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0918 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 123/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0884 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 124/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0994 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 125/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1077 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 126/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1214 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 127/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1143 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 128/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1135 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 129/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1037 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 130/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1116 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 131/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0907 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 132/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1295 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 133/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1038 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 134/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0872 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 135/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0980 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 136/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1000 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 137/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1057 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 138/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1083 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 139/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0998 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 140/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1108 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 141/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0876 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 142/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0962 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 143/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1075 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 144/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0993 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 145/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1143 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 146/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1216 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 147/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1089 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 148/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1124 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 149/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1045 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 150/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0877 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 151/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0966 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 152/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1102 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 153/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1240 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 154/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1220 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 155/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1206 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 156/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0838 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 157/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1084 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 158/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1140 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 159/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1230 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 160/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0735 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 161/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0946 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 162/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0977 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 163/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1221 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 164/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0924 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 165/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0943 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 166/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1002 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 167/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1096 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 168/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1151 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 169/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1238 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 170/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1277 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 171/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0921 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 172/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1033 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 173/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1060 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 174/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1121 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 175/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1216 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 176/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0756 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 177/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1173 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 178/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1131 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 179/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0940 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 180/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1092 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 181/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1318 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 182/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1015 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 183/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0999 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 184/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0967 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 185/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1022 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 186/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0941 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 187/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0979 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 188/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1172 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 189/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1259 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 190/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1257 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 191/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0959 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 192/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1011 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 193/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1049 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 194/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0972 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 195/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1171 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 196/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0891 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 197/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1041 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 198/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1266 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 199/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1011 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n",
            "Epoch 200/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1001 - loss: 5.0499 - val_accuracy: 0.1319 - val_loss: 5.0499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Dense(1024,activation='elu',input_dim=13))\n",
        "model.add(Dense(1024,activation='elu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(512,activation='elu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512,activation='elu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256,activation='elu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(8,activation='softmax'))\n",
        "# Compile the model\n",
        "optimizer = RMSprop(learning_rate=0.006019793254027166)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "hjhHTphqOMRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=200,batch_size=32, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fnon9XvFSFID",
        "outputId": "5d62ba92-37ae-4dc1-84bd-9e37a46176db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.1689 - loss: 38.6983 - val_accuracy: 0.2604 - val_loss: 2.0617\n",
            "Epoch 2/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1900 - loss: 2.5422 - val_accuracy: 0.1354 - val_loss: 2.2483\n",
            "Epoch 3/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1550 - loss: 2.9964 - val_accuracy: 0.1354 - val_loss: 2.3363\n",
            "Epoch 4/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1378 - loss: 2.6204 - val_accuracy: 0.1562 - val_loss: 2.2311\n",
            "Epoch 5/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1116 - loss: 2.7879 - val_accuracy: 0.1562 - val_loss: 2.4742\n",
            "Epoch 6/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1443 - loss: 2.6668 - val_accuracy: 0.1528 - val_loss: 2.2475\n",
            "Epoch 7/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1455 - loss: 3.9503 - val_accuracy: 0.1597 - val_loss: 2.6152\n",
            "Epoch 8/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1123 - loss: 2.7014 - val_accuracy: 0.1562 - val_loss: 2.3645\n",
            "Epoch 9/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1199 - loss: 2.5622 - val_accuracy: 0.0486 - val_loss: 2.5875\n",
            "Epoch 10/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1303 - loss: 2.5681 - val_accuracy: 0.1354 - val_loss: 2.3854\n",
            "Epoch 11/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1378 - loss: 2.4751 - val_accuracy: 0.1354 - val_loss: 2.4938\n",
            "Epoch 12/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1369 - loss: 2.5251 - val_accuracy: 0.0486 - val_loss: 2.2266\n",
            "Epoch 13/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1353 - loss: 2.4903 - val_accuracy: 0.1319 - val_loss: 2.1881\n",
            "Epoch 14/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1285 - loss: 2.5161 - val_accuracy: 0.1424 - val_loss: 2.2222\n",
            "Epoch 15/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1169 - loss: 2.4534 - val_accuracy: 0.1528 - val_loss: 2.1503\n",
            "Epoch 16/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1274 - loss: 2.5808 - val_accuracy: 0.1528 - val_loss: 2.3272\n",
            "Epoch 17/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1502 - loss: 2.7186 - val_accuracy: 0.1528 - val_loss: 2.5332\n",
            "Epoch 18/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1388 - loss: 3.0411 - val_accuracy: 0.1319 - val_loss: 2.4142\n",
            "Epoch 19/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1058 - loss: 2.5187 - val_accuracy: 0.1562 - val_loss: 2.1630\n",
            "Epoch 20/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1281 - loss: 2.4513 - val_accuracy: 0.1424 - val_loss: 2.5650\n",
            "Epoch 21/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1335 - loss: 2.5024 - val_accuracy: 0.1528 - val_loss: 2.2391\n",
            "Epoch 22/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1202 - loss: 2.5549 - val_accuracy: 0.1354 - val_loss: 2.3146\n",
            "Epoch 23/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1375 - loss: 2.5274 - val_accuracy: 0.1354 - val_loss: 2.2686\n",
            "Epoch 24/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1290 - loss: 2.4264 - val_accuracy: 0.1354 - val_loss: 2.5175\n",
            "Epoch 25/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1021 - loss: 2.5987 - val_accuracy: 0.1562 - val_loss: 2.2128\n",
            "Epoch 26/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1315 - loss: 2.5059 - val_accuracy: 0.1285 - val_loss: 2.1600\n",
            "Epoch 27/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1048 - loss: 2.4451 - val_accuracy: 0.1042 - val_loss: 2.6367\n",
            "Epoch 28/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1269 - loss: 2.4410 - val_accuracy: 0.1424 - val_loss: 2.5833\n",
            "Epoch 29/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1131 - loss: 2.4936 - val_accuracy: 0.1042 - val_loss: 2.2335\n",
            "Epoch 30/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1171 - loss: 2.4545 - val_accuracy: 0.1319 - val_loss: 2.4631\n",
            "Epoch 31/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1000 - loss: 2.5508 - val_accuracy: 0.1562 - val_loss: 2.2391\n",
            "Epoch 32/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1397 - loss: 2.4373 - val_accuracy: 0.1528 - val_loss: 2.2192\n",
            "Epoch 33/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1330 - loss: 2.4969 - val_accuracy: 0.1319 - val_loss: 2.4555\n",
            "Epoch 34/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1302 - loss: 2.4980 - val_accuracy: 0.1042 - val_loss: 3.1254\n",
            "Epoch 35/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1117 - loss: 2.7143 - val_accuracy: 0.1285 - val_loss: 2.2728\n",
            "Epoch 36/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1233 - loss: 2.4025 - val_accuracy: 0.1042 - val_loss: 2.3395\n",
            "Epoch 37/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1150 - loss: 2.4142 - val_accuracy: 0.1528 - val_loss: 2.2603\n",
            "Epoch 38/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1254 - loss: 2.3999 - val_accuracy: 0.1285 - val_loss: 2.1511\n",
            "Epoch 39/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1083 - loss: 2.8028 - val_accuracy: 0.1285 - val_loss: 2.3120\n",
            "Epoch 40/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1344 - loss: 2.3691 - val_accuracy: 0.1528 - val_loss: 2.5064\n",
            "Epoch 41/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1647 - loss: 2.4590 - val_accuracy: 0.0486 - val_loss: 2.4445\n",
            "Epoch 42/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1300 - loss: 2.4223 - val_accuracy: 0.1424 - val_loss: 2.5288\n",
            "Epoch 43/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1451 - loss: 2.3856 - val_accuracy: 0.1042 - val_loss: 2.7676\n",
            "Epoch 44/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1587 - loss: 2.4198 - val_accuracy: 0.0486 - val_loss: 2.4369\n",
            "Epoch 45/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1274 - loss: 2.4858 - val_accuracy: 0.1319 - val_loss: 2.7948\n",
            "Epoch 46/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1313 - loss: 2.5100 - val_accuracy: 0.1354 - val_loss: 2.3285\n",
            "Epoch 47/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1291 - loss: 2.4465 - val_accuracy: 0.1528 - val_loss: 2.3226\n",
            "Epoch 48/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1300 - loss: 2.4363 - val_accuracy: 0.1285 - val_loss: 2.5333\n",
            "Epoch 49/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1407 - loss: 2.4895 - val_accuracy: 0.1354 - val_loss: 2.6273\n",
            "Epoch 50/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1336 - loss: 2.4716 - val_accuracy: 0.1042 - val_loss: 2.6269\n",
            "Epoch 51/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1267 - loss: 2.5162 - val_accuracy: 0.1528 - val_loss: 2.2352\n",
            "Epoch 52/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1480 - loss: 2.3667 - val_accuracy: 0.1424 - val_loss: 2.1426\n",
            "Epoch 53/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1269 - loss: 2.5218 - val_accuracy: 0.1424 - val_loss: 2.4137\n",
            "Epoch 54/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1092 - loss: 2.5435 - val_accuracy: 0.1562 - val_loss: 2.2748\n",
            "Epoch 55/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1135 - loss: 2.5436 - val_accuracy: 0.1285 - val_loss: 2.1256\n",
            "Epoch 56/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1400 - loss: 2.4260 - val_accuracy: 0.1042 - val_loss: 2.5432\n",
            "Epoch 57/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1364 - loss: 2.4608 - val_accuracy: 0.1354 - val_loss: 2.6236\n",
            "Epoch 58/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1099 - loss: 2.5179 - val_accuracy: 0.1528 - val_loss: 2.3092\n",
            "Epoch 59/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1000 - loss: 2.5141 - val_accuracy: 0.1354 - val_loss: 2.2760\n",
            "Epoch 60/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1315 - loss: 2.5373 - val_accuracy: 0.1285 - val_loss: 2.2124\n",
            "Epoch 61/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1202 - loss: 2.4544 - val_accuracy: 0.1042 - val_loss: 2.4386\n",
            "Epoch 62/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1024 - loss: 2.5416 - val_accuracy: 0.1354 - val_loss: 2.5885\n",
            "Epoch 63/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1248 - loss: 2.5073 - val_accuracy: 0.1042 - val_loss: 2.4362\n",
            "Epoch 64/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1395 - loss: 2.4355 - val_accuracy: 0.1319 - val_loss: 2.5149\n",
            "Epoch 65/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1316 - loss: 2.4528 - val_accuracy: 0.1042 - val_loss: 2.9075\n",
            "Epoch 66/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1461 - loss: 2.4923 - val_accuracy: 0.1424 - val_loss: 2.4349\n",
            "Epoch 67/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1341 - loss: 2.4219 - val_accuracy: 0.1562 - val_loss: 2.6726\n",
            "Epoch 68/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1306 - loss: 2.5041 - val_accuracy: 0.1528 - val_loss: 2.2684\n",
            "Epoch 69/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1220 - loss: 2.4287 - val_accuracy: 0.1042 - val_loss: 2.9205\n",
            "Epoch 70/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1428 - loss: 2.4929 - val_accuracy: 0.1424 - val_loss: 2.3847\n",
            "Epoch 71/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1311 - loss: 2.5010 - val_accuracy: 0.1562 - val_loss: 2.2812\n",
            "Epoch 72/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1113 - loss: 2.4501 - val_accuracy: 0.1528 - val_loss: 2.5310\n",
            "Epoch 73/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1389 - loss: 2.4884 - val_accuracy: 0.1354 - val_loss: 2.6233\n",
            "Epoch 74/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1227 - loss: 2.5235 - val_accuracy: 0.1042 - val_loss: 2.4822\n",
            "Epoch 75/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1522 - loss: 2.4642 - val_accuracy: 0.1528 - val_loss: 2.1167\n",
            "Epoch 76/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1226 - loss: 2.4501 - val_accuracy: 0.1319 - val_loss: 2.2271\n",
            "Epoch 77/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1433 - loss: 2.3778 - val_accuracy: 0.1562 - val_loss: 2.7242\n",
            "Epoch 78/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1230 - loss: 2.4999 - val_accuracy: 0.1042 - val_loss: 2.5804\n",
            "Epoch 79/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1180 - loss: 2.4944 - val_accuracy: 0.1424 - val_loss: 2.3066\n",
            "Epoch 80/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1158 - loss: 2.4876 - val_accuracy: 0.1354 - val_loss: 2.2590\n",
            "Epoch 81/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1083 - loss: 2.5419 - val_accuracy: 0.1042 - val_loss: 2.3550\n",
            "Epoch 82/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1256 - loss: 2.4023 - val_accuracy: 0.1319 - val_loss: 2.4880\n",
            "Epoch 83/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1231 - loss: 2.4824 - val_accuracy: 0.1528 - val_loss: 2.7653\n",
            "Epoch 84/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1229 - loss: 2.4974 - val_accuracy: 0.1562 - val_loss: 2.3350\n",
            "Epoch 85/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1240 - loss: 2.4183 - val_accuracy: 0.1042 - val_loss: 2.2580\n",
            "Epoch 86/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1294 - loss: 2.4253 - val_accuracy: 0.1528 - val_loss: 2.7053\n",
            "Epoch 87/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1415 - loss: 2.4178 - val_accuracy: 0.1319 - val_loss: 2.6647\n",
            "Epoch 88/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1293 - loss: 2.4434 - val_accuracy: 0.1042 - val_loss: 2.6975\n",
            "Epoch 89/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1251 - loss: 2.4951 - val_accuracy: 0.1424 - val_loss: 2.3415\n",
            "Epoch 90/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1261 - loss: 2.4599 - val_accuracy: 0.1354 - val_loss: 2.2290\n",
            "Epoch 91/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1446 - loss: 2.3912 - val_accuracy: 0.1042 - val_loss: 2.2781\n",
            "Epoch 92/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1602 - loss: 2.3795 - val_accuracy: 0.1528 - val_loss: 2.4825\n",
            "Epoch 93/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1283 - loss: 2.5269 - val_accuracy: 0.1042 - val_loss: 2.7381\n",
            "Epoch 94/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1199 - loss: 2.4514 - val_accuracy: 0.1250 - val_loss: 2.4988\n",
            "Epoch 95/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1212 - loss: 2.4764 - val_accuracy: 0.1424 - val_loss: 2.6747\n",
            "Epoch 96/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1232 - loss: 2.5900 - val_accuracy: 0.1424 - val_loss: 2.2020\n",
            "Epoch 97/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1313 - loss: 2.4321 - val_accuracy: 0.1042 - val_loss: 2.5272\n",
            "Epoch 98/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1089 - loss: 2.5278 - val_accuracy: 0.1319 - val_loss: 2.4706\n",
            "Epoch 99/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1048 - loss: 2.4720 - val_accuracy: 0.1285 - val_loss: 2.5843\n",
            "Epoch 100/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1457 - loss: 2.4359 - val_accuracy: 0.1528 - val_loss: 2.1524\n",
            "Epoch 101/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1248 - loss: 2.3935 - val_accuracy: 0.1042 - val_loss: 2.3414\n",
            "Epoch 102/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1466 - loss: 2.4761 - val_accuracy: 0.1424 - val_loss: 2.3613\n",
            "Epoch 103/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1299 - loss: 2.4372 - val_accuracy: 0.1042 - val_loss: 2.3549\n",
            "Epoch 104/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1173 - loss: 2.4244 - val_accuracy: 0.0486 - val_loss: 2.2168\n",
            "Epoch 105/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1037 - loss: 2.4546 - val_accuracy: 0.1424 - val_loss: 2.2241\n",
            "Epoch 106/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1256 - loss: 2.4621 - val_accuracy: 0.1319 - val_loss: 2.3289\n",
            "Epoch 107/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1271 - loss: 2.4597 - val_accuracy: 0.1354 - val_loss: 2.5549\n",
            "Epoch 108/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1256 - loss: 2.4299 - val_accuracy: 0.1562 - val_loss: 2.1952\n",
            "Epoch 109/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1342 - loss: 2.3815 - val_accuracy: 0.1424 - val_loss: 2.3373\n",
            "Epoch 110/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1488 - loss: 2.4426 - val_accuracy: 0.1354 - val_loss: 2.4102\n",
            "Epoch 111/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1335 - loss: 2.5203 - val_accuracy: 0.1042 - val_loss: 2.4850\n",
            "Epoch 112/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1265 - loss: 2.5239 - val_accuracy: 0.1424 - val_loss: 2.4571\n",
            "Epoch 113/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1294 - loss: 2.4581 - val_accuracy: 0.1319 - val_loss: 2.4276\n",
            "Epoch 114/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1254 - loss: 2.5194 - val_accuracy: 0.1562 - val_loss: 2.1383\n",
            "Epoch 115/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1038 - loss: 2.4219 - val_accuracy: 0.1285 - val_loss: 2.2623\n",
            "Epoch 116/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1222 - loss: 2.4324 - val_accuracy: 0.1528 - val_loss: 2.2224\n",
            "Epoch 117/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1321 - loss: 2.4550 - val_accuracy: 0.1562 - val_loss: 2.5195\n",
            "Epoch 118/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1267 - loss: 2.4556 - val_accuracy: 0.1424 - val_loss: 2.3183\n",
            "Epoch 119/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1307 - loss: 2.4845 - val_accuracy: 0.1319 - val_loss: 2.5112\n",
            "Epoch 120/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1334 - loss: 2.4578 - val_accuracy: 0.1562 - val_loss: 2.4839\n",
            "Epoch 121/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1055 - loss: 2.4972 - val_accuracy: 0.1042 - val_loss: 2.1799\n",
            "Epoch 122/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1557 - loss: 2.3579 - val_accuracy: 0.1319 - val_loss: 2.5847\n",
            "Epoch 123/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1386 - loss: 2.4408 - val_accuracy: 0.1562 - val_loss: 2.6763\n",
            "Epoch 124/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1417 - loss: 2.5033 - val_accuracy: 0.1319 - val_loss: 2.1982\n",
            "Epoch 125/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1381 - loss: 2.4200 - val_accuracy: 0.1319 - val_loss: 2.2312\n",
            "Epoch 126/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1301 - loss: 2.4065 - val_accuracy: 0.1042 - val_loss: 2.2078\n",
            "Epoch 127/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1395 - loss: 2.4877 - val_accuracy: 0.1424 - val_loss: 2.3387\n",
            "Epoch 128/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1377 - loss: 2.3874 - val_accuracy: 0.1354 - val_loss: 2.6811\n",
            "Epoch 129/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1191 - loss: 2.5085 - val_accuracy: 0.0486 - val_loss: 2.5189\n",
            "Epoch 130/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1500 - loss: 2.4364 - val_accuracy: 0.1285 - val_loss: 2.4136\n",
            "Epoch 131/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1395 - loss: 2.4419 - val_accuracy: 0.1285 - val_loss: 2.2355\n",
            "Epoch 132/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1175 - loss: 2.4642 - val_accuracy: 0.1424 - val_loss: 2.3727\n",
            "Epoch 133/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1250 - loss: 2.5313 - val_accuracy: 0.1285 - val_loss: 2.1970\n",
            "Epoch 134/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1492 - loss: 2.4188 - val_accuracy: 0.1285 - val_loss: 2.4194\n",
            "Epoch 135/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1273 - loss: 2.4454 - val_accuracy: 0.1285 - val_loss: 2.3998\n",
            "Epoch 136/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1059 - loss: 2.4741 - val_accuracy: 0.1562 - val_loss: 2.3045\n",
            "Epoch 137/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1245 - loss: 2.4196 - val_accuracy: 0.1562 - val_loss: 2.3556\n",
            "Epoch 138/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1317 - loss: 2.4219 - val_accuracy: 0.1562 - val_loss: 2.1707\n",
            "Epoch 139/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1333 - loss: 2.4523 - val_accuracy: 0.1354 - val_loss: 2.5933\n",
            "Epoch 140/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1418 - loss: 2.4169 - val_accuracy: 0.1562 - val_loss: 2.1404\n",
            "Epoch 141/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1186 - loss: 2.3735 - val_accuracy: 0.1354 - val_loss: 2.3164\n",
            "Epoch 142/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1228 - loss: 2.4651 - val_accuracy: 0.1528 - val_loss: 2.2711\n",
            "Epoch 143/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1163 - loss: 2.4336 - val_accuracy: 0.1424 - val_loss: 2.5467\n",
            "Epoch 144/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1496 - loss: 2.4614 - val_accuracy: 0.1042 - val_loss: 2.3201\n",
            "Epoch 145/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1054 - loss: 2.5039 - val_accuracy: 0.1042 - val_loss: 2.4644\n",
            "Epoch 146/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1437 - loss: 2.4722 - val_accuracy: 0.1042 - val_loss: 2.4719\n",
            "Epoch 147/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1340 - loss: 2.5017 - val_accuracy: 0.1285 - val_loss: 2.4394\n",
            "Epoch 148/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1133 - loss: 2.5221 - val_accuracy: 0.1042 - val_loss: 2.5808\n",
            "Epoch 149/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1479 - loss: 2.4074 - val_accuracy: 0.1354 - val_loss: 2.9183\n",
            "Epoch 150/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1308 - loss: 2.4959 - val_accuracy: 0.1319 - val_loss: 2.2656\n",
            "Epoch 151/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1319 - loss: 2.4433 - val_accuracy: 0.1354 - val_loss: 2.2976\n",
            "Epoch 152/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1376 - loss: 2.4612 - val_accuracy: 0.0486 - val_loss: 2.2722\n",
            "Epoch 153/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0846 - loss: 2.4662 - val_accuracy: 0.1354 - val_loss: 2.2632\n",
            "Epoch 154/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1307 - loss: 2.4389 - val_accuracy: 0.1042 - val_loss: 2.3338\n",
            "Epoch 155/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1356 - loss: 2.5215 - val_accuracy: 0.1285 - val_loss: 2.3856\n",
            "Epoch 156/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1289 - loss: 2.4047 - val_accuracy: 0.1319 - val_loss: 2.4160\n",
            "Epoch 157/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1107 - loss: 2.5527 - val_accuracy: 0.1528 - val_loss: 2.3418\n",
            "Epoch 158/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1422 - loss: 2.3950 - val_accuracy: 0.1319 - val_loss: 2.8813\n",
            "Epoch 159/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1319 - loss: 2.5474 - val_accuracy: 0.1354 - val_loss: 2.6029\n",
            "Epoch 160/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1391 - loss: 2.4871 - val_accuracy: 0.0486 - val_loss: 2.3716\n",
            "Epoch 161/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1011 - loss: 2.4311 - val_accuracy: 0.1424 - val_loss: 2.4543\n",
            "Epoch 162/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1457 - loss: 2.4986 - val_accuracy: 0.1528 - val_loss: 2.2992\n",
            "Epoch 163/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1288 - loss: 2.4113 - val_accuracy: 0.1042 - val_loss: 2.4080\n",
            "Epoch 164/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1436 - loss: 2.3928 - val_accuracy: 0.1285 - val_loss: 2.4820\n",
            "Epoch 165/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1517 - loss: 2.4217 - val_accuracy: 0.1562 - val_loss: 2.2709\n",
            "Epoch 166/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1110 - loss: 2.4968 - val_accuracy: 0.1424 - val_loss: 2.2059\n",
            "Epoch 167/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1385 - loss: 2.4014 - val_accuracy: 0.1285 - val_loss: 2.3539\n",
            "Epoch 168/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1230 - loss: 2.4365 - val_accuracy: 0.0486 - val_loss: 2.6610\n",
            "Epoch 169/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1115 - loss: 2.5736 - val_accuracy: 0.1042 - val_loss: 2.3382\n",
            "Epoch 170/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1160 - loss: 2.5004 - val_accuracy: 0.1562 - val_loss: 2.6611\n",
            "Epoch 171/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1247 - loss: 2.4778 - val_accuracy: 0.1528 - val_loss: 2.2621\n",
            "Epoch 172/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1551 - loss: 2.3465 - val_accuracy: 0.0486 - val_loss: 2.2939\n",
            "Epoch 173/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1403 - loss: 2.3805 - val_accuracy: 0.1424 - val_loss: 2.2739\n",
            "Epoch 174/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1166 - loss: 2.4709 - val_accuracy: 0.1424 - val_loss: 2.3729\n",
            "Epoch 175/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1154 - loss: 2.5063 - val_accuracy: 0.1319 - val_loss: 2.5151\n",
            "Epoch 176/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1293 - loss: 2.4765 - val_accuracy: 0.1354 - val_loss: 2.3292\n",
            "Epoch 177/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1120 - loss: 2.4850 - val_accuracy: 0.1042 - val_loss: 2.7241\n",
            "Epoch 178/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1226 - loss: 2.5000 - val_accuracy: 0.1562 - val_loss: 2.2713\n",
            "Epoch 179/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1329 - loss: 2.4223 - val_accuracy: 0.1562 - val_loss: 2.3314\n",
            "Epoch 180/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1542 - loss: 2.3712 - val_accuracy: 0.0486 - val_loss: 2.7004\n",
            "Epoch 181/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1238 - loss: 2.4559 - val_accuracy: 0.1528 - val_loss: 2.5917\n",
            "Epoch 182/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1499 - loss: 2.4171 - val_accuracy: 0.1042 - val_loss: 2.4344\n",
            "Epoch 183/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1290 - loss: 2.4236 - val_accuracy: 0.1354 - val_loss: 2.2886\n",
            "Epoch 184/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1563 - loss: 2.4396 - val_accuracy: 0.1424 - val_loss: 2.2158\n",
            "Epoch 185/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1252 - loss: 2.4404 - val_accuracy: 0.1528 - val_loss: 2.4838\n",
            "Epoch 186/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1150 - loss: 2.5570 - val_accuracy: 0.1424 - val_loss: 2.2519\n",
            "Epoch 187/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1455 - loss: 2.4214 - val_accuracy: 0.1528 - val_loss: 2.4030\n",
            "Epoch 188/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1323 - loss: 2.4387 - val_accuracy: 0.1319 - val_loss: 2.4208\n",
            "Epoch 189/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1495 - loss: 2.4596 - val_accuracy: 0.1285 - val_loss: 2.4320\n",
            "Epoch 190/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1382 - loss: 2.4334 - val_accuracy: 0.1528 - val_loss: 2.6789\n",
            "Epoch 191/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1274 - loss: 2.4817 - val_accuracy: 0.1042 - val_loss: 2.4474\n",
            "Epoch 192/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1444 - loss: 2.4350 - val_accuracy: 0.1562 - val_loss: 2.2574\n",
            "Epoch 193/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1342 - loss: 2.4702 - val_accuracy: 0.1042 - val_loss: 2.3273\n",
            "Epoch 194/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1490 - loss: 2.4849 - val_accuracy: 0.1528 - val_loss: 2.1453\n",
            "Epoch 195/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1215 - loss: 2.3911 - val_accuracy: 0.1042 - val_loss: 2.3984\n",
            "Epoch 196/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1260 - loss: 2.5010 - val_accuracy: 0.1042 - val_loss: 2.3740\n",
            "Epoch 197/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0980 - loss: 2.4888 - val_accuracy: 0.1042 - val_loss: 2.5617\n",
            "Epoch 198/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1294 - loss: 2.4613 - val_accuracy: 0.1285 - val_loss: 2.2751\n",
            "Epoch 199/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1151 - loss: 2.4344 - val_accuracy: 0.1424 - val_loss: 2.2569\n",
            "Epoch 200/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1197 - loss: 2.5071 - val_accuracy: 0.1042 - val_loss: 2.3715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Dense(1024,activation='relu',input_dim=13))\n",
        "model.add(Dense(1024,activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(8,activation='softmax'))\n",
        "# Compile the model\n",
        "optimizer = RMSprop(learning_rate=0.006019793254027166)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "9cocqOkYSFE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=200,batch_size=32, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrCKvlLQSFCK",
        "outputId": "cfdfc7a8-925d-4cbb-f6cd-cd6aa364377f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - accuracy: 0.1777 - loss: 3.1252 - val_accuracy: 0.1701 - val_loss: 2.0325\n",
            "Epoch 2/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.2198 - loss: 2.0233 - val_accuracy: 0.2222 - val_loss: 1.9109\n",
            "Epoch 3/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2681 - loss: 1.8743 - val_accuracy: 0.3299 - val_loss: 1.7557\n",
            "Epoch 4/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3024 - loss: 1.7826 - val_accuracy: 0.3542 - val_loss: 1.7332\n",
            "Epoch 5/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3726 - loss: 1.7496 - val_accuracy: 0.3993 - val_loss: 1.6415\n",
            "Epoch 6/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3943 - loss: 1.6275 - val_accuracy: 0.4271 - val_loss: 1.6411\n",
            "Epoch 7/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3786 - loss: 1.6350 - val_accuracy: 0.3819 - val_loss: 1.8500\n",
            "Epoch 8/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4057 - loss: 1.5563 - val_accuracy: 0.4062 - val_loss: 1.6311\n",
            "Epoch 9/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4267 - loss: 1.5529 - val_accuracy: 0.4132 - val_loss: 1.6129\n",
            "Epoch 10/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4501 - loss: 1.4871 - val_accuracy: 0.4271 - val_loss: 1.5708\n",
            "Epoch 11/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5147 - loss: 1.3523 - val_accuracy: 0.4861 - val_loss: 1.5589\n",
            "Epoch 12/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5003 - loss: 1.3363 - val_accuracy: 0.4757 - val_loss: 1.7265\n",
            "Epoch 13/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5350 - loss: 1.2914 - val_accuracy: 0.4340 - val_loss: 1.9488\n",
            "Epoch 14/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5453 - loss: 1.2913 - val_accuracy: 0.4236 - val_loss: 1.6189\n",
            "Epoch 15/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5586 - loss: 1.2372 - val_accuracy: 0.5556 - val_loss: 1.5452\n",
            "Epoch 16/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6140 - loss: 1.0815 - val_accuracy: 0.4653 - val_loss: 1.9051\n",
            "Epoch 17/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5962 - loss: 1.2363 - val_accuracy: 0.5243 - val_loss: 1.6029\n",
            "Epoch 18/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6325 - loss: 1.1552 - val_accuracy: 0.5208 - val_loss: 1.8382\n",
            "Epoch 19/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6511 - loss: 1.1738 - val_accuracy: 0.5069 - val_loss: 1.9866\n",
            "Epoch 20/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6619 - loss: 1.0279 - val_accuracy: 0.5104 - val_loss: 1.6091\n",
            "Epoch 21/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6168 - loss: 1.1310 - val_accuracy: 0.5729 - val_loss: 1.6490\n",
            "Epoch 22/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6649 - loss: 0.9847 - val_accuracy: 0.5069 - val_loss: 2.0432\n",
            "Epoch 23/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6042 - loss: 1.1574 - val_accuracy: 0.5000 - val_loss: 2.0517\n",
            "Epoch 24/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6865 - loss: 1.0559 - val_accuracy: 0.5069 - val_loss: 2.6048\n",
            "Epoch 25/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6503 - loss: 1.0807 - val_accuracy: 0.5486 - val_loss: 1.9979\n",
            "Epoch 26/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6844 - loss: 0.9885 - val_accuracy: 0.5521 - val_loss: 1.5619\n",
            "Epoch 27/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6985 - loss: 0.9506 - val_accuracy: 0.5590 - val_loss: 1.5706\n",
            "Epoch 28/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7268 - loss: 1.0464 - val_accuracy: 0.5174 - val_loss: 2.1574\n",
            "Epoch 29/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7073 - loss: 0.9597 - val_accuracy: 0.5521 - val_loss: 2.2778\n",
            "Epoch 30/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6883 - loss: 0.9930 - val_accuracy: 0.5174 - val_loss: 2.0509\n",
            "Epoch 31/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7195 - loss: 0.9289 - val_accuracy: 0.4965 - val_loss: 2.3154\n",
            "Epoch 32/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6903 - loss: 1.0267 - val_accuracy: 0.5486 - val_loss: 2.6626\n",
            "Epoch 33/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7165 - loss: 1.1310 - val_accuracy: 0.5625 - val_loss: 2.4330\n",
            "Epoch 34/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7075 - loss: 0.9778 - val_accuracy: 0.5799 - val_loss: 2.0106\n",
            "Epoch 35/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7040 - loss: 1.0129 - val_accuracy: 0.6007 - val_loss: 2.1415\n",
            "Epoch 36/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7331 - loss: 0.8927 - val_accuracy: 0.5417 - val_loss: 2.0806\n",
            "Epoch 37/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6835 - loss: 1.0812 - val_accuracy: 0.5590 - val_loss: 2.3426\n",
            "Epoch 38/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7315 - loss: 0.7604 - val_accuracy: 0.5694 - val_loss: 2.7591\n",
            "Epoch 39/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7366 - loss: 0.9438 - val_accuracy: 0.5243 - val_loss: 3.8173\n",
            "Epoch 40/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7410 - loss: 0.9002 - val_accuracy: 0.5833 - val_loss: 3.0018\n",
            "Epoch 41/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7612 - loss: 0.9533 - val_accuracy: 0.5451 - val_loss: 3.0279\n",
            "Epoch 42/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7271 - loss: 0.9334 - val_accuracy: 0.5278 - val_loss: 3.0190\n",
            "Epoch 43/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7114 - loss: 0.9945 - val_accuracy: 0.5799 - val_loss: 2.7323\n",
            "Epoch 44/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7911 - loss: 0.7685 - val_accuracy: 0.5556 - val_loss: 3.2479\n",
            "Epoch 45/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7389 - loss: 0.8858 - val_accuracy: 0.5312 - val_loss: 2.1653\n",
            "Epoch 46/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7262 - loss: 1.0167 - val_accuracy: 0.5694 - val_loss: 2.8842\n",
            "Epoch 47/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7733 - loss: 0.9165 - val_accuracy: 0.5903 - val_loss: 2.3133\n",
            "Epoch 48/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7462 - loss: 0.9388 - val_accuracy: 0.5451 - val_loss: 3.1118\n",
            "Epoch 49/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7536 - loss: 0.9403 - val_accuracy: 0.5764 - val_loss: 2.5749\n",
            "Epoch 50/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7536 - loss: 1.0036 - val_accuracy: 0.5451 - val_loss: 2.7388\n",
            "Epoch 51/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7311 - loss: 1.0258 - val_accuracy: 0.5764 - val_loss: 2.7803\n",
            "Epoch 52/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7671 - loss: 0.9483 - val_accuracy: 0.5868 - val_loss: 2.5485\n",
            "Epoch 53/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7649 - loss: 1.0188 - val_accuracy: 0.5833 - val_loss: 3.8480\n",
            "Epoch 54/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7603 - loss: 0.8399 - val_accuracy: 0.5486 - val_loss: 3.1751\n",
            "Epoch 55/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7581 - loss: 0.8167 - val_accuracy: 0.6007 - val_loss: 3.1045\n",
            "Epoch 56/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7707 - loss: 0.8001 - val_accuracy: 0.5694 - val_loss: 2.8334\n",
            "Epoch 57/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7629 - loss: 0.9024 - val_accuracy: 0.5660 - val_loss: 3.2382\n",
            "Epoch 58/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7686 - loss: 0.7805 - val_accuracy: 0.5972 - val_loss: 3.4831\n",
            "Epoch 59/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7829 - loss: 0.8109 - val_accuracy: 0.6076 - val_loss: 3.5632\n",
            "Epoch 60/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7482 - loss: 1.1400 - val_accuracy: 0.6111 - val_loss: 2.7863\n",
            "Epoch 61/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7522 - loss: 1.2219 - val_accuracy: 0.5903 - val_loss: 2.6179\n",
            "Epoch 62/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7592 - loss: 0.9643 - val_accuracy: 0.5764 - val_loss: 2.5320\n",
            "Epoch 63/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7906 - loss: 0.8167 - val_accuracy: 0.5347 - val_loss: 2.9104\n",
            "Epoch 64/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7570 - loss: 0.8346 - val_accuracy: 0.5104 - val_loss: 3.4459\n",
            "Epoch 65/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6954 - loss: 1.1024 - val_accuracy: 0.6146 - val_loss: 4.2062\n",
            "Epoch 66/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7516 - loss: 1.0201 - val_accuracy: 0.5660 - val_loss: 4.0384\n",
            "Epoch 67/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7503 - loss: 0.9420 - val_accuracy: 0.5729 - val_loss: 5.8556\n",
            "Epoch 68/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7168 - loss: 1.2443 - val_accuracy: 0.5312 - val_loss: 4.9156\n",
            "Epoch 69/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7702 - loss: 1.2203 - val_accuracy: 0.5903 - val_loss: 2.3257\n",
            "Epoch 70/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7170 - loss: 1.0324 - val_accuracy: 0.5590 - val_loss: 3.3409\n",
            "Epoch 71/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7407 - loss: 1.2314 - val_accuracy: 0.6007 - val_loss: 4.0794\n",
            "Epoch 72/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7094 - loss: 1.1800 - val_accuracy: 0.5556 - val_loss: 4.1411\n",
            "Epoch 73/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7710 - loss: 0.7383 - val_accuracy: 0.5451 - val_loss: 6.0395\n",
            "Epoch 74/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7228 - loss: 1.4363 - val_accuracy: 0.5382 - val_loss: 4.6081\n",
            "Epoch 75/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7567 - loss: 1.0099 - val_accuracy: 0.5625 - val_loss: 5.4379\n",
            "Epoch 76/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7507 - loss: 1.4377 - val_accuracy: 0.5799 - val_loss: 2.6777\n",
            "Epoch 77/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7638 - loss: 0.8578 - val_accuracy: 0.5139 - val_loss: 2.7076\n",
            "Epoch 78/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7544 - loss: 1.0805 - val_accuracy: 0.6042 - val_loss: 3.2186\n",
            "Epoch 79/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7425 - loss: 1.0573 - val_accuracy: 0.5556 - val_loss: 3.4476\n",
            "Epoch 80/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7368 - loss: 1.1144 - val_accuracy: 0.5729 - val_loss: 2.9905\n",
            "Epoch 81/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7286 - loss: 1.0491 - val_accuracy: 0.5486 - val_loss: 4.9659\n",
            "Epoch 82/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6888 - loss: 1.2583 - val_accuracy: 0.5451 - val_loss: 4.9955\n",
            "Epoch 83/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7478 - loss: 0.8636 - val_accuracy: 0.4688 - val_loss: 3.2250\n",
            "Epoch 84/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7048 - loss: 1.1911 - val_accuracy: 0.5243 - val_loss: 3.2508\n",
            "Epoch 85/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6922 - loss: 1.4528 - val_accuracy: 0.5764 - val_loss: 2.9476\n",
            "Epoch 86/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7193 - loss: 1.1893 - val_accuracy: 0.5660 - val_loss: 6.8741\n",
            "Epoch 87/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7031 - loss: 1.1559 - val_accuracy: 0.5174 - val_loss: 4.0047\n",
            "Epoch 88/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6732 - loss: 1.2697 - val_accuracy: 0.5174 - val_loss: 3.5627\n",
            "Epoch 89/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6783 - loss: 1.2077 - val_accuracy: 0.6042 - val_loss: 5.6175\n",
            "Epoch 90/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6779 - loss: 1.6241 - val_accuracy: 0.5451 - val_loss: 3.8672\n",
            "Epoch 91/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6301 - loss: 1.1829 - val_accuracy: 0.4688 - val_loss: 2.7789\n",
            "Epoch 92/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6931 - loss: 1.1249 - val_accuracy: 0.5104 - val_loss: 5.4796\n",
            "Epoch 93/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7112 - loss: 1.0789 - val_accuracy: 0.4826 - val_loss: 3.2586\n",
            "Epoch 94/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6619 - loss: 1.1034 - val_accuracy: 0.5451 - val_loss: 3.1538\n",
            "Epoch 95/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6624 - loss: 1.3619 - val_accuracy: 0.5347 - val_loss: 3.0636\n",
            "Epoch 96/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6594 - loss: 1.2359 - val_accuracy: 0.5451 - val_loss: 5.1588\n",
            "Epoch 97/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6549 - loss: 1.4056 - val_accuracy: 0.5521 - val_loss: 6.0150\n",
            "Epoch 98/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6784 - loss: 1.4155 - val_accuracy: 0.5174 - val_loss: 7.8512\n",
            "Epoch 99/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6835 - loss: 1.6477 - val_accuracy: 0.5347 - val_loss: 8.6407\n",
            "Epoch 100/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6454 - loss: 1.2905 - val_accuracy: 0.4688 - val_loss: 4.9628\n",
            "Epoch 101/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5933 - loss: 1.5490 - val_accuracy: 0.4653 - val_loss: 6.2979\n",
            "Epoch 102/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6239 - loss: 1.3226 - val_accuracy: 0.5208 - val_loss: 5.6216\n",
            "Epoch 103/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6329 - loss: 1.4671 - val_accuracy: 0.4722 - val_loss: 4.8527\n",
            "Epoch 104/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5693 - loss: 2.4520 - val_accuracy: 0.3819 - val_loss: 2.5682\n",
            "Epoch 105/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5631 - loss: 1.5754 - val_accuracy: 0.4167 - val_loss: 4.0815\n",
            "Epoch 106/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5917 - loss: 1.8383 - val_accuracy: 0.4896 - val_loss: 6.0558\n",
            "Epoch 107/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5967 - loss: 1.4687 - val_accuracy: 0.4688 - val_loss: 6.9283\n",
            "Epoch 108/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5542 - loss: 2.1642 - val_accuracy: 0.4861 - val_loss: 3.0243\n",
            "Epoch 109/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5161 - loss: 2.2086 - val_accuracy: 0.4688 - val_loss: 4.1648\n",
            "Epoch 110/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5672 - loss: 1.4664 - val_accuracy: 0.3993 - val_loss: 3.4832\n",
            "Epoch 111/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5241 - loss: 2.4073 - val_accuracy: 0.3299 - val_loss: 2.4487\n",
            "Epoch 112/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4670 - loss: 2.2120 - val_accuracy: 0.3576 - val_loss: 3.4582\n",
            "Epoch 113/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5352 - loss: 1.6320 - val_accuracy: 0.2917 - val_loss: 5.2657\n",
            "Epoch 114/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4489 - loss: 1.6734 - val_accuracy: 0.3889 - val_loss: 6.3792\n",
            "Epoch 115/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5090 - loss: 2.0461 - val_accuracy: 0.2951 - val_loss: 2.5431\n",
            "Epoch 116/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4631 - loss: 1.6250 - val_accuracy: 0.3299 - val_loss: 6.7650\n",
            "Epoch 117/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4969 - loss: 1.4771 - val_accuracy: 0.3924 - val_loss: 9.4976\n",
            "Epoch 118/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4857 - loss: 2.6138 - val_accuracy: 0.3576 - val_loss: 4.4842\n",
            "Epoch 119/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5010 - loss: 2.0893 - val_accuracy: 0.3854 - val_loss: 7.8681\n",
            "Epoch 120/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4790 - loss: 2.6718 - val_accuracy: 0.3576 - val_loss: 7.4661\n",
            "Epoch 121/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4734 - loss: 1.6810 - val_accuracy: 0.3299 - val_loss: 7.7454\n",
            "Epoch 122/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4687 - loss: 2.4614 - val_accuracy: 0.4028 - val_loss: 9.0203\n",
            "Epoch 123/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4926 - loss: 2.7254 - val_accuracy: 0.3576 - val_loss: 5.2556\n",
            "Epoch 124/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4902 - loss: 1.8286 - val_accuracy: 0.3785 - val_loss: 5.0069\n",
            "Epoch 125/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4912 - loss: 2.7520 - val_accuracy: 0.3333 - val_loss: 5.6588\n",
            "Epoch 126/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3933 - loss: 2.1436 - val_accuracy: 0.2465 - val_loss: 3.8728\n",
            "Epoch 127/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3272 - loss: 2.1096 - val_accuracy: 0.2674 - val_loss: 4.6437\n",
            "Epoch 128/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3024 - loss: 2.0792 - val_accuracy: 0.2535 - val_loss: 4.3261\n",
            "Epoch 129/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2848 - loss: 2.1575 - val_accuracy: 0.2083 - val_loss: 2.9240\n",
            "Epoch 130/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2221 - loss: 2.1328 - val_accuracy: 0.1910 - val_loss: 3.8838\n",
            "Epoch 131/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3088 - loss: 1.9105 - val_accuracy: 0.1875 - val_loss: 3.3374\n",
            "Epoch 132/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2944 - loss: 2.1538 - val_accuracy: 0.1840 - val_loss: 2.6974\n",
            "Epoch 133/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2849 - loss: 2.4324 - val_accuracy: 0.1562 - val_loss: 2.8364\n",
            "Epoch 134/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2587 - loss: 1.8492 - val_accuracy: 0.1806 - val_loss: 3.9295\n",
            "Epoch 135/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2968 - loss: 2.1961 - val_accuracy: 0.1493 - val_loss: 5.7519\n",
            "Epoch 136/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2330 - loss: 2.6675 - val_accuracy: 0.1424 - val_loss: 3.9742\n",
            "Epoch 137/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2405 - loss: 2.1953 - val_accuracy: 0.2257 - val_loss: 5.5416\n",
            "Epoch 138/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2951 - loss: 2.1457 - val_accuracy: 0.1354 - val_loss: 2.7190\n",
            "Epoch 139/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2463 - loss: 2.0612 - val_accuracy: 0.1319 - val_loss: 4.4480\n",
            "Epoch 140/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2272 - loss: 1.9365 - val_accuracy: 0.1632 - val_loss: 4.2892\n",
            "Epoch 141/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2288 - loss: 2.0500 - val_accuracy: 0.1493 - val_loss: 3.2310\n",
            "Epoch 142/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2485 - loss: 2.0535 - val_accuracy: 0.1528 - val_loss: 3.0681\n",
            "Epoch 143/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2269 - loss: 2.0498 - val_accuracy: 0.1736 - val_loss: 4.4252\n",
            "Epoch 144/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2422 - loss: 2.1274 - val_accuracy: 0.2118 - val_loss: 3.9003\n",
            "Epoch 145/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2216 - loss: 2.3937 - val_accuracy: 0.1632 - val_loss: 2.2441\n",
            "Epoch 146/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1922 - loss: 2.4457 - val_accuracy: 0.1528 - val_loss: 2.3158\n",
            "Epoch 147/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2239 - loss: 2.1319 - val_accuracy: 0.1285 - val_loss: 2.9703\n",
            "Epoch 148/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1992 - loss: 2.6259 - val_accuracy: 0.1250 - val_loss: 2.2919\n",
            "Epoch 149/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1881 - loss: 2.4144 - val_accuracy: 0.1215 - val_loss: 2.4758\n",
            "Epoch 150/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1791 - loss: 2.0746 - val_accuracy: 0.1181 - val_loss: 2.1193\n",
            "Epoch 151/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1960 - loss: 1.9756 - val_accuracy: 0.1181 - val_loss: 2.5003\n",
            "Epoch 152/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1640 - loss: 2.3352 - val_accuracy: 0.1146 - val_loss: 2.1868\n",
            "Epoch 153/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1726 - loss: 2.0464 - val_accuracy: 0.1042 - val_loss: 2.0536\n",
            "Epoch 154/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1775 - loss: 2.0044 - val_accuracy: 0.1042 - val_loss: 2.0531\n",
            "Epoch 155/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1810 - loss: 2.0049 - val_accuracy: 0.1146 - val_loss: 2.0316\n",
            "Epoch 156/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1534 - loss: 2.3795 - val_accuracy: 0.1111 - val_loss: 2.1299\n",
            "Epoch 157/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1560 - loss: 2.2763 - val_accuracy: 0.1042 - val_loss: 2.0521\n",
            "Epoch 158/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1466 - loss: 2.0761 - val_accuracy: 0.1042 - val_loss: 2.2597\n",
            "Epoch 159/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1241 - loss: 2.0775 - val_accuracy: 0.1042 - val_loss: 2.0521\n",
            "Epoch 160/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1607 - loss: 2.6056 - val_accuracy: 0.1042 - val_loss: 2.0528\n",
            "Epoch 161/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1515 - loss: 2.0744 - val_accuracy: 0.1076 - val_loss: 2.0461\n",
            "Epoch 162/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1728 - loss: 2.0911 - val_accuracy: 0.1111 - val_loss: 2.0403\n",
            "Epoch 163/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1532 - loss: 2.0560 - val_accuracy: 0.1111 - val_loss: 2.0405\n",
            "Epoch 164/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1747 - loss: 2.0325 - val_accuracy: 0.1111 - val_loss: 2.0401\n",
            "Epoch 165/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1783 - loss: 2.2035 - val_accuracy: 0.1215 - val_loss: 2.0203\n",
            "Epoch 166/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1671 - loss: 2.0881 - val_accuracy: 0.1076 - val_loss: 2.0447\n",
            "Epoch 167/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1471 - loss: 2.0662 - val_accuracy: 0.1111 - val_loss: 2.0370\n",
            "Epoch 168/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1458 - loss: 2.1142 - val_accuracy: 0.1111 - val_loss: 2.0365\n",
            "Epoch 169/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1484 - loss: 2.0253 - val_accuracy: 0.1111 - val_loss: 2.0369\n",
            "Epoch 170/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1609 - loss: 2.0503 - val_accuracy: 0.1146 - val_loss: 2.0292\n",
            "Epoch 171/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1679 - loss: 2.0440 - val_accuracy: 0.1111 - val_loss: 2.0360\n",
            "Epoch 172/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1338 - loss: 2.0296 - val_accuracy: 0.1111 - val_loss: 3.1839\n",
            "Epoch 173/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1576 - loss: 2.0285 - val_accuracy: 0.1111 - val_loss: 2.1563\n",
            "Epoch 174/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1554 - loss: 2.0260 - val_accuracy: 0.1076 - val_loss: 2.3404\n",
            "Epoch 175/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1563 - loss: 1.9974 - val_accuracy: 0.1181 - val_loss: 2.8122\n",
            "Epoch 176/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1598 - loss: 2.2637 - val_accuracy: 0.1146 - val_loss: 2.0288\n",
            "Epoch 177/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1790 - loss: 2.0090 - val_accuracy: 0.1181 - val_loss: 2.0203\n",
            "Epoch 178/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1565 - loss: 2.0777 - val_accuracy: 0.1285 - val_loss: 2.0796\n",
            "Epoch 179/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1909 - loss: 2.6832 - val_accuracy: 0.1146 - val_loss: 2.0303\n",
            "Epoch 180/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1653 - loss: 2.4914 - val_accuracy: 0.1146 - val_loss: 2.0292\n",
            "Epoch 181/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1634 - loss: 2.0170 - val_accuracy: 0.1111 - val_loss: 2.0364\n",
            "Epoch 182/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1603 - loss: 2.3186 - val_accuracy: 0.1181 - val_loss: 2.0223\n",
            "Epoch 183/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1627 - loss: 2.2481 - val_accuracy: 0.1111 - val_loss: 2.0364\n",
            "Epoch 184/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1696 - loss: 2.0128 - val_accuracy: 0.1215 - val_loss: 2.0144\n",
            "Epoch 185/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1664 - loss: 2.0155 - val_accuracy: 0.1250 - val_loss: 2.6090\n",
            "Epoch 186/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1653 - loss: 2.2319 - val_accuracy: 0.1250 - val_loss: 2.0097\n",
            "Epoch 187/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1681 - loss: 2.0336 - val_accuracy: 0.1181 - val_loss: 2.7933\n",
            "Epoch 188/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1580 - loss: 2.0183 - val_accuracy: 0.1181 - val_loss: 3.3483\n",
            "Epoch 189/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1671 - loss: 2.0990 - val_accuracy: 0.1181 - val_loss: 2.0215\n",
            "Epoch 190/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1575 - loss: 2.0036 - val_accuracy: 0.1146 - val_loss: 2.4826\n",
            "Epoch 191/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1517 - loss: 2.8046 - val_accuracy: 0.1215 - val_loss: 2.0134\n",
            "Epoch 192/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1665 - loss: 2.0037 - val_accuracy: 0.1146 - val_loss: 2.3401\n",
            "Epoch 193/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1571 - loss: 2.0347 - val_accuracy: 0.1111 - val_loss: 2.0344\n",
            "Epoch 194/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1765 - loss: 2.0617 - val_accuracy: 0.1076 - val_loss: 2.0444\n",
            "Epoch 195/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1597 - loss: 2.0655 - val_accuracy: 0.1076 - val_loss: 2.3028\n",
            "Epoch 196/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1540 - loss: 2.0985 - val_accuracy: 0.1111 - val_loss: 2.0376\n",
            "Epoch 197/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1607 - loss: 2.0168 - val_accuracy: 0.1146 - val_loss: 2.0313\n",
            "Epoch 198/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1541 - loss: 2.3000 - val_accuracy: 0.1146 - val_loss: 2.0308\n",
            "Epoch 199/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1582 - loss: 2.0207 - val_accuracy: 0.1146 - val_loss: 2.0309\n",
            "Epoch 200/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1683 - loss: 2.0266 - val_accuracy: 0.1146 - val_loss: 2.0308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###67"
      ],
      "metadata": {
        "id": "ymNriV4_T_a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Dense(390,activation='relu',input_dim=15))\n",
        "model.add(Dense(780,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(8,activation='softmax'))\n",
        "# Compile the model\n",
        "optimizer = RMSprop(learning_rate=0.006019793254027166)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "D0z59oaMSE-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=200,batch_size=32, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0kukDsTSEs3",
        "outputId": "64b984f0-7130-4dc2-b00b-6881236c9dd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.2108 - loss: 3.1166 - val_accuracy: 0.2396 - val_loss: 3.4816\n",
            "Epoch 2/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.3168 - loss: 2.0539 - val_accuracy: 0.2257 - val_loss: 5.2246\n",
            "Epoch 3/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.3328 - loss: 1.8741 - val_accuracy: 0.3333 - val_loss: 5.3599\n",
            "Epoch 4/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.4145 - loss: 1.6940 - val_accuracy: 0.3472 - val_loss: 3.4947\n",
            "Epoch 5/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.4418 - loss: 1.5487 - val_accuracy: 0.3819 - val_loss: 2.0382\n",
            "Epoch 6/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5241 - loss: 1.4615 - val_accuracy: 0.4653 - val_loss: 1.8102\n",
            "Epoch 7/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5152 - loss: 1.4020 - val_accuracy: 0.4410 - val_loss: 1.9584\n",
            "Epoch 8/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5658 - loss: 1.2238 - val_accuracy: 0.4375 - val_loss: 2.0402\n",
            "Epoch 9/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5892 - loss: 1.1522 - val_accuracy: 0.4132 - val_loss: 2.0543\n",
            "Epoch 10/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.5864 - loss: 1.2168 - val_accuracy: 0.4722 - val_loss: 1.6988\n",
            "Epoch 11/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.6007 - loss: 1.0601 - val_accuracy: 0.4688 - val_loss: 1.5758\n",
            "Epoch 12/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.6631 - loss: 0.9523 - val_accuracy: 0.4757 - val_loss: 1.8161\n",
            "Epoch 13/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.6701 - loss: 0.8996 - val_accuracy: 0.4826 - val_loss: 1.8666\n",
            "Epoch 14/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.6940 - loss: 0.8356 - val_accuracy: 0.4965 - val_loss: 1.7220\n",
            "Epoch 15/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6974 - loss: 0.8611 - val_accuracy: 0.5556 - val_loss: 1.5476\n",
            "Epoch 16/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7137 - loss: 0.8358 - val_accuracy: 0.4965 - val_loss: 1.8098\n",
            "Epoch 17/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7651 - loss: 0.7514 - val_accuracy: 0.5486 - val_loss: 1.5023\n",
            "Epoch 18/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7647 - loss: 0.7318 - val_accuracy: 0.5208 - val_loss: 1.5970\n",
            "Epoch 19/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7654 - loss: 0.6856 - val_accuracy: 0.5694 - val_loss: 1.6709\n",
            "Epoch 20/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7937 - loss: 0.6171 - val_accuracy: 0.5104 - val_loss: 1.7778\n",
            "Epoch 21/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7824 - loss: 0.6451 - val_accuracy: 0.5382 - val_loss: 1.7830\n",
            "Epoch 22/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7938 - loss: 0.5698 - val_accuracy: 0.5312 - val_loss: 1.9065\n",
            "Epoch 23/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7973 - loss: 0.6144 - val_accuracy: 0.5104 - val_loss: 1.6812\n",
            "Epoch 24/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.8184 - loss: 0.4808 - val_accuracy: 0.5417 - val_loss: 1.9038\n",
            "Epoch 25/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.8184 - loss: 0.5660 - val_accuracy: 0.5347 - val_loss: 1.7107\n",
            "Epoch 26/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.8511 - loss: 0.4545 - val_accuracy: 0.4896 - val_loss: 1.9151\n",
            "Epoch 27/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8163 - loss: 0.5145 - val_accuracy: 0.5382 - val_loss: 1.7012\n",
            "Epoch 28/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8645 - loss: 0.4069 - val_accuracy: 0.5972 - val_loss: 1.6106\n",
            "Epoch 29/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8616 - loss: 0.4420 - val_accuracy: 0.5312 - val_loss: 1.8125\n",
            "Epoch 30/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8656 - loss: 0.3826 - val_accuracy: 0.5486 - val_loss: 1.8695\n",
            "Epoch 31/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8921 - loss: 0.3504 - val_accuracy: 0.5764 - val_loss: 1.8460\n",
            "Epoch 32/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8441 - loss: 0.4225 - val_accuracy: 0.5590 - val_loss: 1.9353\n",
            "Epoch 33/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8437 - loss: 0.4342 - val_accuracy: 0.5312 - val_loss: 2.0050\n",
            "Epoch 34/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8926 - loss: 0.3486 - val_accuracy: 0.5694 - val_loss: 2.0415\n",
            "Epoch 35/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9065 - loss: 0.3577 - val_accuracy: 0.5590 - val_loss: 1.8838\n",
            "Epoch 36/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.8685 - loss: 0.3657 - val_accuracy: 0.5660 - val_loss: 1.8333\n",
            "Epoch 37/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.8840 - loss: 0.3262 - val_accuracy: 0.5729 - val_loss: 1.8291\n",
            "Epoch 38/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.8782 - loss: 0.3170 - val_accuracy: 0.5590 - val_loss: 1.9289\n",
            "Epoch 39/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8934 - loss: 0.3099 - val_accuracy: 0.5938 - val_loss: 1.7696\n",
            "Epoch 40/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8813 - loss: 0.3366 - val_accuracy: 0.5833 - val_loss: 1.8158\n",
            "Epoch 41/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8872 - loss: 0.3744 - val_accuracy: 0.5590 - val_loss: 1.9851\n",
            "Epoch 42/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9069 - loss: 0.2773 - val_accuracy: 0.5799 - val_loss: 1.8758\n",
            "Epoch 43/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8806 - loss: 0.3457 - val_accuracy: 0.5660 - val_loss: 2.1986\n",
            "Epoch 44/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8968 - loss: 0.2954 - val_accuracy: 0.5903 - val_loss: 1.9397\n",
            "Epoch 45/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8937 - loss: 0.3010 - val_accuracy: 0.5764 - val_loss: 1.9803\n",
            "Epoch 46/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9206 - loss: 0.2683 - val_accuracy: 0.5382 - val_loss: 2.3155\n",
            "Epoch 47/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9009 - loss: 0.3128 - val_accuracy: 0.6285 - val_loss: 1.6132\n",
            "Epoch 48/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9123 - loss: 0.2605 - val_accuracy: 0.5694 - val_loss: 1.9607\n",
            "Epoch 49/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9202 - loss: 0.2518 - val_accuracy: 0.5660 - val_loss: 2.2207\n",
            "Epoch 50/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9195 - loss: 0.2172 - val_accuracy: 0.5799 - val_loss: 2.1361\n",
            "Epoch 51/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9234 - loss: 0.2234 - val_accuracy: 0.5903 - val_loss: 1.9102\n",
            "Epoch 52/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8922 - loss: 0.3051 - val_accuracy: 0.5625 - val_loss: 2.2459\n",
            "Epoch 53/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9144 - loss: 0.2772 - val_accuracy: 0.5799 - val_loss: 2.0171\n",
            "Epoch 54/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9132 - loss: 0.2578 - val_accuracy: 0.5590 - val_loss: 2.2706\n",
            "Epoch 55/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9072 - loss: 0.2707 - val_accuracy: 0.5694 - val_loss: 2.1893\n",
            "Epoch 56/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9229 - loss: 0.2419 - val_accuracy: 0.5799 - val_loss: 2.2940\n",
            "Epoch 57/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9284 - loss: 0.2084 - val_accuracy: 0.5903 - val_loss: 2.0351\n",
            "Epoch 58/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9422 - loss: 0.1961 - val_accuracy: 0.5660 - val_loss: 2.1063\n",
            "Epoch 59/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9221 - loss: 0.2289 - val_accuracy: 0.5486 - val_loss: 2.2458\n",
            "Epoch 60/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9222 - loss: 0.2253 - val_accuracy: 0.6146 - val_loss: 1.9242\n",
            "Epoch 61/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9272 - loss: 0.1957 - val_accuracy: 0.6146 - val_loss: 2.0015\n",
            "Epoch 62/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.9324 - loss: 0.2474 - val_accuracy: 0.5903 - val_loss: 1.9793\n",
            "Epoch 63/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9264 - loss: 0.2603 - val_accuracy: 0.5799 - val_loss: 1.9820\n",
            "Epoch 64/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9280 - loss: 0.2018 - val_accuracy: 0.5764 - val_loss: 2.1857\n",
            "Epoch 65/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9378 - loss: 0.1732 - val_accuracy: 0.6007 - val_loss: 2.2882\n",
            "Epoch 66/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9421 - loss: 0.1917 - val_accuracy: 0.5799 - val_loss: 2.2994\n",
            "Epoch 67/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9201 - loss: 0.2696 - val_accuracy: 0.6007 - val_loss: 2.0450\n",
            "Epoch 68/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9408 - loss: 0.1759 - val_accuracy: 0.6007 - val_loss: 2.1431\n",
            "Epoch 69/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9354 - loss: 0.1873 - val_accuracy: 0.5660 - val_loss: 2.2292\n",
            "Epoch 70/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9490 - loss: 0.1610 - val_accuracy: 0.6007 - val_loss: 2.0682\n",
            "Epoch 71/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9374 - loss: 0.2351 - val_accuracy: 0.6250 - val_loss: 1.9099\n",
            "Epoch 72/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9469 - loss: 0.1644 - val_accuracy: 0.5694 - val_loss: 2.3366\n",
            "Epoch 73/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9421 - loss: 0.1679 - val_accuracy: 0.5903 - val_loss: 2.2223\n",
            "Epoch 74/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9530 - loss: 0.1557 - val_accuracy: 0.5903 - val_loss: 2.3566\n",
            "Epoch 75/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9443 - loss: 0.1752 - val_accuracy: 0.5868 - val_loss: 2.0754\n",
            "Epoch 76/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9585 - loss: 0.1365 - val_accuracy: 0.6215 - val_loss: 2.1038\n",
            "Epoch 77/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9389 - loss: 0.2274 - val_accuracy: 0.6111 - val_loss: 2.1306\n",
            "Epoch 78/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9465 - loss: 0.1505 - val_accuracy: 0.6215 - val_loss: 2.1887\n",
            "Epoch 79/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9456 - loss: 0.1745 - val_accuracy: 0.5868 - val_loss: 2.0883\n",
            "Epoch 80/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9690 - loss: 0.1072 - val_accuracy: 0.6042 - val_loss: 2.2314\n",
            "Epoch 81/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9549 - loss: 0.1444 - val_accuracy: 0.6285 - val_loss: 2.0963\n",
            "Epoch 82/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9467 - loss: 0.1672 - val_accuracy: 0.6076 - val_loss: 2.1582\n",
            "Epoch 83/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9523 - loss: 0.1629 - val_accuracy: 0.5868 - val_loss: 2.1106\n",
            "Epoch 84/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9528 - loss: 0.1534 - val_accuracy: 0.6042 - val_loss: 2.2065\n",
            "Epoch 85/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9366 - loss: 0.1517 - val_accuracy: 0.5799 - val_loss: 2.2691\n",
            "Epoch 86/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9458 - loss: 0.1681 - val_accuracy: 0.5833 - val_loss: 2.1562\n",
            "Epoch 87/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9439 - loss: 0.1725 - val_accuracy: 0.5833 - val_loss: 2.1190\n",
            "Epoch 88/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9412 - loss: 0.1652 - val_accuracy: 0.6111 - val_loss: 2.1026\n",
            "Epoch 89/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9597 - loss: 0.1087 - val_accuracy: 0.6181 - val_loss: 2.1016\n",
            "Epoch 90/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9553 - loss: 0.1333 - val_accuracy: 0.6215 - val_loss: 2.0668\n",
            "Epoch 91/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9376 - loss: 0.2122 - val_accuracy: 0.6215 - val_loss: 2.0054\n",
            "Epoch 92/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9578 - loss: 0.1201 - val_accuracy: 0.5938 - val_loss: 2.3144\n",
            "Epoch 93/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9458 - loss: 0.1476 - val_accuracy: 0.6042 - val_loss: 2.1997\n",
            "Epoch 94/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9749 - loss: 0.0749 - val_accuracy: 0.6285 - val_loss: 2.4007\n",
            "Epoch 95/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9586 - loss: 0.1203 - val_accuracy: 0.5417 - val_loss: 2.7840\n",
            "Epoch 96/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9385 - loss: 0.2159 - val_accuracy: 0.6076 - val_loss: 2.3590\n",
            "Epoch 97/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9503 - loss: 0.1487 - val_accuracy: 0.6042 - val_loss: 2.0448\n",
            "Epoch 98/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.9596 - loss: 0.1340 - val_accuracy: 0.5799 - val_loss: 2.4568\n",
            "Epoch 99/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9516 - loss: 0.1555 - val_accuracy: 0.5833 - val_loss: 2.3231\n",
            "Epoch 100/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9514 - loss: 0.1404 - val_accuracy: 0.6285 - val_loss: 2.2629\n",
            "Epoch 101/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9770 - loss: 0.0697 - val_accuracy: 0.5799 - val_loss: 2.3782\n",
            "Epoch 102/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9647 - loss: 0.0944 - val_accuracy: 0.5694 - val_loss: 2.5396\n",
            "Epoch 103/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9558 - loss: 0.1384 - val_accuracy: 0.6111 - val_loss: 2.4005\n",
            "Epoch 104/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9659 - loss: 0.1198 - val_accuracy: 0.6111 - val_loss: 2.1684\n",
            "Epoch 105/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9558 - loss: 0.1239 - val_accuracy: 0.5833 - val_loss: 2.5530\n",
            "Epoch 106/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9683 - loss: 0.1130 - val_accuracy: 0.6076 - val_loss: 2.3607\n",
            "Epoch 107/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9390 - loss: 0.1793 - val_accuracy: 0.5833 - val_loss: 2.1967\n",
            "Epoch 108/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9694 - loss: 0.0827 - val_accuracy: 0.5764 - val_loss: 2.3166\n",
            "Epoch 109/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9654 - loss: 0.1131 - val_accuracy: 0.5764 - val_loss: 2.3503\n",
            "Epoch 110/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9506 - loss: 0.1464 - val_accuracy: 0.5938 - val_loss: 2.4823\n",
            "Epoch 111/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9563 - loss: 0.1217 - val_accuracy: 0.6181 - val_loss: 2.3303\n",
            "Epoch 112/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9474 - loss: 0.1688 - val_accuracy: 0.6354 - val_loss: 2.3372\n",
            "Epoch 113/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9555 - loss: 0.1263 - val_accuracy: 0.6111 - val_loss: 2.2219\n",
            "Epoch 114/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9574 - loss: 0.1375 - val_accuracy: 0.5938 - val_loss: 2.2965\n",
            "Epoch 115/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9567 - loss: 0.1310 - val_accuracy: 0.6215 - val_loss: 2.3240\n",
            "Epoch 116/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9717 - loss: 0.0915 - val_accuracy: 0.5799 - val_loss: 2.7520\n",
            "Epoch 117/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9634 - loss: 0.1307 - val_accuracy: 0.6146 - val_loss: 2.7522\n",
            "Epoch 118/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9568 - loss: 0.1143 - val_accuracy: 0.6319 - val_loss: 2.4140\n",
            "Epoch 119/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9649 - loss: 0.1157 - val_accuracy: 0.6146 - val_loss: 2.5160\n",
            "Epoch 120/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9699 - loss: 0.0961 - val_accuracy: 0.5972 - val_loss: 2.2884\n",
            "Epoch 121/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9662 - loss: 0.1051 - val_accuracy: 0.5833 - val_loss: 2.3908\n",
            "Epoch 122/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9644 - loss: 0.0832 - val_accuracy: 0.5972 - val_loss: 2.4371\n",
            "Epoch 123/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9647 - loss: 0.1124 - val_accuracy: 0.5833 - val_loss: 2.3859\n",
            "Epoch 124/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9480 - loss: 0.1522 - val_accuracy: 0.5660 - val_loss: 2.3842\n",
            "Epoch 125/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9650 - loss: 0.1052 - val_accuracy: 0.5903 - val_loss: 2.5384\n",
            "Epoch 126/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9673 - loss: 0.0806 - val_accuracy: 0.6181 - val_loss: 2.2966\n",
            "Epoch 127/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9512 - loss: 0.1704 - val_accuracy: 0.5556 - val_loss: 2.6266\n",
            "Epoch 128/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9480 - loss: 0.1493 - val_accuracy: 0.5833 - val_loss: 2.5386\n",
            "Epoch 129/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9755 - loss: 0.0823 - val_accuracy: 0.5764 - val_loss: 2.5408\n",
            "Epoch 130/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9686 - loss: 0.1004 - val_accuracy: 0.5764 - val_loss: 2.6079\n",
            "Epoch 131/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9635 - loss: 0.1097 - val_accuracy: 0.5764 - val_loss: 2.9069\n",
            "Epoch 132/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9535 - loss: 0.1650 - val_accuracy: 0.5833 - val_loss: 2.4878\n",
            "Epoch 133/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9504 - loss: 0.1380 - val_accuracy: 0.6007 - val_loss: 2.7546\n",
            "Epoch 134/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9753 - loss: 0.0795 - val_accuracy: 0.6007 - val_loss: 2.6237\n",
            "Epoch 135/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9468 - loss: 0.1755 - val_accuracy: 0.6042 - val_loss: 2.4403\n",
            "Epoch 136/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9690 - loss: 0.1052 - val_accuracy: 0.6111 - val_loss: 2.4999\n",
            "Epoch 137/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9724 - loss: 0.0921 - val_accuracy: 0.6007 - val_loss: 2.3025\n",
            "Epoch 138/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9775 - loss: 0.0847 - val_accuracy: 0.5660 - val_loss: 2.5701\n",
            "Epoch 139/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9551 - loss: 0.1168 - val_accuracy: 0.5903 - val_loss: 2.5500\n",
            "Epoch 140/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.9737 - loss: 0.0842 - val_accuracy: 0.5729 - val_loss: 2.5641\n",
            "Epoch 141/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.9677 - loss: 0.0962 - val_accuracy: 0.5660 - val_loss: 2.6766\n",
            "Epoch 142/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.9584 - loss: 0.1397 - val_accuracy: 0.5833 - val_loss: 2.5047\n",
            "Epoch 143/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.9645 - loss: 0.1059 - val_accuracy: 0.5729 - val_loss: 2.5998\n",
            "Epoch 144/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9568 - loss: 0.1252 - val_accuracy: 0.6042 - val_loss: 2.3255\n",
            "Epoch 145/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9569 - loss: 0.1267 - val_accuracy: 0.5903 - val_loss: 2.3952\n",
            "Epoch 146/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9685 - loss: 0.1070 - val_accuracy: 0.6076 - val_loss: 2.4267\n",
            "Epoch 147/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9676 - loss: 0.1102 - val_accuracy: 0.6076 - val_loss: 2.4070\n",
            "Epoch 148/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9564 - loss: 0.1267 - val_accuracy: 0.6285 - val_loss: 2.1760\n",
            "Epoch 149/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9618 - loss: 0.1169 - val_accuracy: 0.5938 - val_loss: 2.2794\n",
            "Epoch 150/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.9673 - loss: 0.1110 - val_accuracy: 0.6250 - val_loss: 2.1591\n",
            "Epoch 151/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9722 - loss: 0.0911 - val_accuracy: 0.6076 - val_loss: 2.2851\n",
            "Epoch 152/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9694 - loss: 0.0901 - val_accuracy: 0.6042 - val_loss: 2.1592\n",
            "Epoch 153/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9680 - loss: 0.0864 - val_accuracy: 0.6181 - val_loss: 2.2577\n",
            "Epoch 154/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9615 - loss: 0.1205 - val_accuracy: 0.5972 - val_loss: 2.4007\n",
            "Epoch 155/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9661 - loss: 0.0928 - val_accuracy: 0.6076 - val_loss: 2.3157\n",
            "Epoch 156/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9592 - loss: 0.1506 - val_accuracy: 0.6042 - val_loss: 2.2257\n",
            "Epoch 157/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9729 - loss: 0.0815 - val_accuracy: 0.5938 - val_loss: 2.1842\n",
            "Epoch 158/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9801 - loss: 0.0581 - val_accuracy: 0.6111 - val_loss: 2.2688\n",
            "Epoch 159/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.9697 - loss: 0.0773 - val_accuracy: 0.5833 - val_loss: 2.6595\n",
            "Epoch 160/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9761 - loss: 0.0833 - val_accuracy: 0.5833 - val_loss: 2.8524\n",
            "Epoch 161/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9768 - loss: 0.0918 - val_accuracy: 0.6111 - val_loss: 2.7025\n",
            "Epoch 162/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9685 - loss: 0.1026 - val_accuracy: 0.6285 - val_loss: 2.4745\n",
            "Epoch 163/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9780 - loss: 0.0656 - val_accuracy: 0.6250 - val_loss: 2.6230\n",
            "Epoch 164/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9644 - loss: 0.0964 - val_accuracy: 0.6250 - val_loss: 2.7389\n",
            "Epoch 165/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9818 - loss: 0.0564 - val_accuracy: 0.6007 - val_loss: 2.5744\n",
            "Epoch 166/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9680 - loss: 0.0993 - val_accuracy: 0.6076 - val_loss: 2.4022\n",
            "Epoch 167/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9685 - loss: 0.1305 - val_accuracy: 0.6076 - val_loss: 2.3084\n",
            "Epoch 168/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9707 - loss: 0.0893 - val_accuracy: 0.6424 - val_loss: 2.2040\n",
            "Epoch 169/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9811 - loss: 0.0645 - val_accuracy: 0.6354 - val_loss: 2.5665\n",
            "Epoch 170/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9723 - loss: 0.0712 - val_accuracy: 0.6250 - val_loss: 2.4176\n",
            "Epoch 171/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9511 - loss: 0.1623 - val_accuracy: 0.5903 - val_loss: 2.7351\n",
            "Epoch 172/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9658 - loss: 0.1065 - val_accuracy: 0.6042 - val_loss: 2.7427\n",
            "Epoch 173/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9692 - loss: 0.0957 - val_accuracy: 0.6111 - val_loss: 2.3753\n",
            "Epoch 174/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9807 - loss: 0.0698 - val_accuracy: 0.6215 - val_loss: 2.2446\n",
            "Epoch 175/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9619 - loss: 0.0965 - val_accuracy: 0.5938 - val_loss: 2.6813\n",
            "Epoch 176/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9628 - loss: 0.1274 - val_accuracy: 0.6285 - val_loss: 2.4030\n",
            "Epoch 177/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9567 - loss: 0.1130 - val_accuracy: 0.5868 - val_loss: 2.4962\n",
            "Epoch 178/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9759 - loss: 0.0699 - val_accuracy: 0.5972 - val_loss: 2.5387\n",
            "Epoch 179/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9705 - loss: 0.0832 - val_accuracy: 0.6042 - val_loss: 2.5819\n",
            "Epoch 180/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9605 - loss: 0.1474 - val_accuracy: 0.5729 - val_loss: 2.5432\n",
            "Epoch 181/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9711 - loss: 0.0716 - val_accuracy: 0.6250 - val_loss: 2.5427\n",
            "Epoch 182/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9819 - loss: 0.0487 - val_accuracy: 0.5938 - val_loss: 2.7609\n",
            "Epoch 183/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9703 - loss: 0.1027 - val_accuracy: 0.5590 - val_loss: 2.7662\n",
            "Epoch 184/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9676 - loss: 0.1150 - val_accuracy: 0.6007 - val_loss: 2.8324\n",
            "Epoch 185/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9636 - loss: 0.0970 - val_accuracy: 0.6076 - val_loss: 2.5171\n",
            "Epoch 186/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9853 - loss: 0.0593 - val_accuracy: 0.6111 - val_loss: 2.4792\n",
            "Epoch 187/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9649 - loss: 0.0955 - val_accuracy: 0.6285 - val_loss: 2.3621\n",
            "Epoch 188/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9723 - loss: 0.1102 - val_accuracy: 0.6319 - val_loss: 2.3204\n",
            "Epoch 189/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9785 - loss: 0.0780 - val_accuracy: 0.6076 - val_loss: 2.4211\n",
            "Epoch 190/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9676 - loss: 0.0912 - val_accuracy: 0.5868 - val_loss: 2.4054\n",
            "Epoch 191/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9721 - loss: 0.1058 - val_accuracy: 0.5833 - val_loss: 2.6028\n",
            "Epoch 192/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9721 - loss: 0.0891 - val_accuracy: 0.6181 - val_loss: 2.6017\n",
            "Epoch 193/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9731 - loss: 0.0949 - val_accuracy: 0.6076 - val_loss: 2.5219\n",
            "Epoch 194/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9752 - loss: 0.0809 - val_accuracy: 0.6389 - val_loss: 2.4221\n",
            "Epoch 195/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9813 - loss: 0.0500 - val_accuracy: 0.5972 - val_loss: 2.6464\n",
            "Epoch 196/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.9650 - loss: 0.1120 - val_accuracy: 0.5833 - val_loss: 2.6072\n",
            "Epoch 197/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9742 - loss: 0.0797 - val_accuracy: 0.5868 - val_loss: 2.4295\n",
            "Epoch 198/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9837 - loss: 0.0447 - val_accuracy: 0.6111 - val_loss: 2.6756\n",
            "Epoch 199/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9778 - loss: 0.0667 - val_accuracy: 0.5938 - val_loss: 2.3963\n",
            "Epoch 200/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9645 - loss: 0.1101 - val_accuracy: 0.5903 - val_loss: 2.4637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 68"
      ],
      "metadata": {
        "id": "Zo2f6D5zU-Sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Dense(390,activation='relu',input_dim=15))\n",
        "model.add(Dense(780,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(8,activation='softmax'))\n",
        "# Compile the model\n",
        "optimizer = RMSprop(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "qaC6dMbCUH27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=100,batch_size=32, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IX3gADeUSaH",
        "outputId": "fe3cf123-244a-465b-cd74-94b070c9a068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 68ms/step - accuracy: 0.1675 - loss: 2.7843 - val_accuracy: 0.1389 - val_loss: 2.0588\n",
            "Epoch 2/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.2841 - loss: 2.2466 - val_accuracy: 0.1389 - val_loss: 2.0556\n",
            "Epoch 3/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - accuracy: 0.3704 - loss: 1.8275 - val_accuracy: 0.1389 - val_loss: 2.0566\n",
            "Epoch 4/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - accuracy: 0.4060 - loss: 1.7211 - val_accuracy: 0.1493 - val_loss: 2.0535\n",
            "Epoch 5/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.4656 - loss: 1.5619 - val_accuracy: 0.1597 - val_loss: 2.0316\n",
            "Epoch 6/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.4826 - loss: 1.4961 - val_accuracy: 0.2118 - val_loss: 1.9779\n",
            "Epoch 7/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.5308 - loss: 1.2902 - val_accuracy: 0.2708 - val_loss: 1.8928\n",
            "Epoch 8/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.5546 - loss: 1.2572 - val_accuracy: 0.2917 - val_loss: 1.8310\n",
            "Epoch 9/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5704 - loss: 1.1892 - val_accuracy: 0.3472 - val_loss: 1.7390\n",
            "Epoch 10/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.5818 - loss: 1.2094 - val_accuracy: 0.3993 - val_loss: 1.6243\n",
            "Epoch 11/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.6224 - loss: 1.0713 - val_accuracy: 0.4340 - val_loss: 1.5755\n",
            "Epoch 12/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6330 - loss: 1.0461 - val_accuracy: 0.4722 - val_loss: 1.5069\n",
            "Epoch 13/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6806 - loss: 0.9483 - val_accuracy: 0.4826 - val_loss: 1.4310\n",
            "Epoch 14/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6709 - loss: 0.9234 - val_accuracy: 0.5000 - val_loss: 1.3864\n",
            "Epoch 15/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7062 - loss: 0.8415 - val_accuracy: 0.5243 - val_loss: 1.3087\n",
            "Epoch 16/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6980 - loss: 0.8496 - val_accuracy: 0.5104 - val_loss: 1.3295\n",
            "Epoch 17/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7201 - loss: 0.8000 - val_accuracy: 0.5451 - val_loss: 1.3194\n",
            "Epoch 18/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7445 - loss: 0.7287 - val_accuracy: 0.5694 - val_loss: 1.2703\n",
            "Epoch 19/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7771 - loss: 0.6540 - val_accuracy: 0.5625 - val_loss: 1.2797\n",
            "Epoch 20/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.7784 - loss: 0.6027 - val_accuracy: 0.5486 - val_loss: 1.2927\n",
            "Epoch 21/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.7429 - loss: 0.6812 - val_accuracy: 0.5556 - val_loss: 1.2726\n",
            "Epoch 22/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.8076 - loss: 0.6106 - val_accuracy: 0.5417 - val_loss: 1.2952\n",
            "Epoch 23/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.7938 - loss: 0.6247 - val_accuracy: 0.5729 - val_loss: 1.2451\n",
            "Epoch 24/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8008 - loss: 0.5559 - val_accuracy: 0.5799 - val_loss: 1.2402\n",
            "Epoch 25/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8329 - loss: 0.5097 - val_accuracy: 0.5833 - val_loss: 1.2269\n",
            "Epoch 26/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8366 - loss: 0.4964 - val_accuracy: 0.5660 - val_loss: 1.2434\n",
            "Epoch 27/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8091 - loss: 0.5188 - val_accuracy: 0.5729 - val_loss: 1.2614\n",
            "Epoch 28/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8540 - loss: 0.4290 - val_accuracy: 0.5764 - val_loss: 1.2552\n",
            "Epoch 29/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.8185 - loss: 0.5154 - val_accuracy: 0.5764 - val_loss: 1.2889\n",
            "Epoch 30/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8487 - loss: 0.4104 - val_accuracy: 0.5486 - val_loss: 1.3678\n",
            "Epoch 31/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.8346 - loss: 0.4418 - val_accuracy: 0.5521 - val_loss: 1.3820\n",
            "Epoch 32/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.8646 - loss: 0.3812 - val_accuracy: 0.5868 - val_loss: 1.2989\n",
            "Epoch 33/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.8753 - loss: 0.3845 - val_accuracy: 0.5590 - val_loss: 1.3950\n",
            "Epoch 34/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8713 - loss: 0.3539 - val_accuracy: 0.5694 - val_loss: 1.3507\n",
            "Epoch 35/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8861 - loss: 0.3200 - val_accuracy: 0.5938 - val_loss: 1.3421\n",
            "Epoch 36/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8816 - loss: 0.3136 - val_accuracy: 0.5938 - val_loss: 1.3738\n",
            "Epoch 37/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.8852 - loss: 0.3299 - val_accuracy: 0.6007 - val_loss: 1.3368\n",
            "Epoch 38/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8915 - loss: 0.3147 - val_accuracy: 0.5903 - val_loss: 1.3491\n",
            "Epoch 39/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9231 - loss: 0.2582 - val_accuracy: 0.6076 - val_loss: 1.3878\n",
            "Epoch 40/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9033 - loss: 0.2791 - val_accuracy: 0.6042 - val_loss: 1.3736\n",
            "Epoch 41/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9214 - loss: 0.2530 - val_accuracy: 0.5903 - val_loss: 1.4415\n",
            "Epoch 42/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9303 - loss: 0.2089 - val_accuracy: 0.5938 - val_loss: 1.3660\n",
            "Epoch 43/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9051 - loss: 0.2594 - val_accuracy: 0.5764 - val_loss: 1.4337\n",
            "Epoch 44/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9078 - loss: 0.3047 - val_accuracy: 0.5799 - val_loss: 1.3746\n",
            "Epoch 45/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.9216 - loss: 0.2335 - val_accuracy: 0.5660 - val_loss: 1.3913\n",
            "Epoch 46/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9285 - loss: 0.2363 - val_accuracy: 0.5972 - val_loss: 1.4324\n",
            "Epoch 47/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9167 - loss: 0.2446 - val_accuracy: 0.5660 - val_loss: 1.4877\n",
            "Epoch 48/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9396 - loss: 0.1961 - val_accuracy: 0.5868 - val_loss: 1.4434\n",
            "Epoch 49/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9330 - loss: 0.1951 - val_accuracy: 0.6042 - val_loss: 1.4585\n",
            "Epoch 50/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9266 - loss: 0.2203 - val_accuracy: 0.5660 - val_loss: 1.4978\n",
            "Epoch 51/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9485 - loss: 0.1800 - val_accuracy: 0.5799 - val_loss: 1.4772\n",
            "Epoch 52/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9415 - loss: 0.1607 - val_accuracy: 0.6215 - val_loss: 1.4161\n",
            "Epoch 53/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9423 - loss: 0.1790 - val_accuracy: 0.6146 - val_loss: 1.4462\n",
            "Epoch 54/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9489 - loss: 0.1646 - val_accuracy: 0.5972 - val_loss: 1.4767\n",
            "Epoch 55/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9520 - loss: 0.1424 - val_accuracy: 0.5660 - val_loss: 1.5208\n",
            "Epoch 56/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9590 - loss: 0.1354 - val_accuracy: 0.5764 - val_loss: 1.6103\n",
            "Epoch 57/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9530 - loss: 0.1537 - val_accuracy: 0.5972 - val_loss: 1.5657\n",
            "Epoch 58/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9433 - loss: 0.1601 - val_accuracy: 0.5868 - val_loss: 1.5446\n",
            "Epoch 59/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.9476 - loss: 0.1692 - val_accuracy: 0.6076 - val_loss: 1.6223\n",
            "Epoch 60/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.9453 - loss: 0.1494 - val_accuracy: 0.5799 - val_loss: 1.6339\n",
            "Epoch 61/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9451 - loss: 0.1508 - val_accuracy: 0.5903 - val_loss: 1.6350\n",
            "Epoch 62/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9398 - loss: 0.1729 - val_accuracy: 0.6042 - val_loss: 1.5471\n",
            "Epoch 63/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9644 - loss: 0.1285 - val_accuracy: 0.5903 - val_loss: 1.6196\n",
            "Epoch 64/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9702 - loss: 0.1016 - val_accuracy: 0.5972 - val_loss: 1.6517\n",
            "Epoch 65/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9591 - loss: 0.1436 - val_accuracy: 0.5972 - val_loss: 1.5911\n",
            "Epoch 66/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.9564 - loss: 0.1271 - val_accuracy: 0.6111 - val_loss: 1.6900\n",
            "Epoch 67/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9501 - loss: 0.1385 - val_accuracy: 0.5868 - val_loss: 1.6814\n",
            "Epoch 68/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9604 - loss: 0.1265 - val_accuracy: 0.5833 - val_loss: 1.6983\n",
            "Epoch 69/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9532 - loss: 0.1335 - val_accuracy: 0.5799 - val_loss: 1.7414\n",
            "Epoch 70/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9648 - loss: 0.1060 - val_accuracy: 0.5764 - val_loss: 1.6839\n",
            "Epoch 71/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9714 - loss: 0.1002 - val_accuracy: 0.5764 - val_loss: 1.7529\n",
            "Epoch 72/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9751 - loss: 0.0955 - val_accuracy: 0.5833 - val_loss: 1.7161\n",
            "Epoch 73/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.9800 - loss: 0.0957 - val_accuracy: 0.6007 - val_loss: 1.7284\n",
            "Epoch 74/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9668 - loss: 0.1046 - val_accuracy: 0.6007 - val_loss: 1.7570\n",
            "Epoch 75/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9708 - loss: 0.0877 - val_accuracy: 0.6076 - val_loss: 1.6962\n",
            "Epoch 76/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9650 - loss: 0.0975 - val_accuracy: 0.5972 - val_loss: 1.7341\n",
            "Epoch 77/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9708 - loss: 0.1002 - val_accuracy: 0.5625 - val_loss: 1.8543\n",
            "Epoch 78/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9753 - loss: 0.0843 - val_accuracy: 0.5729 - val_loss: 1.8224\n",
            "Epoch 79/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9660 - loss: 0.1009 - val_accuracy: 0.5868 - val_loss: 1.7047\n",
            "Epoch 80/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9602 - loss: 0.1037 - val_accuracy: 0.5972 - val_loss: 1.7670\n",
            "Epoch 81/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9774 - loss: 0.0698 - val_accuracy: 0.6146 - val_loss: 1.6844\n",
            "Epoch 82/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9724 - loss: 0.0971 - val_accuracy: 0.6076 - val_loss: 1.7505\n",
            "Epoch 83/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9811 - loss: 0.0571 - val_accuracy: 0.5764 - val_loss: 1.8084\n",
            "Epoch 84/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.9797 - loss: 0.0783 - val_accuracy: 0.6146 - val_loss: 1.7373\n",
            "Epoch 85/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9674 - loss: 0.0930 - val_accuracy: 0.6111 - val_loss: 1.7387\n",
            "Epoch 86/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9774 - loss: 0.0681 - val_accuracy: 0.6076 - val_loss: 1.7900\n",
            "Epoch 87/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9842 - loss: 0.0518 - val_accuracy: 0.5833 - val_loss: 1.7764\n",
            "Epoch 88/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9791 - loss: 0.0659 - val_accuracy: 0.6076 - val_loss: 1.8679\n",
            "Epoch 89/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9781 - loss: 0.0846 - val_accuracy: 0.5972 - val_loss: 1.7945\n",
            "Epoch 90/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9784 - loss: 0.0703 - val_accuracy: 0.6181 - val_loss: 1.7920\n",
            "Epoch 91/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9795 - loss: 0.0695 - val_accuracy: 0.6285 - val_loss: 1.7637\n",
            "Epoch 92/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9824 - loss: 0.0503 - val_accuracy: 0.6076 - val_loss: 1.8579\n",
            "Epoch 93/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9748 - loss: 0.0819 - val_accuracy: 0.6319 - val_loss: 1.8401\n",
            "Epoch 94/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9775 - loss: 0.0794 - val_accuracy: 0.6076 - val_loss: 1.8322\n",
            "Epoch 95/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9703 - loss: 0.0840 - val_accuracy: 0.6354 - val_loss: 1.7790\n",
            "Epoch 96/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9830 - loss: 0.0495 - val_accuracy: 0.6042 - val_loss: 1.8250\n",
            "Epoch 97/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.9808 - loss: 0.0732 - val_accuracy: 0.6076 - val_loss: 1.8868\n",
            "Epoch 98/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9742 - loss: 0.0709 - val_accuracy: 0.5764 - val_loss: 1.9365\n",
            "Epoch 99/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9729 - loss: 0.0696 - val_accuracy: 0.5972 - val_loss: 1.8925\n",
            "Epoch 100/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9823 - loss: 0.0549 - val_accuracy: 0.5938 - val_loss: 1.9065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 72"
      ],
      "metadata": {
        "id": "HjVBLdSmW9xA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Dense(390,activation='relu',input_dim=15))\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(390,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(8,activation='softmax'))\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "E9RJFvRZVDaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=200,batch_size=64, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqutPVNWVIDN",
        "outputId": "e0233ee1-b11e-4142-cf69-9ffdf931765e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.1310 - loss: 2.9904 - val_accuracy: 0.1875 - val_loss: 2.0596\n",
            "Epoch 2/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.2628 - loss: 2.2985 - val_accuracy: 0.2118 - val_loss: 2.0518\n",
            "Epoch 3/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.2890 - loss: 2.0390 - val_accuracy: 0.2049 - val_loss: 2.0465\n",
            "Epoch 4/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.3471 - loss: 1.9379 - val_accuracy: 0.2083 - val_loss: 2.0377\n",
            "Epoch 5/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.3990 - loss: 1.7825 - val_accuracy: 0.2118 - val_loss: 2.0275\n",
            "Epoch 6/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.4417 - loss: 1.6255 - val_accuracy: 0.2188 - val_loss: 2.0163\n",
            "Epoch 7/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4492 - loss: 1.5924 - val_accuracy: 0.2153 - val_loss: 1.9924\n",
            "Epoch 8/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4825 - loss: 1.4341 - val_accuracy: 0.2257 - val_loss: 1.9654\n",
            "Epoch 9/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.4958 - loss: 1.4064 - val_accuracy: 0.2326 - val_loss: 1.9448\n",
            "Epoch 10/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5281 - loss: 1.3487 - val_accuracy: 0.2361 - val_loss: 1.9201\n",
            "Epoch 11/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.5428 - loss: 1.2937 - val_accuracy: 0.2500 - val_loss: 1.8809\n",
            "Epoch 12/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5975 - loss: 1.1701 - val_accuracy: 0.2569 - val_loss: 1.8437\n",
            "Epoch 13/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5507 - loss: 1.2023 - val_accuracy: 0.2778 - val_loss: 1.8063\n",
            "Epoch 14/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6187 - loss: 1.0457 - val_accuracy: 0.2882 - val_loss: 1.7692\n",
            "Epoch 15/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6318 - loss: 1.0388 - val_accuracy: 0.3021 - val_loss: 1.7312\n",
            "Epoch 16/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6225 - loss: 1.0370 - val_accuracy: 0.3438 - val_loss: 1.6952\n",
            "Epoch 17/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6347 - loss: 1.0397 - val_accuracy: 0.3507 - val_loss: 1.6514\n",
            "Epoch 18/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6516 - loss: 0.9550 - val_accuracy: 0.3854 - val_loss: 1.5962\n",
            "Epoch 19/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.6673 - loss: 0.9182 - val_accuracy: 0.4062 - val_loss: 1.5355\n",
            "Epoch 20/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.6857 - loss: 0.9065 - val_accuracy: 0.4340 - val_loss: 1.4986\n",
            "Epoch 21/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6871 - loss: 0.8792 - val_accuracy: 0.4410 - val_loss: 1.4605\n",
            "Epoch 22/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6697 - loss: 0.9301 - val_accuracy: 0.4514 - val_loss: 1.4224\n",
            "Epoch 23/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.6944 - loss: 0.8929 - val_accuracy: 0.4653 - val_loss: 1.3956\n",
            "Epoch 24/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.7228 - loss: 0.7516 - val_accuracy: 0.4653 - val_loss: 1.3783\n",
            "Epoch 25/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7238 - loss: 0.7687 - val_accuracy: 0.4792 - val_loss: 1.3503\n",
            "Epoch 26/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7048 - loss: 0.7897 - val_accuracy: 0.5139 - val_loss: 1.3269\n",
            "Epoch 27/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.7344 - loss: 0.7326 - val_accuracy: 0.4861 - val_loss: 1.3247\n",
            "Epoch 28/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7622 - loss: 0.6934 - val_accuracy: 0.5139 - val_loss: 1.3113\n",
            "Epoch 29/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7513 - loss: 0.6570 - val_accuracy: 0.5243 - val_loss: 1.2868\n",
            "Epoch 30/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7810 - loss: 0.6671 - val_accuracy: 0.5278 - val_loss: 1.2672\n",
            "Epoch 31/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7903 - loss: 0.6167 - val_accuracy: 0.5278 - val_loss: 1.2520\n",
            "Epoch 32/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7788 - loss: 0.5956 - val_accuracy: 0.5278 - val_loss: 1.2454\n",
            "Epoch 33/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7798 - loss: 0.6333 - val_accuracy: 0.5451 - val_loss: 1.2331\n",
            "Epoch 34/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7938 - loss: 0.5995 - val_accuracy: 0.5486 - val_loss: 1.2268\n",
            "Epoch 35/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8122 - loss: 0.5735 - val_accuracy: 0.5521 - val_loss: 1.2328\n",
            "Epoch 36/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7983 - loss: 0.5783 - val_accuracy: 0.5590 - val_loss: 1.2480\n",
            "Epoch 37/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8160 - loss: 0.5479 - val_accuracy: 0.5590 - val_loss: 1.2511\n",
            "Epoch 38/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7967 - loss: 0.5348 - val_accuracy: 0.5799 - val_loss: 1.2426\n",
            "Epoch 39/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.8223 - loss: 0.5087 - val_accuracy: 0.5556 - val_loss: 1.2687\n",
            "Epoch 40/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.8258 - loss: 0.4777 - val_accuracy: 0.5521 - val_loss: 1.2814\n",
            "Epoch 41/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8229 - loss: 0.4952 - val_accuracy: 0.5660 - val_loss: 1.2888\n",
            "Epoch 42/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8510 - loss: 0.4609 - val_accuracy: 0.5729 - val_loss: 1.2967\n",
            "Epoch 43/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8521 - loss: 0.4370 - val_accuracy: 0.5417 - val_loss: 1.3065\n",
            "Epoch 44/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8675 - loss: 0.4027 - val_accuracy: 0.5625 - val_loss: 1.3241\n",
            "Epoch 45/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8649 - loss: 0.4077 - val_accuracy: 0.5625 - val_loss: 1.3192\n",
            "Epoch 46/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8564 - loss: 0.3921 - val_accuracy: 0.5590 - val_loss: 1.3198\n",
            "Epoch 47/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.8910 - loss: 0.3463 - val_accuracy: 0.5694 - val_loss: 1.3343\n",
            "Epoch 48/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.8604 - loss: 0.3896 - val_accuracy: 0.5799 - val_loss: 1.3388\n",
            "Epoch 49/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.8747 - loss: 0.3564 - val_accuracy: 0.5799 - val_loss: 1.3469\n",
            "Epoch 50/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.8940 - loss: 0.3399 - val_accuracy: 0.5590 - val_loss: 1.3556\n",
            "Epoch 51/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.8827 - loss: 0.3501 - val_accuracy: 0.5625 - val_loss: 1.3597\n",
            "Epoch 52/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8759 - loss: 0.3588 - val_accuracy: 0.5799 - val_loss: 1.3451\n",
            "Epoch 53/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.8906 - loss: 0.3197 - val_accuracy: 0.5764 - val_loss: 1.3495\n",
            "Epoch 54/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8883 - loss: 0.3508 - val_accuracy: 0.5660 - val_loss: 1.3471\n",
            "Epoch 55/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.8821 - loss: 0.3153 - val_accuracy: 0.5764 - val_loss: 1.3466\n",
            "Epoch 56/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8844 - loss: 0.2992 - val_accuracy: 0.5799 - val_loss: 1.3293\n",
            "Epoch 57/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.8991 - loss: 0.2963 - val_accuracy: 0.5694 - val_loss: 1.3414\n",
            "Epoch 58/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8838 - loss: 0.3136 - val_accuracy: 0.5694 - val_loss: 1.3513\n",
            "Epoch 59/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8889 - loss: 0.3196 - val_accuracy: 0.5660 - val_loss: 1.3452\n",
            "Epoch 60/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8952 - loss: 0.2913 - val_accuracy: 0.5729 - val_loss: 1.3726\n",
            "Epoch 61/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9246 - loss: 0.2479 - val_accuracy: 0.5694 - val_loss: 1.3737\n",
            "Epoch 62/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8948 - loss: 0.2997 - val_accuracy: 0.5729 - val_loss: 1.3839\n",
            "Epoch 63/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9269 - loss: 0.2372 - val_accuracy: 0.5799 - val_loss: 1.3957\n",
            "Epoch 64/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9308 - loss: 0.2231 - val_accuracy: 0.5799 - val_loss: 1.3993\n",
            "Epoch 65/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9242 - loss: 0.2330 - val_accuracy: 0.5764 - val_loss: 1.4099\n",
            "Epoch 66/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9370 - loss: 0.2240 - val_accuracy: 0.5833 - val_loss: 1.4184\n",
            "Epoch 67/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9187 - loss: 0.2345 - val_accuracy: 0.5694 - val_loss: 1.4263\n",
            "Epoch 68/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9217 - loss: 0.2342 - val_accuracy: 0.5660 - val_loss: 1.3957\n",
            "Epoch 69/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9159 - loss: 0.2471 - val_accuracy: 0.5660 - val_loss: 1.4165\n",
            "Epoch 70/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9277 - loss: 0.2116 - val_accuracy: 0.5729 - val_loss: 1.3999\n",
            "Epoch 71/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9330 - loss: 0.2046 - val_accuracy: 0.5868 - val_loss: 1.4211\n",
            "Epoch 72/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9283 - loss: 0.2111 - val_accuracy: 0.5660 - val_loss: 1.4476\n",
            "Epoch 73/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.9396 - loss: 0.1687 - val_accuracy: 0.5799 - val_loss: 1.4476\n",
            "Epoch 74/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9555 - loss: 0.1635 - val_accuracy: 0.5903 - val_loss: 1.4202\n",
            "Epoch 75/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9429 - loss: 0.1813 - val_accuracy: 0.5799 - val_loss: 1.4283\n",
            "Epoch 76/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9439 - loss: 0.1840 - val_accuracy: 0.5903 - val_loss: 1.4334\n",
            "Epoch 77/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9444 - loss: 0.1854 - val_accuracy: 0.5833 - val_loss: 1.4245\n",
            "Epoch 78/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9369 - loss: 0.1823 - val_accuracy: 0.5799 - val_loss: 1.4416\n",
            "Epoch 79/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9417 - loss: 0.1759 - val_accuracy: 0.5764 - val_loss: 1.4546\n",
            "Epoch 80/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9352 - loss: 0.2071 - val_accuracy: 0.5833 - val_loss: 1.4523\n",
            "Epoch 81/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9414 - loss: 0.1701 - val_accuracy: 0.5868 - val_loss: 1.4553\n",
            "Epoch 82/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9413 - loss: 0.1763 - val_accuracy: 0.5799 - val_loss: 1.4749\n",
            "Epoch 83/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9604 - loss: 0.1371 - val_accuracy: 0.5694 - val_loss: 1.4998\n",
            "Epoch 84/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9468 - loss: 0.1759 - val_accuracy: 0.5660 - val_loss: 1.5054\n",
            "Epoch 85/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9523 - loss: 0.1592 - val_accuracy: 0.5660 - val_loss: 1.4780\n",
            "Epoch 86/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9694 - loss: 0.1203 - val_accuracy: 0.5868 - val_loss: 1.4419\n",
            "Epoch 87/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9545 - loss: 0.1388 - val_accuracy: 0.5903 - val_loss: 1.4718\n",
            "Epoch 88/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9652 - loss: 0.1184 - val_accuracy: 0.6076 - val_loss: 1.4993\n",
            "Epoch 89/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9650 - loss: 0.1187 - val_accuracy: 0.5903 - val_loss: 1.4612\n",
            "Epoch 90/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9665 - loss: 0.1306 - val_accuracy: 0.5868 - val_loss: 1.4743\n",
            "Epoch 91/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9737 - loss: 0.1022 - val_accuracy: 0.5694 - val_loss: 1.4720\n",
            "Epoch 92/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9569 - loss: 0.1323 - val_accuracy: 0.5660 - val_loss: 1.4743\n",
            "Epoch 93/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9681 - loss: 0.1183 - val_accuracy: 0.5764 - val_loss: 1.5289\n",
            "Epoch 94/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9625 - loss: 0.1257 - val_accuracy: 0.5660 - val_loss: 1.5203\n",
            "Epoch 95/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9698 - loss: 0.1141 - val_accuracy: 0.5764 - val_loss: 1.5324\n",
            "Epoch 96/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9518 - loss: 0.1364 - val_accuracy: 0.5903 - val_loss: 1.5197\n",
            "Epoch 97/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9499 - loss: 0.1258 - val_accuracy: 0.5903 - val_loss: 1.5241\n",
            "Epoch 98/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9609 - loss: 0.1346 - val_accuracy: 0.5903 - val_loss: 1.5186\n",
            "Epoch 99/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9712 - loss: 0.0959 - val_accuracy: 0.5833 - val_loss: 1.4990\n",
            "Epoch 100/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9598 - loss: 0.1137 - val_accuracy: 0.5729 - val_loss: 1.5212\n",
            "Epoch 101/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9747 - loss: 0.0918 - val_accuracy: 0.5799 - val_loss: 1.5553\n",
            "Epoch 102/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9611 - loss: 0.1259 - val_accuracy: 0.5694 - val_loss: 1.5755\n",
            "Epoch 103/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9676 - loss: 0.1120 - val_accuracy: 0.5799 - val_loss: 1.5711\n",
            "Epoch 104/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9775 - loss: 0.0930 - val_accuracy: 0.5833 - val_loss: 1.5717\n",
            "Epoch 105/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9734 - loss: 0.1026 - val_accuracy: 0.5903 - val_loss: 1.5502\n",
            "Epoch 106/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9582 - loss: 0.1123 - val_accuracy: 0.5938 - val_loss: 1.5502\n",
            "Epoch 107/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9791 - loss: 0.0892 - val_accuracy: 0.5903 - val_loss: 1.5763\n",
            "Epoch 108/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9768 - loss: 0.0867 - val_accuracy: 0.5868 - val_loss: 1.5857\n",
            "Epoch 109/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9716 - loss: 0.0937 - val_accuracy: 0.5868 - val_loss: 1.6250\n",
            "Epoch 110/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9679 - loss: 0.0941 - val_accuracy: 0.5868 - val_loss: 1.6271\n",
            "Epoch 111/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9811 - loss: 0.0749 - val_accuracy: 0.5799 - val_loss: 1.5791\n",
            "Epoch 112/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9560 - loss: 0.1056 - val_accuracy: 0.5764 - val_loss: 1.6059\n",
            "Epoch 113/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9663 - loss: 0.0984 - val_accuracy: 0.5903 - val_loss: 1.6399\n",
            "Epoch 114/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9746 - loss: 0.0884 - val_accuracy: 0.5868 - val_loss: 1.6184\n",
            "Epoch 115/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9771 - loss: 0.0770 - val_accuracy: 0.5938 - val_loss: 1.6550\n",
            "Epoch 116/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9884 - loss: 0.0686 - val_accuracy: 0.5903 - val_loss: 1.6312\n",
            "Epoch 117/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9773 - loss: 0.0922 - val_accuracy: 0.5729 - val_loss: 1.6645\n",
            "Epoch 118/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9757 - loss: 0.1113 - val_accuracy: 0.5729 - val_loss: 1.6659\n",
            "Epoch 119/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9764 - loss: 0.0845 - val_accuracy: 0.5833 - val_loss: 1.6498\n",
            "Epoch 120/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9799 - loss: 0.0698 - val_accuracy: 0.5660 - val_loss: 1.6864\n",
            "Epoch 121/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9759 - loss: 0.0841 - val_accuracy: 0.5868 - val_loss: 1.7021\n",
            "Epoch 122/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9680 - loss: 0.0970 - val_accuracy: 0.5833 - val_loss: 1.6820\n",
            "Epoch 123/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9831 - loss: 0.0734 - val_accuracy: 0.5868 - val_loss: 1.6488\n",
            "Epoch 124/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9801 - loss: 0.0767 - val_accuracy: 0.5764 - val_loss: 1.6806\n",
            "Epoch 125/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9825 - loss: 0.0649 - val_accuracy: 0.5729 - val_loss: 1.6800\n",
            "Epoch 126/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9777 - loss: 0.0771 - val_accuracy: 0.5764 - val_loss: 1.6260\n",
            "Epoch 127/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9665 - loss: 0.0896 - val_accuracy: 0.5903 - val_loss: 1.6521\n",
            "Epoch 128/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9854 - loss: 0.0560 - val_accuracy: 0.5903 - val_loss: 1.6597\n",
            "Epoch 129/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9811 - loss: 0.0666 - val_accuracy: 0.5694 - val_loss: 1.6901\n",
            "Epoch 130/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9856 - loss: 0.0556 - val_accuracy: 0.5590 - val_loss: 1.7244\n",
            "Epoch 131/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9714 - loss: 0.0728 - val_accuracy: 0.5764 - val_loss: 1.7130\n",
            "Epoch 132/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9812 - loss: 0.0652 - val_accuracy: 0.5764 - val_loss: 1.7066\n",
            "Epoch 133/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9749 - loss: 0.0791 - val_accuracy: 0.5903 - val_loss: 1.7209\n",
            "Epoch 134/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9830 - loss: 0.0653 - val_accuracy: 0.5799 - val_loss: 1.7351\n",
            "Epoch 135/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9879 - loss: 0.0541 - val_accuracy: 0.5868 - val_loss: 1.7196\n",
            "Epoch 136/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9834 - loss: 0.0508 - val_accuracy: 0.5729 - val_loss: 1.7652\n",
            "Epoch 137/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9856 - loss: 0.0658 - val_accuracy: 0.5903 - val_loss: 1.7346\n",
            "Epoch 138/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9864 - loss: 0.0513 - val_accuracy: 0.5903 - val_loss: 1.7170\n",
            "Epoch 139/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9854 - loss: 0.0508 - val_accuracy: 0.5972 - val_loss: 1.7167\n",
            "Epoch 140/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9827 - loss: 0.0643 - val_accuracy: 0.5938 - val_loss: 1.7362\n",
            "Epoch 141/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9787 - loss: 0.0685 - val_accuracy: 0.5903 - val_loss: 1.7506\n",
            "Epoch 142/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9785 - loss: 0.0577 - val_accuracy: 0.5903 - val_loss: 1.7278\n",
            "Epoch 143/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9831 - loss: 0.0551 - val_accuracy: 0.5938 - val_loss: 1.7762\n",
            "Epoch 144/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9853 - loss: 0.0551 - val_accuracy: 0.5938 - val_loss: 1.7567\n",
            "Epoch 145/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9877 - loss: 0.0482 - val_accuracy: 0.5903 - val_loss: 1.7856\n",
            "Epoch 146/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9861 - loss: 0.0558 - val_accuracy: 0.6111 - val_loss: 1.7481\n",
            "Epoch 147/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9884 - loss: 0.0484 - val_accuracy: 0.6007 - val_loss: 1.7914\n",
            "Epoch 148/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9812 - loss: 0.0590 - val_accuracy: 0.6007 - val_loss: 1.8863\n",
            "Epoch 149/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9842 - loss: 0.0552 - val_accuracy: 0.6007 - val_loss: 1.8771\n",
            "Epoch 150/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9772 - loss: 0.0764 - val_accuracy: 0.6076 - val_loss: 1.8109\n",
            "Epoch 151/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9858 - loss: 0.0617 - val_accuracy: 0.5903 - val_loss: 1.7957\n",
            "Epoch 152/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9930 - loss: 0.0369 - val_accuracy: 0.5833 - val_loss: 1.7777\n",
            "Epoch 153/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9864 - loss: 0.0483 - val_accuracy: 0.5833 - val_loss: 1.7922\n",
            "Epoch 154/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9810 - loss: 0.0610 - val_accuracy: 0.5764 - val_loss: 1.8138\n",
            "Epoch 155/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9897 - loss: 0.0392 - val_accuracy: 0.5903 - val_loss: 1.8708\n",
            "Epoch 156/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9858 - loss: 0.0478 - val_accuracy: 0.5764 - val_loss: 1.8742\n",
            "Epoch 157/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9866 - loss: 0.0472 - val_accuracy: 0.6007 - val_loss: 1.8385\n",
            "Epoch 158/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.9831 - loss: 0.0649 - val_accuracy: 0.5694 - val_loss: 1.8952\n",
            "Epoch 159/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9908 - loss: 0.0378 - val_accuracy: 0.5972 - val_loss: 1.9039\n",
            "Epoch 160/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9857 - loss: 0.0445 - val_accuracy: 0.5764 - val_loss: 1.9110\n",
            "Epoch 161/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9720 - loss: 0.0711 - val_accuracy: 0.5868 - val_loss: 1.9102\n",
            "Epoch 162/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9846 - loss: 0.0476 - val_accuracy: 0.5764 - val_loss: 1.9027\n",
            "Epoch 163/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9865 - loss: 0.0430 - val_accuracy: 0.5833 - val_loss: 1.8833\n",
            "Epoch 164/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9938 - loss: 0.0367 - val_accuracy: 0.5868 - val_loss: 1.8716\n",
            "Epoch 165/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9902 - loss: 0.0409 - val_accuracy: 0.5799 - val_loss: 1.9135\n",
            "Epoch 166/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9886 - loss: 0.0348 - val_accuracy: 0.5799 - val_loss: 1.9117\n",
            "Epoch 167/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9854 - loss: 0.0472 - val_accuracy: 0.5799 - val_loss: 1.9119\n",
            "Epoch 168/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9946 - loss: 0.0363 - val_accuracy: 0.5868 - val_loss: 1.8919\n",
            "Epoch 169/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9896 - loss: 0.0356 - val_accuracy: 0.5764 - val_loss: 1.8608\n",
            "Epoch 170/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9930 - loss: 0.0389 - val_accuracy: 0.5799 - val_loss: 1.9230\n",
            "Epoch 171/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9850 - loss: 0.0465 - val_accuracy: 0.5694 - val_loss: 1.8691\n",
            "Epoch 172/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9892 - loss: 0.0452 - val_accuracy: 0.6007 - val_loss: 1.8472\n",
            "Epoch 173/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9873 - loss: 0.0436 - val_accuracy: 0.5868 - val_loss: 1.8715\n",
            "Epoch 174/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9901 - loss: 0.0399 - val_accuracy: 0.5833 - val_loss: 1.8707\n",
            "Epoch 175/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9919 - loss: 0.0323 - val_accuracy: 0.5903 - val_loss: 1.8254\n",
            "Epoch 176/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9827 - loss: 0.0442 - val_accuracy: 0.5868 - val_loss: 1.8166\n",
            "Epoch 177/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9845 - loss: 0.0467 - val_accuracy: 0.6007 - val_loss: 1.8405\n",
            "Epoch 178/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.9796 - loss: 0.0592 - val_accuracy: 0.5903 - val_loss: 1.9071\n",
            "Epoch 179/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9952 - loss: 0.0265 - val_accuracy: 0.6042 - val_loss: 1.8717\n",
            "Epoch 180/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9899 - loss: 0.0338 - val_accuracy: 0.6111 - val_loss: 1.8866\n",
            "Epoch 181/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9887 - loss: 0.0414 - val_accuracy: 0.5972 - val_loss: 1.8746\n",
            "Epoch 182/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9891 - loss: 0.0398 - val_accuracy: 0.6076 - val_loss: 1.8584\n",
            "Epoch 183/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9845 - loss: 0.0467 - val_accuracy: 0.5972 - val_loss: 1.8746\n",
            "Epoch 184/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9869 - loss: 0.0380 - val_accuracy: 0.5799 - val_loss: 1.9050\n",
            "Epoch 185/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9891 - loss: 0.0345 - val_accuracy: 0.5903 - val_loss: 1.9347\n",
            "Epoch 186/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9892 - loss: 0.0305 - val_accuracy: 0.5938 - val_loss: 1.9187\n",
            "Epoch 187/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9867 - loss: 0.0386 - val_accuracy: 0.5972 - val_loss: 1.8920\n",
            "Epoch 188/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9858 - loss: 0.0357 - val_accuracy: 0.5938 - val_loss: 1.9480\n",
            "Epoch 189/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9934 - loss: 0.0257 - val_accuracy: 0.5868 - val_loss: 1.9161\n",
            "Epoch 190/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9938 - loss: 0.0325 - val_accuracy: 0.5903 - val_loss: 1.9431\n",
            "Epoch 191/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9988 - loss: 0.0179 - val_accuracy: 0.5903 - val_loss: 1.9506\n",
            "Epoch 192/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9974 - loss: 0.0230 - val_accuracy: 0.5868 - val_loss: 1.9410\n",
            "Epoch 193/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9874 - loss: 0.0414 - val_accuracy: 0.6007 - val_loss: 1.9631\n",
            "Epoch 194/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9965 - loss: 0.0281 - val_accuracy: 0.6042 - val_loss: 1.9522\n",
            "Epoch 195/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9960 - loss: 0.0237 - val_accuracy: 0.5938 - val_loss: 1.9596\n",
            "Epoch 196/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9945 - loss: 0.0262 - val_accuracy: 0.5903 - val_loss: 1.9723\n",
            "Epoch 197/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9914 - loss: 0.0352 - val_accuracy: 0.6042 - val_loss: 2.0021\n",
            "Epoch 198/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9895 - loss: 0.0337 - val_accuracy: 0.5972 - val_loss: 2.0374\n",
            "Epoch 199/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9796 - loss: 0.0509 - val_accuracy: 0.6007 - val_loss: 2.0091\n",
            "Epoch 200/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9884 - loss: 0.0344 - val_accuracy: 0.6042 - val_loss: 1.9786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Dense(390,activation='relu',input_dim=13))\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(390,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(8,activation='softmax'))\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "oud1r_V9XFM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=200,batch_size=64, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgeDtLiXXKNH",
        "outputId": "26c872d9-f4e2-4b1e-c6f6-31f5d5a6d5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - accuracy: 0.1478 - loss: 3.0487 - val_accuracy: 0.2014 - val_loss: 2.0523\n",
            "Epoch 2/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1751 - loss: 2.7647 - val_accuracy: 0.2083 - val_loss: 2.0393\n",
            "Epoch 3/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2294 - loss: 2.3136 - val_accuracy: 0.2153 - val_loss: 2.0289\n",
            "Epoch 4/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2654 - loss: 2.2212 - val_accuracy: 0.2361 - val_loss: 2.0165\n",
            "Epoch 5/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2792 - loss: 2.1893 - val_accuracy: 0.2465 - val_loss: 2.0078\n",
            "Epoch 6/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2827 - loss: 2.1120 - val_accuracy: 0.1979 - val_loss: 1.9979\n",
            "Epoch 7/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3271 - loss: 1.9307 - val_accuracy: 0.2083 - val_loss: 1.9873\n",
            "Epoch 8/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3825 - loss: 1.9076 - val_accuracy: 0.2188 - val_loss: 1.9757\n",
            "Epoch 9/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3774 - loss: 1.8064 - val_accuracy: 0.2361 - val_loss: 1.9549\n",
            "Epoch 10/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3885 - loss: 1.7216 - val_accuracy: 0.2500 - val_loss: 1.9277\n",
            "Epoch 11/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3718 - loss: 1.7891 - val_accuracy: 0.2639 - val_loss: 1.9041\n",
            "Epoch 12/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4119 - loss: 1.6915 - val_accuracy: 0.2674 - val_loss: 1.8778\n",
            "Epoch 13/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4520 - loss: 1.6648 - val_accuracy: 0.2674 - val_loss: 1.8477\n",
            "Epoch 14/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4533 - loss: 1.5227 - val_accuracy: 0.3021 - val_loss: 1.8035\n",
            "Epoch 15/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4729 - loss: 1.4818 - val_accuracy: 0.3160 - val_loss: 1.7662\n",
            "Epoch 16/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4858 - loss: 1.4645 - val_accuracy: 0.3368 - val_loss: 1.7335\n",
            "Epoch 17/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5134 - loss: 1.4202 - val_accuracy: 0.3681 - val_loss: 1.6889\n",
            "Epoch 18/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5047 - loss: 1.4123 - val_accuracy: 0.3785 - val_loss: 1.6459\n",
            "Epoch 19/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4864 - loss: 1.3886 - val_accuracy: 0.4132 - val_loss: 1.6026\n",
            "Epoch 20/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4564 - loss: 1.5002 - val_accuracy: 0.4306 - val_loss: 1.5490\n",
            "Epoch 21/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5046 - loss: 1.4049 - val_accuracy: 0.4653 - val_loss: 1.5061\n",
            "Epoch 22/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5438 - loss: 1.2973 - val_accuracy: 0.4826 - val_loss: 1.4682\n",
            "Epoch 23/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5461 - loss: 1.2809 - val_accuracy: 0.5000 - val_loss: 1.4166\n",
            "Epoch 24/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5540 - loss: 1.2577 - val_accuracy: 0.5069 - val_loss: 1.3602\n",
            "Epoch 25/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5722 - loss: 1.2107 - val_accuracy: 0.5312 - val_loss: 1.3106\n",
            "Epoch 26/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5651 - loss: 1.2406 - val_accuracy: 0.5660 - val_loss: 1.2872\n",
            "Epoch 27/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5694 - loss: 1.2329 - val_accuracy: 0.5556 - val_loss: 1.2454\n",
            "Epoch 28/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6187 - loss: 1.0913 - val_accuracy: 0.5625 - val_loss: 1.2273\n",
            "Epoch 29/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6039 - loss: 1.1073 - val_accuracy: 0.5660 - val_loss: 1.2029\n",
            "Epoch 30/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6315 - loss: 1.0751 - val_accuracy: 0.6042 - val_loss: 1.1621\n",
            "Epoch 31/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5803 - loss: 1.1303 - val_accuracy: 0.6146 - val_loss: 1.1515\n",
            "Epoch 32/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6252 - loss: 1.0433 - val_accuracy: 0.6076 - val_loss: 1.1443\n",
            "Epoch 33/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6248 - loss: 1.0409 - val_accuracy: 0.6076 - val_loss: 1.1410\n",
            "Epoch 34/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6435 - loss: 1.0083 - val_accuracy: 0.6181 - val_loss: 1.1138\n",
            "Epoch 35/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6498 - loss: 0.9822 - val_accuracy: 0.6042 - val_loss: 1.1059\n",
            "Epoch 36/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6647 - loss: 0.9313 - val_accuracy: 0.6076 - val_loss: 1.0955\n",
            "Epoch 37/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6829 - loss: 0.8747 - val_accuracy: 0.6354 - val_loss: 1.0735\n",
            "Epoch 38/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6594 - loss: 0.9494 - val_accuracy: 0.6319 - val_loss: 1.0608\n",
            "Epoch 39/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6803 - loss: 0.9145 - val_accuracy: 0.6181 - val_loss: 1.0674\n",
            "Epoch 40/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6835 - loss: 0.9252 - val_accuracy: 0.6389 - val_loss: 1.0490\n",
            "Epoch 41/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6959 - loss: 0.8411 - val_accuracy: 0.6597 - val_loss: 1.0419\n",
            "Epoch 42/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7236 - loss: 0.8162 - val_accuracy: 0.6528 - val_loss: 1.0442\n",
            "Epoch 43/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7023 - loss: 0.8409 - val_accuracy: 0.6597 - val_loss: 1.0562\n",
            "Epoch 44/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6928 - loss: 0.8465 - val_accuracy: 0.6424 - val_loss: 1.0230\n",
            "Epoch 45/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7141 - loss: 0.7890 - val_accuracy: 0.6562 - val_loss: 1.0159\n",
            "Epoch 46/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6953 - loss: 0.8742 - val_accuracy: 0.6493 - val_loss: 1.0268\n",
            "Epoch 47/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7163 - loss: 0.7319 - val_accuracy: 0.6458 - val_loss: 1.0347\n",
            "Epoch 48/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7358 - loss: 0.7466 - val_accuracy: 0.6424 - val_loss: 1.0387\n",
            "Epoch 49/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7243 - loss: 0.7570 - val_accuracy: 0.6493 - val_loss: 1.0359\n",
            "Epoch 50/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7121 - loss: 0.7584 - val_accuracy: 0.6493 - val_loss: 1.0173\n",
            "Epoch 51/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7182 - loss: 0.7523 - val_accuracy: 0.6354 - val_loss: 1.0154\n",
            "Epoch 52/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7535 - loss: 0.6971 - val_accuracy: 0.6215 - val_loss: 1.0266\n",
            "Epoch 53/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7475 - loss: 0.7219 - val_accuracy: 0.6285 - val_loss: 1.0062\n",
            "Epoch 54/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7555 - loss: 0.6778 - val_accuracy: 0.6354 - val_loss: 1.0082\n",
            "Epoch 55/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7656 - loss: 0.6906 - val_accuracy: 0.6458 - val_loss: 0.9921\n",
            "Epoch 56/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7874 - loss: 0.6237 - val_accuracy: 0.6562 - val_loss: 0.9783\n",
            "Epoch 57/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7688 - loss: 0.6569 - val_accuracy: 0.6528 - val_loss: 0.9698\n",
            "Epoch 58/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7815 - loss: 0.6119 - val_accuracy: 0.6667 - val_loss: 0.9538\n",
            "Epoch 59/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7613 - loss: 0.6420 - val_accuracy: 0.6562 - val_loss: 0.9465\n",
            "Epoch 60/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7441 - loss: 0.6314 - val_accuracy: 0.6562 - val_loss: 0.9651\n",
            "Epoch 61/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7777 - loss: 0.6202 - val_accuracy: 0.6597 - val_loss: 0.9604\n",
            "Epoch 62/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7941 - loss: 0.6374 - val_accuracy: 0.6701 - val_loss: 0.9639\n",
            "Epoch 63/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7812 - loss: 0.5780 - val_accuracy: 0.6667 - val_loss: 0.9702\n",
            "Epoch 64/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7954 - loss: 0.5911 - val_accuracy: 0.6493 - val_loss: 0.9493\n",
            "Epoch 65/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8025 - loss: 0.5247 - val_accuracy: 0.6840 - val_loss: 0.9204\n",
            "Epoch 66/200\n",
            "\u001b[1m12/18\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8253 - loss: 0.5391 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Dense(1024,activation='elu',input_dim=13))\n",
        "model.add(Dense(1024,activation='elu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(512,activation='elu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512,activation='elu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256,activation='elu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(8,activation='softmax'))\n",
        "# Compile the model\n",
        "optimizer = RMSprop(learning_rate=0.006019793254027166)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "XSROuTZcSEpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=200,batch_size=32, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "id": "eSU2RiNISEl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Dense(1024,activation='elu',input_dim=13))\n",
        "model.add(Dense(1024,activation='elu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(512,activation='elu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512,activation='elu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256,activation='elu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(8,activation='softmax'))\n",
        "# Compile the model\n",
        "optimizer = RMSprop(learning_rate=0.006019793254027166)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "sG-5XIxuSEi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=200,batch_size=32, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "id": "yGB0hz9uSEe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Dense(1024,activation='elu',input_dim=13))\n",
        "model.add(Dense(1024,activation='elu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(512,activation='elu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512,activation='elu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256,activation='elu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(8,activation='softmax'))\n",
        "# Compile the model\n",
        "optimizer = RMSprop(learning_rate=0.006019793254027166)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "JE-mTnNdSEbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=200,batch_size=32, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "id": "ExkFMihhSEYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Dense(1024,activation='elu',input_dim=13))\n",
        "model.add(Dense(1024,activation='elu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(512,activation='elu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512,activation='elu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256,activation='elu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(8,activation='softmax'))\n",
        "# Compile the model\n",
        "optimizer = RMSprop(learning_rate=0.006019793254027166)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "zjnCg3xVSEVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=200,batch_size=32, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "id": "nj2PH_FUSERp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Dense(1024,activation='elu',input_dim=13))\n",
        "model.add(Dense(1024,activation='elu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(512,activation='elu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512,activation='elu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256,activation='elu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(8,activation='softmax'))\n",
        "# Compile the model\n",
        "optimizer = RMSprop(learning_rate=0.006019793254027166)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Jbv32BOcSEOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CxKr-fzWSEKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "feQ-peOtSEHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Bu3R1VKSEDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y6rkQzUTSD_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bfMBTihfSD8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "syV0XL8TSD5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KRwOgAujSDiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pqBblqcJSDel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-qgMBpP3SDaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ADdKNXidSDXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HtFa003rSDUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HJK-LSUWSDQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bLeJX5ygSDNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WneYJmPuSDJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yY4eBBG1SDGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JGDn1O9TSDBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R4WW3s1HSC9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LToOFyY5SC5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jL3Yr2mfSC1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MK43GPzaOMNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O08FMFA3OMJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2nd"
      ],
      "metadata": {
        "id": "KxyPe7jRJ6FZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the HyperModel class\n",
        "class MLPHyperModel2(HyperModel):\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "\n",
        "        # Number of hidden layers\n",
        "        for i in range(hp.Int('num_layers', 1, 110)):\n",
        "            model.add(Dense(\n",
        "                units=hp.Int(f'units_{i}', min_value=52, max_value=500, step=13),\n",
        "                activation=hp.Choice(f'activation_{i}', ['relu', 'elu','tanh','softmax','sigmoid']),\n",
        "                input_dim=13 if i == 0 else None\n",
        "            ))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dropout(rate=hp.Float(f'dropout_{i}', 0.1, 0.8, step=0.1)))\n",
        "\n",
        "        # Output layer\n",
        "        model.add(Dense(8, activation='softmax'))\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-1, sampling='log')),\n",
        "            loss=hp.Choice('loss', ['sparse_categorical_crossentropy', 'categorical_crossentropy']),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model\n"
      ],
      "metadata": {
        "id": "1X5UDfTr5RiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tuner\n",
        "tuner = RandomSearch(\n",
        "    MLPHyperModel2(),\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='mlp_tuning1'\n",
        ")"
      ],
      "metadata": {
        "id": "lfGTislI5RgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(x_train, y_train_sparse, epochs=200,batch_size=32, validation_data=(x_test, y_test_sparse))\n",
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kXqm6MIv8QNS",
        "outputId": "7fc606b2-9d9c-47e7-eb15-5a5f01f9155e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 2 Complete [00h 00m 15s]\n",
            "\n",
            "Best val_accuracy So Far: None\n",
            "Total elapsed time: 00h 01m 03s\n",
            "\n",
            "Search: Running Trial #3\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "20                |102               |num_layers\n",
            "390               |351               |units_0\n",
            "softmax           |sigmoid           |activation_0\n",
            "0.5               |0.5               |dropout_0\n",
            "0.00060174        |0.073653          |learning_rate\n",
            "categorical_cro...|categorical_cro...|loss\n",
            "494               |52                |units_1\n",
            "tanh              |relu              |activation_1\n",
            "0.5               |0.1               |dropout_1\n",
            "416               |52                |units_2\n",
            "sigmoid           |relu              |activation_2\n",
            "0.8               |0.1               |dropout_2\n",
            "247               |52                |units_3\n",
            "tanh              |relu              |activation_3\n",
            "0.1               |0.1               |dropout_3\n",
            "221               |52                |units_4\n",
            "relu              |relu              |activation_4\n",
            "0.7               |0.1               |dropout_4\n",
            "143               |52                |units_5\n",
            "softmax           |relu              |activation_5\n",
            "0.7               |0.1               |dropout_5\n",
            "52                |52                |units_6\n",
            "elu               |relu              |activation_6\n",
            "0.5               |0.1               |dropout_6\n",
            "351               |52                |units_7\n",
            "tanh              |relu              |activation_7\n",
            "0.1               |0.1               |dropout_7\n",
            "260               |52                |units_8\n",
            "relu              |relu              |activation_8\n",
            "0.6               |0.1               |dropout_8\n",
            "260               |52                |units_9\n",
            "relu              |relu              |activation_9\n",
            "0.6               |0.1               |dropout_9\n",
            "455               |52                |units_10\n",
            "softmax           |relu              |activation_10\n",
            "0.2               |0.1               |dropout_10\n",
            "117               |52                |units_11\n",
            "relu              |relu              |activation_11\n",
            "0.3               |0.1               |dropout_11\n",
            "195               |52                |units_12\n",
            "relu              |relu              |activation_12\n",
            "0.4               |0.1               |dropout_12\n",
            "260               |52                |units_13\n",
            "sigmoid           |relu              |activation_13\n",
            "0.6               |0.1               |dropout_13\n",
            "273               |52                |units_14\n",
            "elu               |relu              |activation_14\n",
            "0.4               |0.1               |dropout_14\n",
            "403               |52                |units_15\n",
            "relu              |relu              |activation_15\n",
            "0.2               |0.1               |dropout_15\n",
            "156               |52                |units_16\n",
            "relu              |relu              |activation_16\n",
            "0.3               |0.1               |dropout_16\n",
            "338               |52                |units_17\n",
            "sigmoid           |relu              |activation_17\n",
            "0.2               |0.1               |dropout_17\n",
            "143               |52                |units_18\n",
            "tanh              |relu              |activation_18\n",
            "0.8               |0.1               |dropout_18\n",
            "442               |52                |units_19\n",
            "sigmoid           |relu              |activation_19\n",
            "0.7               |0.1               |dropout_19\n",
            "299               |52                |units_20\n",
            "sigmoid           |relu              |activation_20\n",
            "0.1               |0.1               |dropout_20\n",
            "377               |52                |units_21\n",
            "tanh              |relu              |activation_21\n",
            "0.1               |0.1               |dropout_21\n",
            "481               |52                |units_22\n",
            "tanh              |relu              |activation_22\n",
            "0.1               |0.1               |dropout_22\n",
            "377               |52                |units_23\n",
            "tanh              |relu              |activation_23\n",
            "0.1               |0.1               |dropout_23\n",
            "312               |52                |units_24\n",
            "softmax           |relu              |activation_24\n",
            "0.7               |0.1               |dropout_24\n",
            "104               |52                |units_25\n",
            "sigmoid           |relu              |activation_25\n",
            "0.6               |0.1               |dropout_25\n",
            "117               |52                |units_26\n",
            "tanh              |relu              |activation_26\n",
            "0.2               |0.1               |dropout_26\n",
            "91                |52                |units_27\n",
            "tanh              |relu              |activation_27\n",
            "0.8               |0.1               |dropout_27\n",
            "442               |52                |units_28\n",
            "sigmoid           |relu              |activation_28\n",
            "0.4               |0.1               |dropout_28\n",
            "117               |52                |units_29\n",
            "tanh              |relu              |activation_29\n",
            "0.4               |0.1               |dropout_29\n",
            "455               |52                |units_30\n",
            "relu              |relu              |activation_30\n",
            "0.6               |0.1               |dropout_30\n",
            "169               |52                |units_31\n",
            "sigmoid           |relu              |activation_31\n",
            "0.5               |0.1               |dropout_31\n",
            "156               |52                |units_32\n",
            "relu              |relu              |activation_32\n",
            "0.3               |0.1               |dropout_32\n",
            "156               |52                |units_33\n",
            "relu              |relu              |activation_33\n",
            "0.2               |0.1               |dropout_33\n",
            "338               |52                |units_34\n",
            "tanh              |relu              |activation_34\n",
            "0.7               |0.1               |dropout_34\n",
            "195               |52                |units_35\n",
            "softmax           |relu              |activation_35\n",
            "0.6               |0.1               |dropout_35\n",
            "338               |52                |units_36\n",
            "elu               |relu              |activation_36\n",
            "0.5               |0.1               |dropout_36\n",
            "130               |52                |units_37\n",
            "tanh              |relu              |activation_37\n",
            "0.2               |0.1               |dropout_37\n",
            "169               |52                |units_38\n",
            "elu               |relu              |activation_38\n",
            "0.8               |0.1               |dropout_38\n",
            "273               |52                |units_39\n",
            "softmax           |relu              |activation_39\n",
            "0.5               |0.1               |dropout_39\n",
            "91                |52                |units_40\n",
            "relu              |relu              |activation_40\n",
            "0.8               |0.1               |dropout_40\n",
            "286               |52                |units_41\n",
            "sigmoid           |relu              |activation_41\n",
            "0.5               |0.1               |dropout_41\n",
            "52                |52                |units_42\n",
            "sigmoid           |relu              |activation_42\n",
            "0.2               |0.1               |dropout_42\n",
            "195               |52                |units_43\n",
            "tanh              |relu              |activation_43\n",
            "0.3               |0.1               |dropout_43\n",
            "494               |52                |units_44\n",
            "elu               |relu              |activation_44\n",
            "0.2               |0.1               |dropout_44\n",
            "403               |52                |units_45\n",
            "sigmoid           |relu              |activation_45\n",
            "0.5               |0.1               |dropout_45\n",
            "468               |52                |units_46\n",
            "sigmoid           |relu              |activation_46\n",
            "0.3               |0.1               |dropout_46\n",
            "468               |52                |units_47\n",
            "tanh              |relu              |activation_47\n",
            "0.1               |0.1               |dropout_47\n",
            "208               |52                |units_48\n",
            "tanh              |relu              |activation_48\n",
            "0.1               |0.1               |dropout_48\n",
            "169               |52                |units_49\n",
            "relu              |relu              |activation_49\n",
            "0.2               |0.1               |dropout_49\n",
            "481               |52                |units_50\n",
            "tanh              |relu              |activation_50\n",
            "0.3               |0.1               |dropout_50\n",
            "494               |52                |units_51\n",
            "tanh              |relu              |activation_51\n",
            "0.6               |0.1               |dropout_51\n",
            "338               |52                |units_52\n",
            "softmax           |relu              |activation_52\n",
            "0.6               |0.1               |dropout_52\n",
            "52                |52                |units_53\n",
            "relu              |relu              |activation_53\n",
            "0.1               |0.1               |dropout_53\n",
            "338               |52                |units_54\n",
            "elu               |relu              |activation_54\n",
            "0.3               |0.1               |dropout_54\n",
            "390               |52                |units_55\n",
            "elu               |relu              |activation_55\n",
            "0.3               |0.1               |dropout_55\n",
            "455               |52                |units_56\n",
            "sigmoid           |relu              |activation_56\n",
            "0.7               |0.1               |dropout_56\n",
            "260               |52                |units_57\n",
            "relu              |relu              |activation_57\n",
            "0.1               |0.1               |dropout_57\n",
            "377               |52                |units_58\n",
            "sigmoid           |relu              |activation_58\n",
            "0.2               |0.1               |dropout_58\n",
            "442               |52                |units_59\n",
            "softmax           |relu              |activation_59\n",
            "0.2               |0.1               |dropout_59\n",
            "208               |52                |units_60\n",
            "elu               |relu              |activation_60\n",
            "0.3               |0.1               |dropout_60\n",
            "78                |52                |units_61\n",
            "softmax           |relu              |activation_61\n",
            "0.6               |0.1               |dropout_61\n",
            "156               |52                |units_62\n",
            "softmax           |relu              |activation_62\n",
            "0.6               |0.1               |dropout_62\n",
            "494               |52                |units_63\n",
            "sigmoid           |relu              |activation_63\n",
            "0.1               |0.1               |dropout_63\n",
            "429               |52                |units_64\n",
            "elu               |relu              |activation_64\n",
            "0.6               |0.1               |dropout_64\n",
            "312               |52                |units_65\n",
            "tanh              |relu              |activation_65\n",
            "0.2               |0.1               |dropout_65\n",
            "195               |52                |units_66\n",
            "sigmoid           |relu              |activation_66\n",
            "0.3               |0.1               |dropout_66\n",
            "481               |52                |units_67\n",
            "elu               |relu              |activation_67\n",
            "0.8               |0.1               |dropout_67\n",
            "377               |52                |units_68\n",
            "softmax           |relu              |activation_68\n",
            "0.8               |0.1               |dropout_68\n",
            "481               |52                |units_69\n",
            "softmax           |relu              |activation_69\n",
            "0.8               |0.1               |dropout_69\n",
            "351               |52                |units_70\n",
            "tanh              |relu              |activation_70\n",
            "0.1               |0.1               |dropout_70\n",
            "377               |52                |units_71\n",
            "sigmoid           |relu              |activation_71\n",
            "0.4               |0.1               |dropout_71\n",
            "364               |52                |units_72\n",
            "relu              |relu              |activation_72\n",
            "0.1               |0.1               |dropout_72\n",
            "364               |52                |units_73\n",
            "relu              |relu              |activation_73\n",
            "0.8               |0.1               |dropout_73\n",
            "377               |52                |units_74\n",
            "softmax           |relu              |activation_74\n",
            "0.2               |0.1               |dropout_74\n",
            "312               |52                |units_75\n",
            "tanh              |relu              |activation_75\n",
            "0.1               |0.1               |dropout_75\n",
            "299               |52                |units_76\n",
            "tanh              |relu              |activation_76\n",
            "0.3               |0.1               |dropout_76\n",
            "494               |52                |units_77\n",
            "relu              |relu              |activation_77\n",
            "0.4               |0.1               |dropout_77\n",
            "143               |52                |units_78\n",
            "elu               |relu              |activation_78\n",
            "0.1               |0.1               |dropout_78\n",
            "403               |52                |units_79\n",
            "softmax           |relu              |activation_79\n",
            "0.8               |0.1               |dropout_79\n",
            "234               |52                |units_80\n",
            "sigmoid           |relu              |activation_80\n",
            "0.3               |0.1               |dropout_80\n",
            "312               |52                |units_81\n",
            "sigmoid           |relu              |activation_81\n",
            "0.1               |0.1               |dropout_81\n",
            "117               |52                |units_82\n",
            "softmax           |relu              |activation_82\n",
            "0.5               |0.1               |dropout_82\n",
            "416               |52                |units_83\n",
            "softmax           |relu              |activation_83\n",
            "0.1               |0.1               |dropout_83\n",
            "377               |52                |units_84\n",
            "relu              |relu              |activation_84\n",
            "0.3               |0.1               |dropout_84\n",
            "299               |52                |units_85\n",
            "relu              |relu              |activation_85\n",
            "0.2               |0.1               |dropout_85\n",
            "338               |52                |units_86\n",
            "softmax           |relu              |activation_86\n",
            "0.7               |0.1               |dropout_86\n",
            "65                |52                |units_87\n",
            "tanh              |relu              |activation_87\n",
            "0.8               |0.1               |dropout_87\n",
            "299               |52                |units_88\n",
            "relu              |relu              |activation_88\n",
            "0.2               |0.1               |dropout_88\n",
            "169               |52                |units_89\n",
            "softmax           |relu              |activation_89\n",
            "0.3               |0.1               |dropout_89\n",
            "91                |52                |units_90\n",
            "softmax           |relu              |activation_90\n",
            "0.8               |0.1               |dropout_90\n",
            "208               |52                |units_91\n",
            "softmax           |relu              |activation_91\n",
            "0.2               |0.1               |dropout_91\n",
            "156               |52                |units_92\n",
            "tanh              |relu              |activation_92\n",
            "0.3               |0.1               |dropout_92\n",
            "195               |52                |units_93\n",
            "sigmoid           |relu              |activation_93\n",
            "0.5               |0.1               |dropout_93\n",
            "468               |52                |units_94\n",
            "sigmoid           |relu              |activation_94\n",
            "0.8               |0.1               |dropout_94\n",
            "286               |52                |units_95\n",
            "relu              |relu              |activation_95\n",
            "0.7               |0.1               |dropout_95\n",
            "169               |52                |units_96\n",
            "relu              |relu              |activation_96\n",
            "0.8               |0.1               |dropout_96\n",
            "104               |52                |units_97\n",
            "relu              |relu              |activation_97\n",
            "0.4               |0.1               |dropout_97\n",
            "91                |52                |units_98\n",
            "tanh              |relu              |activation_98\n",
            "0.3               |0.1               |dropout_98\n",
            "299               |52                |units_99\n",
            "softmax           |relu              |activation_99\n",
            "0.7               |0.1               |dropout_99\n",
            "429               |52                |units_100\n",
            "sigmoid           |relu              |activation_100\n",
            "0.7               |0.1               |dropout_100\n",
            "468               |52                |units_101\n",
            "relu              |relu              |activation_101\n",
            "0.1               |0.1               |dropout_101\n",
            "\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n",
            "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n",
            "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n",
            "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n",
            "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n",
            "    return model.fit(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py\", line 554, in categorical_crossentropy\n",
            "    raise ValueError(\n",
            "ValueError: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(32,), output.shape=(32, 8)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py\", line 554, in categorical_crossentropy\n    raise ValueError(\nValueError: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(32,), output.shape=(32, 8)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-587328f3b24a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_sparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36mon_trial_end\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTrial\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mLOCKS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mTHREADS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthread_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_acquire\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mTHREADS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36mend_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_consecutive_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36m_check_consecutive_failures\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[0mconsecutive_failures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconsecutive_failures\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_consecutive_failed_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m    546\u001b[0m                     \u001b[0;34m\"Number of consecutive failures exceeded the limit \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m                     \u001b[0;34mf\"of {self.max_consecutive_failed_trials}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py\", line 554, in categorical_crossentropy\n    raise ValueError(\nValueError: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(32,), output.shape=(32, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the best hyperparameters\n",
        "best_hyperparameters = tuner.get_best_hyperparameters()[0]"
      ],
      "metadata": {
        "id": "IYV31oIf8QKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "for hp_name in best_hyperparameters.values.keys():\n",
        "    print(f\"{hp_name}: {best_hyperparameters.get(hp_name)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxHSg6dR8QHK",
        "outputId": "72396fcc-e103-4b2c-99a7-4587c05840af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "num_layers: 5\n",
            "units_0: 65\n",
            "activation_0: relu\n",
            "dropout_0: 0.1\n",
            "learning_rate: 0.0010343990168720423\n",
            "loss: sparse_categorical_crossentropy\n",
            "units_1: 260\n",
            "activation_1: relu\n",
            "dropout_1: 0.30000000000000004\n",
            "units_2: 364\n",
            "activation_2: relu\n",
            "dropout_2: 0.4\n",
            "units_3: 52\n",
            "activation_3: relu\n",
            "dropout_3: 0.1\n",
            "units_4: 52\n",
            "activation_4: relu\n",
            "dropout_4: 0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP model parameters\n",
        "num_layers = 5\n",
        "units = [65, 260, 364, 52, 52]\n",
        "activations = ['relu', 'relu', 'relu', 'relu', 'relu']\n",
        "dropouts = [0.1, 0.3, 0.4, 0.1, 0.1]\n",
        "learning_rate = 0.0010343990168720423\n",
        "loss_function = 'sparse_categorical_crossentropy'\n",
        "input_shape=(13,)"
      ],
      "metadata": {
        "id": "tSrT5dRU_ra9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the MLP model\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Input layer\n",
        "model.add(layers.Input(shape=input_shape))\n",
        "\n",
        "# Hidden layers\n",
        "for i in range(num_layers):\n",
        "    model.add(layers.Dense(units[i], activation=activations[i]))\n",
        "    model.add(layers.Dropout(dropouts[i]))\n",
        "\n",
        "# Output layer (adjust the number of units to your number of classes)\n",
        "model.add(layers.Dense(8, activation='softmax'))  # Assuming 8 classes for output\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=loss_function,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "spBdWXKd_2Tz",
        "outputId": "25fbd549-bef6-420f-9b84-e6698e2ebc0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │             \u001b[38;5;34m910\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │          \u001b[38;5;34m17,160\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │          \u001b[38;5;34m95,004\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │          \u001b[38;5;34m18,980\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │           \u001b[38;5;34m2,756\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │             \u001b[38;5;34m424\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">910</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,160</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">95,004</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,980</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,756</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">424</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m135,234\u001b[0m (528.26 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">135,234</span> (528.26 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m135,234\u001b[0m (528.26 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">135,234</span> (528.26 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=200,batch_size=32, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBibI7OJAXMr",
        "outputId": "5ca511c4-026f-475d-87e9-322ef414896f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.1262 - loss: 2.0705 - val_accuracy: 0.2396 - val_loss: 1.9188\n",
            "Epoch 2/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2204 - loss: 1.9371 - val_accuracy: 0.3021 - val_loss: 1.8407\n",
            "Epoch 3/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3010 - loss: 1.8134 - val_accuracy: 0.3646 - val_loss: 1.7210\n",
            "Epoch 4/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3317 - loss: 1.7524 - val_accuracy: 0.3507 - val_loss: 1.6648\n",
            "Epoch 5/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3507 - loss: 1.7107 - val_accuracy: 0.4132 - val_loss: 1.5675\n",
            "Epoch 6/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4198 - loss: 1.5629 - val_accuracy: 0.4132 - val_loss: 1.5636\n",
            "Epoch 7/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3980 - loss: 1.5792 - val_accuracy: 0.4340 - val_loss: 1.4833\n",
            "Epoch 8/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4264 - loss: 1.5012 - val_accuracy: 0.4653 - val_loss: 1.4528\n",
            "Epoch 9/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4478 - loss: 1.4669 - val_accuracy: 0.4722 - val_loss: 1.4275\n",
            "Epoch 10/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4838 - loss: 1.4057 - val_accuracy: 0.4340 - val_loss: 1.4484\n",
            "Epoch 11/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4503 - loss: 1.4325 - val_accuracy: 0.4896 - val_loss: 1.3716\n",
            "Epoch 12/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5232 - loss: 1.3458 - val_accuracy: 0.4931 - val_loss: 1.3634\n",
            "Epoch 13/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4814 - loss: 1.3592 - val_accuracy: 0.4722 - val_loss: 1.3816\n",
            "Epoch 14/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5406 - loss: 1.2746 - val_accuracy: 0.4792 - val_loss: 1.3267\n",
            "Epoch 15/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5080 - loss: 1.2783 - val_accuracy: 0.4514 - val_loss: 1.3277\n",
            "Epoch 16/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5628 - loss: 1.2268 - val_accuracy: 0.4826 - val_loss: 1.3229\n",
            "Epoch 17/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5536 - loss: 1.2145 - val_accuracy: 0.5174 - val_loss: 1.2981\n",
            "Epoch 18/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5574 - loss: 1.1818 - val_accuracy: 0.5139 - val_loss: 1.2959\n",
            "Epoch 19/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5719 - loss: 1.1221 - val_accuracy: 0.5243 - val_loss: 1.2883\n",
            "Epoch 20/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5522 - loss: 1.2050 - val_accuracy: 0.5417 - val_loss: 1.2901\n",
            "Epoch 21/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6062 - loss: 1.0938 - val_accuracy: 0.5174 - val_loss: 1.3021\n",
            "Epoch 22/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6018 - loss: 1.0686 - val_accuracy: 0.5174 - val_loss: 1.2695\n",
            "Epoch 23/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6556 - loss: 1.0265 - val_accuracy: 0.5625 - val_loss: 1.2712\n",
            "Epoch 24/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5955 - loss: 1.0698 - val_accuracy: 0.5451 - val_loss: 1.2495\n",
            "Epoch 25/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6125 - loss: 1.0183 - val_accuracy: 0.5694 - val_loss: 1.2490\n",
            "Epoch 26/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6196 - loss: 1.0191 - val_accuracy: 0.5625 - val_loss: 1.2338\n",
            "Epoch 27/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6385 - loss: 1.0362 - val_accuracy: 0.5799 - val_loss: 1.2027\n",
            "Epoch 28/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6469 - loss: 0.9739 - val_accuracy: 0.5625 - val_loss: 1.2123\n",
            "Epoch 29/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6497 - loss: 0.9004 - val_accuracy: 0.5972 - val_loss: 1.1845\n",
            "Epoch 30/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6453 - loss: 0.9585 - val_accuracy: 0.5590 - val_loss: 1.2160\n",
            "Epoch 31/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6703 - loss: 0.9110 - val_accuracy: 0.5521 - val_loss: 1.2453\n",
            "Epoch 32/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7133 - loss: 0.8340 - val_accuracy: 0.5660 - val_loss: 1.2425\n",
            "Epoch 33/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6740 - loss: 0.8966 - val_accuracy: 0.5694 - val_loss: 1.1908\n",
            "Epoch 34/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6843 - loss: 0.8572 - val_accuracy: 0.5903 - val_loss: 1.1859\n",
            "Epoch 35/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6917 - loss: 0.8211 - val_accuracy: 0.5729 - val_loss: 1.2049\n",
            "Epoch 36/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7155 - loss: 0.8007 - val_accuracy: 0.5938 - val_loss: 1.1668\n",
            "Epoch 37/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7315 - loss: 0.7667 - val_accuracy: 0.5868 - val_loss: 1.1982\n",
            "Epoch 38/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7151 - loss: 0.7729 - val_accuracy: 0.5938 - val_loss: 1.2022\n",
            "Epoch 39/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7254 - loss: 0.7851 - val_accuracy: 0.5694 - val_loss: 1.1981\n",
            "Epoch 40/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7121 - loss: 0.8033 - val_accuracy: 0.5799 - val_loss: 1.1878\n",
            "Epoch 41/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7256 - loss: 0.7490 - val_accuracy: 0.5938 - val_loss: 1.1583\n",
            "Epoch 42/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7357 - loss: 0.7708 - val_accuracy: 0.6285 - val_loss: 1.1588\n",
            "Epoch 43/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7491 - loss: 0.7226 - val_accuracy: 0.6319 - val_loss: 1.1436\n",
            "Epoch 44/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7301 - loss: 0.7447 - val_accuracy: 0.6111 - val_loss: 1.1457\n",
            "Epoch 45/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7504 - loss: 0.6917 - val_accuracy: 0.6007 - val_loss: 1.1311\n",
            "Epoch 46/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7380 - loss: 0.7319 - val_accuracy: 0.6146 - val_loss: 1.1660\n",
            "Epoch 47/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7500 - loss: 0.6973 - val_accuracy: 0.5694 - val_loss: 1.2027\n",
            "Epoch 48/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7498 - loss: 0.6986 - val_accuracy: 0.6285 - val_loss: 1.1445\n",
            "Epoch 49/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7590 - loss: 0.6708 - val_accuracy: 0.6146 - val_loss: 1.1511\n",
            "Epoch 50/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7633 - loss: 0.6370 - val_accuracy: 0.6111 - val_loss: 1.1784\n",
            "Epoch 51/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7997 - loss: 0.6284 - val_accuracy: 0.6215 - val_loss: 1.1315\n",
            "Epoch 52/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7861 - loss: 0.6099 - val_accuracy: 0.6250 - val_loss: 1.1788\n",
            "Epoch 53/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7778 - loss: 0.6058 - val_accuracy: 0.6181 - val_loss: 1.2033\n",
            "Epoch 54/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7876 - loss: 0.6097 - val_accuracy: 0.6007 - val_loss: 1.2072\n",
            "Epoch 55/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8129 - loss: 0.6066 - val_accuracy: 0.6007 - val_loss: 1.2245\n",
            "Epoch 56/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7727 - loss: 0.6395 - val_accuracy: 0.6146 - val_loss: 1.2008\n",
            "Epoch 57/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7915 - loss: 0.5650 - val_accuracy: 0.6146 - val_loss: 1.1576\n",
            "Epoch 58/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8229 - loss: 0.5541 - val_accuracy: 0.5938 - val_loss: 1.1492\n",
            "Epoch 59/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7957 - loss: 0.5704 - val_accuracy: 0.6181 - val_loss: 1.1790\n",
            "Epoch 60/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8126 - loss: 0.5497 - val_accuracy: 0.6076 - val_loss: 1.1860\n",
            "Epoch 61/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8203 - loss: 0.5311 - val_accuracy: 0.6146 - val_loss: 1.1797\n",
            "Epoch 62/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8506 - loss: 0.5009 - val_accuracy: 0.6215 - val_loss: 1.1932\n",
            "Epoch 63/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8102 - loss: 0.5356 - val_accuracy: 0.6389 - val_loss: 1.1943\n",
            "Epoch 64/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7992 - loss: 0.5383 - val_accuracy: 0.6181 - val_loss: 1.1853\n",
            "Epoch 65/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8247 - loss: 0.5198 - val_accuracy: 0.6285 - val_loss: 1.2458\n",
            "Epoch 66/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8498 - loss: 0.4529 - val_accuracy: 0.6146 - val_loss: 1.2118\n",
            "Epoch 67/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8246 - loss: 0.4716 - val_accuracy: 0.6389 - val_loss: 1.2210\n",
            "Epoch 68/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8241 - loss: 0.5023 - val_accuracy: 0.6424 - val_loss: 1.1176\n",
            "Epoch 69/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8300 - loss: 0.5213 - val_accuracy: 0.6389 - val_loss: 1.2300\n",
            "Epoch 70/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8334 - loss: 0.4749 - val_accuracy: 0.6111 - val_loss: 1.2504\n",
            "Epoch 71/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8275 - loss: 0.4898 - val_accuracy: 0.6458 - val_loss: 1.1614\n",
            "Epoch 72/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8597 - loss: 0.4166 - val_accuracy: 0.6354 - val_loss: 1.1940\n",
            "Epoch 73/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8498 - loss: 0.4440 - val_accuracy: 0.6285 - val_loss: 1.1528\n",
            "Epoch 74/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8176 - loss: 0.5025 - val_accuracy: 0.6111 - val_loss: 1.2258\n",
            "Epoch 75/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8502 - loss: 0.4779 - val_accuracy: 0.6215 - val_loss: 1.1411\n",
            "Epoch 76/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8340 - loss: 0.4751 - val_accuracy: 0.6597 - val_loss: 1.1683\n",
            "Epoch 77/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8359 - loss: 0.4706 - val_accuracy: 0.6389 - val_loss: 1.2189\n",
            "Epoch 78/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8438 - loss: 0.4344 - val_accuracy: 0.6285 - val_loss: 1.1444\n",
            "Epoch 79/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8710 - loss: 0.3918 - val_accuracy: 0.6215 - val_loss: 1.2060\n",
            "Epoch 80/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8483 - loss: 0.4088 - val_accuracy: 0.6354 - val_loss: 1.1951\n",
            "Epoch 81/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8744 - loss: 0.3988 - val_accuracy: 0.6528 - val_loss: 1.1973\n",
            "Epoch 82/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8793 - loss: 0.3894 - val_accuracy: 0.6250 - val_loss: 1.2071\n",
            "Epoch 83/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8495 - loss: 0.4564 - val_accuracy: 0.6424 - val_loss: 1.2001\n",
            "Epoch 84/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8755 - loss: 0.3920 - val_accuracy: 0.6458 - val_loss: 1.2437\n",
            "Epoch 85/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8585 - loss: 0.4046 - val_accuracy: 0.6250 - val_loss: 1.2260\n",
            "Epoch 86/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8636 - loss: 0.3770 - val_accuracy: 0.6215 - val_loss: 1.2224\n",
            "Epoch 87/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8716 - loss: 0.3778 - val_accuracy: 0.6354 - val_loss: 1.1793\n",
            "Epoch 88/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8585 - loss: 0.4037 - val_accuracy: 0.6389 - val_loss: 1.2009\n",
            "Epoch 89/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8499 - loss: 0.4487 - val_accuracy: 0.6389 - val_loss: 1.2790\n",
            "Epoch 90/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8880 - loss: 0.3816 - val_accuracy: 0.6354 - val_loss: 1.3079\n",
            "Epoch 91/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8523 - loss: 0.4229 - val_accuracy: 0.6319 - val_loss: 1.1913\n",
            "Epoch 92/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8848 - loss: 0.3614 - val_accuracy: 0.6424 - val_loss: 1.2203\n",
            "Epoch 93/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8822 - loss: 0.3514 - val_accuracy: 0.6076 - val_loss: 1.2646\n",
            "Epoch 94/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8868 - loss: 0.3357 - val_accuracy: 0.6250 - val_loss: 1.2628\n",
            "Epoch 95/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8613 - loss: 0.3925 - val_accuracy: 0.6319 - val_loss: 1.2252\n",
            "Epoch 96/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8634 - loss: 0.3947 - val_accuracy: 0.6632 - val_loss: 1.2301\n",
            "Epoch 97/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8887 - loss: 0.3516 - val_accuracy: 0.6528 - val_loss: 1.1459\n",
            "Epoch 98/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8715 - loss: 0.3812 - val_accuracy: 0.6424 - val_loss: 1.2318\n",
            "Epoch 99/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8686 - loss: 0.4066 - val_accuracy: 0.6285 - val_loss: 1.2872\n",
            "Epoch 100/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8987 - loss: 0.3214 - val_accuracy: 0.6458 - val_loss: 1.2452\n",
            "Epoch 101/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8830 - loss: 0.3093 - val_accuracy: 0.6493 - val_loss: 1.2161\n",
            "Epoch 102/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8777 - loss: 0.3595 - val_accuracy: 0.6285 - val_loss: 1.2189\n",
            "Epoch 103/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8648 - loss: 0.3445 - val_accuracy: 0.6389 - val_loss: 1.1956\n",
            "Epoch 104/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8941 - loss: 0.3325 - val_accuracy: 0.6493 - val_loss: 1.2444\n",
            "Epoch 105/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8876 - loss: 0.3879 - val_accuracy: 0.6562 - val_loss: 1.2013\n",
            "Epoch 106/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9022 - loss: 0.3188 - val_accuracy: 0.6319 - val_loss: 1.2337\n",
            "Epoch 107/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8885 - loss: 0.3511 - val_accuracy: 0.6667 - val_loss: 1.2287\n",
            "Epoch 108/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8874 - loss: 0.3223 - val_accuracy: 0.6424 - val_loss: 1.2721\n",
            "Epoch 109/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8834 - loss: 0.3505 - val_accuracy: 0.6250 - val_loss: 1.2662\n",
            "Epoch 110/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9135 - loss: 0.2658 - val_accuracy: 0.6250 - val_loss: 1.3396\n",
            "Epoch 111/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8822 - loss: 0.3441 - val_accuracy: 0.6424 - val_loss: 1.2805\n",
            "Epoch 112/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8816 - loss: 0.3086 - val_accuracy: 0.6562 - val_loss: 1.2186\n",
            "Epoch 113/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8872 - loss: 0.3094 - val_accuracy: 0.6389 - val_loss: 1.2752\n",
            "Epoch 114/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8795 - loss: 0.3460 - val_accuracy: 0.6424 - val_loss: 1.2203\n",
            "Epoch 115/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9212 - loss: 0.2495 - val_accuracy: 0.6701 - val_loss: 1.2343\n",
            "Epoch 116/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8802 - loss: 0.3319 - val_accuracy: 0.6701 - val_loss: 1.2622\n",
            "Epoch 117/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8947 - loss: 0.3042 - val_accuracy: 0.6424 - val_loss: 1.2694\n",
            "Epoch 118/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9112 - loss: 0.2956 - val_accuracy: 0.6528 - val_loss: 1.2898\n",
            "Epoch 119/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8839 - loss: 0.3661 - val_accuracy: 0.6424 - val_loss: 1.2321\n",
            "Epoch 120/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9187 - loss: 0.2676 - val_accuracy: 0.6389 - val_loss: 1.2955\n",
            "Epoch 121/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9091 - loss: 0.2914 - val_accuracy: 0.6215 - val_loss: 1.2889\n",
            "Epoch 122/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9102 - loss: 0.2943 - val_accuracy: 0.6562 - val_loss: 1.2499\n",
            "Epoch 123/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9004 - loss: 0.3084 - val_accuracy: 0.6736 - val_loss: 1.2789\n",
            "Epoch 124/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8990 - loss: 0.3024 - val_accuracy: 0.6458 - val_loss: 1.2808\n",
            "Epoch 125/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9009 - loss: 0.3095 - val_accuracy: 0.6562 - val_loss: 1.2812\n",
            "Epoch 126/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8985 - loss: 0.3200 - val_accuracy: 0.6736 - val_loss: 1.2459\n",
            "Epoch 127/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9199 - loss: 0.2518 - val_accuracy: 0.6458 - val_loss: 1.2980\n",
            "Epoch 128/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9197 - loss: 0.2633 - val_accuracy: 0.6701 - val_loss: 1.2947\n",
            "Epoch 129/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9287 - loss: 0.2350 - val_accuracy: 0.6806 - val_loss: 1.2781\n",
            "Epoch 130/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9009 - loss: 0.3330 - val_accuracy: 0.6910 - val_loss: 1.2018\n",
            "Epoch 131/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8960 - loss: 0.3205 - val_accuracy: 0.6285 - val_loss: 1.3164\n",
            "Epoch 132/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8905 - loss: 0.3166 - val_accuracy: 0.6528 - val_loss: 1.3251\n",
            "Epoch 133/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9303 - loss: 0.2300 - val_accuracy: 0.6458 - val_loss: 1.2770\n",
            "Epoch 134/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9171 - loss: 0.2501 - val_accuracy: 0.6562 - val_loss: 1.2808\n",
            "Epoch 135/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9158 - loss: 0.2594 - val_accuracy: 0.6562 - val_loss: 1.3225\n",
            "Epoch 136/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9020 - loss: 0.2535 - val_accuracy: 0.6458 - val_loss: 1.3860\n",
            "Epoch 137/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9015 - loss: 0.2606 - val_accuracy: 0.6667 - val_loss: 1.3186\n",
            "Epoch 138/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9202 - loss: 0.2187 - val_accuracy: 0.6319 - val_loss: 1.3774\n",
            "Epoch 139/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9158 - loss: 0.2540 - val_accuracy: 0.6632 - val_loss: 1.2629\n",
            "Epoch 140/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9134 - loss: 0.2720 - val_accuracy: 0.6736 - val_loss: 1.2504\n",
            "Epoch 141/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9065 - loss: 0.2788 - val_accuracy: 0.6701 - val_loss: 1.2816\n",
            "Epoch 142/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9051 - loss: 0.2620 - val_accuracy: 0.6771 - val_loss: 1.2491\n",
            "Epoch 143/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9301 - loss: 0.2469 - val_accuracy: 0.6806 - val_loss: 1.2788\n",
            "Epoch 144/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9001 - loss: 0.3099 - val_accuracy: 0.6562 - val_loss: 1.2858\n",
            "Epoch 145/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9141 - loss: 0.2459 - val_accuracy: 0.6597 - val_loss: 1.3298\n",
            "Epoch 146/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9307 - loss: 0.2557 - val_accuracy: 0.6528 - val_loss: 1.2940\n",
            "Epoch 147/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9161 - loss: 0.2415 - val_accuracy: 0.6389 - val_loss: 1.3239\n",
            "Epoch 148/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9234 - loss: 0.2556 - val_accuracy: 0.6389 - val_loss: 1.3829\n",
            "Epoch 149/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9295 - loss: 0.2053 - val_accuracy: 0.6701 - val_loss: 1.3665\n",
            "Epoch 150/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9355 - loss: 0.2113 - val_accuracy: 0.6632 - val_loss: 1.4422\n",
            "Epoch 151/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9311 - loss: 0.2256 - val_accuracy: 0.6493 - val_loss: 1.3983\n",
            "Epoch 152/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9192 - loss: 0.2601 - val_accuracy: 0.6597 - val_loss: 1.3822\n",
            "Epoch 153/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9117 - loss: 0.2681 - val_accuracy: 0.6562 - val_loss: 1.3050\n",
            "Epoch 154/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9281 - loss: 0.2206 - val_accuracy: 0.6771 - val_loss: 1.2762\n",
            "Epoch 155/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9239 - loss: 0.2049 - val_accuracy: 0.6424 - val_loss: 1.3740\n",
            "Epoch 156/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9337 - loss: 0.2110 - val_accuracy: 0.6528 - val_loss: 1.3575\n",
            "Epoch 157/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9201 - loss: 0.2554 - val_accuracy: 0.6562 - val_loss: 1.3173\n",
            "Epoch 158/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9240 - loss: 0.2153 - val_accuracy: 0.6493 - val_loss: 1.4092\n",
            "Epoch 159/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9223 - loss: 0.2295 - val_accuracy: 0.6319 - val_loss: 1.4457\n",
            "Epoch 160/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9189 - loss: 0.2621 - val_accuracy: 0.6354 - val_loss: 1.3025\n",
            "Epoch 161/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9158 - loss: 0.2105 - val_accuracy: 0.6493 - val_loss: 1.4613\n",
            "Epoch 162/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9406 - loss: 0.1923 - val_accuracy: 0.6493 - val_loss: 1.3398\n",
            "Epoch 163/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9110 - loss: 0.2602 - val_accuracy: 0.6458 - val_loss: 1.3552\n",
            "Epoch 164/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9302 - loss: 0.2043 - val_accuracy: 0.6597 - val_loss: 1.4361\n",
            "Epoch 165/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9478 - loss: 0.1891 - val_accuracy: 0.6597 - val_loss: 1.4415\n",
            "Epoch 166/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9342 - loss: 0.1970 - val_accuracy: 0.6597 - val_loss: 1.4639\n",
            "Epoch 167/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9274 - loss: 0.2356 - val_accuracy: 0.6701 - val_loss: 1.3258\n",
            "Epoch 168/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9389 - loss: 0.1980 - val_accuracy: 0.6597 - val_loss: 1.3275\n",
            "Epoch 169/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9310 - loss: 0.2032 - val_accuracy: 0.6528 - val_loss: 1.3979\n",
            "Epoch 170/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9243 - loss: 0.2564 - val_accuracy: 0.6146 - val_loss: 1.4236\n",
            "Epoch 171/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9256 - loss: 0.2049 - val_accuracy: 0.6389 - val_loss: 1.3194\n",
            "Epoch 172/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9402 - loss: 0.2200 - val_accuracy: 0.6424 - val_loss: 1.3391\n",
            "Epoch 173/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9374 - loss: 0.2146 - val_accuracy: 0.6840 - val_loss: 1.3243\n",
            "Epoch 174/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9412 - loss: 0.2021 - val_accuracy: 0.6667 - val_loss: 1.3617\n",
            "Epoch 175/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9259 - loss: 0.2482 - val_accuracy: 0.6528 - val_loss: 1.4006\n",
            "Epoch 176/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9226 - loss: 0.2559 - val_accuracy: 0.6528 - val_loss: 1.3348\n",
            "Epoch 177/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9120 - loss: 0.2725 - val_accuracy: 0.6667 - val_loss: 1.3608\n",
            "Epoch 178/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9240 - loss: 0.2293 - val_accuracy: 0.6736 - val_loss: 1.2746\n",
            "Epoch 179/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9418 - loss: 0.1929 - val_accuracy: 0.6875 - val_loss: 1.2545\n",
            "Epoch 180/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9129 - loss: 0.2768 - val_accuracy: 0.6528 - val_loss: 1.3427\n",
            "Epoch 181/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9205 - loss: 0.2031 - val_accuracy: 0.6493 - val_loss: 1.3505\n",
            "Epoch 182/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9381 - loss: 0.2074 - val_accuracy: 0.6701 - val_loss: 1.3664\n",
            "Epoch 183/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9150 - loss: 0.2141 - val_accuracy: 0.6424 - val_loss: 1.4299\n",
            "Epoch 184/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9264 - loss: 0.2551 - val_accuracy: 0.6632 - val_loss: 1.3829\n",
            "Epoch 185/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9241 - loss: 0.2223 - val_accuracy: 0.6493 - val_loss: 1.3989\n",
            "Epoch 186/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9364 - loss: 0.1920 - val_accuracy: 0.6424 - val_loss: 1.4066\n",
            "Epoch 187/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9335 - loss: 0.2025 - val_accuracy: 0.6667 - val_loss: 1.3891\n",
            "Epoch 188/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9266 - loss: 0.2323 - val_accuracy: 0.6806 - val_loss: 1.3716\n",
            "Epoch 189/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9371 - loss: 0.2081 - val_accuracy: 0.6771 - val_loss: 1.3761\n",
            "Epoch 190/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9389 - loss: 0.1730 - val_accuracy: 0.6736 - val_loss: 1.4780\n",
            "Epoch 191/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9393 - loss: 0.2160 - val_accuracy: 0.6667 - val_loss: 1.4313\n",
            "Epoch 192/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9355 - loss: 0.2036 - val_accuracy: 0.6667 - val_loss: 1.4015\n",
            "Epoch 193/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9389 - loss: 0.2215 - val_accuracy: 0.6701 - val_loss: 1.3790\n",
            "Epoch 194/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9299 - loss: 0.2599 - val_accuracy: 0.6458 - val_loss: 1.4075\n",
            "Epoch 195/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9408 - loss: 0.2034 - val_accuracy: 0.6701 - val_loss: 1.3393\n",
            "Epoch 196/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9341 - loss: 0.1939 - val_accuracy: 0.6389 - val_loss: 1.4122\n",
            "Epoch 197/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9478 - loss: 0.1683 - val_accuracy: 0.6667 - val_loss: 1.3873\n",
            "Epoch 198/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9542 - loss: 0.1442 - val_accuracy: 0.6562 - val_loss: 1.4309\n",
            "Epoch 199/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9485 - loss: 0.1772 - val_accuracy: 0.6562 - val_loss: 1.4123\n",
            "Epoch 200/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9418 - loss: 0.1777 - val_accuracy: 0.6597 - val_loss: 1.4278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP model parameters\n",
        "num_layers = 5\n",
        "units = [364, 260, 364, 260, 52]\n",
        "activations = ['relu', 'relu', 'relu', 'relu', 'relu']\n",
        "dropouts = [0.1, 0.3, 0.4, 0.1, 0.1]\n",
        "learning_rate = 0.0010343990168720423\n",
        "loss_function = 'sparse_categorical_crossentropy'\n",
        "input_shape=(13,)\n"
      ],
      "metadata": {
        "id": "ZCeGtMKHBMgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the MLP model\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Input layer\n",
        "model.add(layers.Input(shape=input_shape))\n",
        "\n",
        "# Hidden layers\n",
        "for i in range(num_layers):\n",
        "    model.add(layers.Dense(units[i], activation=activations[i]))\n",
        "    model.add(layers.Dropout(dropouts[i]))\n",
        "\n",
        "# Output layer (adjust the number of units to your number of classes)\n",
        "model.add(layers.Dense(8, activation='softmax'))  # Assuming 8 classes for output\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=loss_function,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "0oM_Z2VaBT-y",
        "outputId": "4536719e-6ce8-49e3-c7d4-465f7faec5f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │           \u001b[38;5;34m5,096\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │          \u001b[38;5;34m94,900\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │          \u001b[38;5;34m95,004\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │          \u001b[38;5;34m94,900\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │          \u001b[38;5;34m13,572\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │             \u001b[38;5;34m424\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,096</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">94,900</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">95,004</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">94,900</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">13,572</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">424</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m303,896\u001b[0m (1.16 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">303,896</span> (1.16 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m303,896\u001b[0m (1.16 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">303,896</span> (1.16 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=200,batch_size=32, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU-XHNNEBcAm",
        "outputId": "5e0d42b8-faf4-42b2-98e3-4f899647d48a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.1656 - loss: 2.0143 - val_accuracy: 0.3368 - val_loss: 1.7937\n",
            "Epoch 2/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2860 - loss: 1.8165 - val_accuracy: 0.3646 - val_loss: 1.6659\n",
            "Epoch 3/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3856 - loss: 1.6549 - val_accuracy: 0.4028 - val_loss: 1.5180\n",
            "Epoch 4/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3876 - loss: 1.5604 - val_accuracy: 0.4236 - val_loss: 1.4685\n",
            "Epoch 5/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4637 - loss: 1.4459 - val_accuracy: 0.4618 - val_loss: 1.4761\n",
            "Epoch 6/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4694 - loss: 1.3766 - val_accuracy: 0.4653 - val_loss: 1.3858\n",
            "Epoch 7/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.5304 - loss: 1.2715 - val_accuracy: 0.4826 - val_loss: 1.4469\n",
            "Epoch 8/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.5402 - loss: 1.2479 - val_accuracy: 0.4792 - val_loss: 1.3261\n",
            "Epoch 9/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.5311 - loss: 1.2322 - val_accuracy: 0.4757 - val_loss: 1.3820\n",
            "Epoch 10/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.5842 - loss: 1.1123 - val_accuracy: 0.5208 - val_loss: 1.2815\n",
            "Epoch 11/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5659 - loss: 1.0989 - val_accuracy: 0.5139 - val_loss: 1.3063\n",
            "Epoch 12/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6273 - loss: 1.0093 - val_accuracy: 0.5660 - val_loss: 1.2533\n",
            "Epoch 13/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6839 - loss: 0.9021 - val_accuracy: 0.5417 - val_loss: 1.2422\n",
            "Epoch 14/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6598 - loss: 0.9316 - val_accuracy: 0.5521 - val_loss: 1.2938\n",
            "Epoch 15/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6576 - loss: 0.9412 - val_accuracy: 0.5556 - val_loss: 1.2437\n",
            "Epoch 16/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7044 - loss: 0.8471 - val_accuracy: 0.5382 - val_loss: 1.3166\n",
            "Epoch 17/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6933 - loss: 0.8762 - val_accuracy: 0.5451 - val_loss: 1.2916\n",
            "Epoch 18/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7060 - loss: 0.8051 - val_accuracy: 0.5799 - val_loss: 1.3204\n",
            "Epoch 19/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7109 - loss: 0.8238 - val_accuracy: 0.5729 - val_loss: 1.2831\n",
            "Epoch 20/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7411 - loss: 0.7180 - val_accuracy: 0.6250 - val_loss: 1.1911\n",
            "Epoch 21/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7497 - loss: 0.7303 - val_accuracy: 0.5833 - val_loss: 1.2414\n",
            "Epoch 22/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7289 - loss: 0.7174 - val_accuracy: 0.5868 - val_loss: 1.2735\n",
            "Epoch 23/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7889 - loss: 0.6172 - val_accuracy: 0.6076 - val_loss: 1.2686\n",
            "Epoch 24/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7638 - loss: 0.6418 - val_accuracy: 0.6319 - val_loss: 1.2665\n",
            "Epoch 25/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7959 - loss: 0.5861 - val_accuracy: 0.5799 - val_loss: 1.3148\n",
            "Epoch 26/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7807 - loss: 0.5900 - val_accuracy: 0.5868 - val_loss: 1.3713\n",
            "Epoch 27/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7946 - loss: 0.5495 - val_accuracy: 0.5833 - val_loss: 1.3000\n",
            "Epoch 28/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7964 - loss: 0.5553 - val_accuracy: 0.5729 - val_loss: 1.3629\n",
            "Epoch 29/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8074 - loss: 0.5457 - val_accuracy: 0.5764 - val_loss: 1.3609\n",
            "Epoch 30/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8400 - loss: 0.4497 - val_accuracy: 0.6215 - val_loss: 1.3368\n",
            "Epoch 31/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8339 - loss: 0.4481 - val_accuracy: 0.6215 - val_loss: 1.3415\n",
            "Epoch 32/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8597 - loss: 0.4460 - val_accuracy: 0.6076 - val_loss: 1.4370\n",
            "Epoch 33/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8431 - loss: 0.4367 - val_accuracy: 0.6111 - val_loss: 1.5171\n",
            "Epoch 34/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8561 - loss: 0.3892 - val_accuracy: 0.6042 - val_loss: 1.5009\n",
            "Epoch 35/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8598 - loss: 0.3873 - val_accuracy: 0.6181 - val_loss: 1.4316\n",
            "Epoch 36/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8481 - loss: 0.4905 - val_accuracy: 0.6181 - val_loss: 1.3559\n",
            "Epoch 37/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8987 - loss: 0.3074 - val_accuracy: 0.6007 - val_loss: 1.5642\n",
            "Epoch 38/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8795 - loss: 0.3638 - val_accuracy: 0.6076 - val_loss: 1.4943\n",
            "Epoch 39/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8600 - loss: 0.3799 - val_accuracy: 0.6285 - val_loss: 1.4322\n",
            "Epoch 40/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8726 - loss: 0.3483 - val_accuracy: 0.5972 - val_loss: 1.5180\n",
            "Epoch 41/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8704 - loss: 0.3620 - val_accuracy: 0.6111 - val_loss: 1.5490\n",
            "Epoch 42/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8736 - loss: 0.3411 - val_accuracy: 0.6250 - val_loss: 1.5201\n",
            "Epoch 43/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8921 - loss: 0.3280 - val_accuracy: 0.6076 - val_loss: 1.4887\n",
            "Epoch 44/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8947 - loss: 0.3038 - val_accuracy: 0.5903 - val_loss: 1.6581\n",
            "Epoch 45/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9040 - loss: 0.2872 - val_accuracy: 0.6042 - val_loss: 1.5946\n",
            "Epoch 46/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9030 - loss: 0.3168 - val_accuracy: 0.6215 - val_loss: 1.4482\n",
            "Epoch 47/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9082 - loss: 0.2850 - val_accuracy: 0.6285 - val_loss: 1.5019\n",
            "Epoch 48/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8968 - loss: 0.3023 - val_accuracy: 0.6215 - val_loss: 1.6241\n",
            "Epoch 49/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8781 - loss: 0.3259 - val_accuracy: 0.6493 - val_loss: 1.4483\n",
            "Epoch 50/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9217 - loss: 0.2281 - val_accuracy: 0.6181 - val_loss: 1.5491\n",
            "Epoch 51/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9265 - loss: 0.2201 - val_accuracy: 0.6354 - val_loss: 1.5564\n",
            "Epoch 52/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9071 - loss: 0.2385 - val_accuracy: 0.6215 - val_loss: 1.5541\n",
            "Epoch 53/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9101 - loss: 0.2763 - val_accuracy: 0.6215 - val_loss: 1.5725\n",
            "Epoch 54/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9200 - loss: 0.2300 - val_accuracy: 0.6111 - val_loss: 1.5452\n",
            "Epoch 55/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9187 - loss: 0.2340 - val_accuracy: 0.6076 - val_loss: 1.6346\n",
            "Epoch 56/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9272 - loss: 0.2157 - val_accuracy: 0.5972 - val_loss: 1.6939\n",
            "Epoch 57/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9248 - loss: 0.2384 - val_accuracy: 0.6458 - val_loss: 1.4747\n",
            "Epoch 58/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9267 - loss: 0.2249 - val_accuracy: 0.6215 - val_loss: 1.6897\n",
            "Epoch 59/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9410 - loss: 0.2005 - val_accuracy: 0.6146 - val_loss: 1.5507\n",
            "Epoch 60/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9463 - loss: 0.1723 - val_accuracy: 0.6146 - val_loss: 1.6561\n",
            "Epoch 61/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9153 - loss: 0.2315 - val_accuracy: 0.6146 - val_loss: 1.7191\n",
            "Epoch 62/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9162 - loss: 0.2476 - val_accuracy: 0.6389 - val_loss: 1.6010\n",
            "Epoch 63/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9450 - loss: 0.1672 - val_accuracy: 0.6319 - val_loss: 1.5390\n",
            "Epoch 64/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9480 - loss: 0.1789 - val_accuracy: 0.6319 - val_loss: 1.5282\n",
            "Epoch 65/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9252 - loss: 0.2308 - val_accuracy: 0.6111 - val_loss: 1.6756\n",
            "Epoch 66/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9590 - loss: 0.1389 - val_accuracy: 0.6250 - val_loss: 1.7325\n",
            "Epoch 67/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9550 - loss: 0.1459 - val_accuracy: 0.6146 - val_loss: 1.7339\n",
            "Epoch 68/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9349 - loss: 0.1861 - val_accuracy: 0.6250 - val_loss: 1.6970\n",
            "Epoch 69/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9482 - loss: 0.1757 - val_accuracy: 0.6285 - val_loss: 1.7675\n",
            "Epoch 70/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9601 - loss: 0.1282 - val_accuracy: 0.6319 - val_loss: 1.8167\n",
            "Epoch 71/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9269 - loss: 0.1988 - val_accuracy: 0.6562 - val_loss: 1.6741\n",
            "Epoch 72/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9441 - loss: 0.1707 - val_accuracy: 0.6458 - val_loss: 1.6892\n",
            "Epoch 73/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9164 - loss: 0.2273 - val_accuracy: 0.6771 - val_loss: 1.5425\n",
            "Epoch 74/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9396 - loss: 0.2035 - val_accuracy: 0.6285 - val_loss: 1.6400\n",
            "Epoch 75/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9431 - loss: 0.1648 - val_accuracy: 0.6493 - val_loss: 1.7717\n",
            "Epoch 76/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9377 - loss: 0.1792 - val_accuracy: 0.6424 - val_loss: 1.7225\n",
            "Epoch 77/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9406 - loss: 0.1624 - val_accuracy: 0.6354 - val_loss: 1.7397\n",
            "Epoch 78/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9361 - loss: 0.1892 - val_accuracy: 0.6562 - val_loss: 1.6143\n",
            "Epoch 79/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9619 - loss: 0.1230 - val_accuracy: 0.6424 - val_loss: 1.6789\n",
            "Epoch 80/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9629 - loss: 0.1342 - val_accuracy: 0.6493 - val_loss: 1.7265\n",
            "Epoch 81/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9605 - loss: 0.1310 - val_accuracy: 0.6285 - val_loss: 1.9155\n",
            "Epoch 82/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9377 - loss: 0.1991 - val_accuracy: 0.6354 - val_loss: 1.8203\n",
            "Epoch 83/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9514 - loss: 0.1309 - val_accuracy: 0.6181 - val_loss: 1.9821\n",
            "Epoch 84/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9422 - loss: 0.1771 - val_accuracy: 0.6319 - val_loss: 1.7463\n",
            "Epoch 85/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9508 - loss: 0.1458 - val_accuracy: 0.6354 - val_loss: 1.8021\n",
            "Epoch 86/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9581 - loss: 0.1224 - val_accuracy: 0.6424 - val_loss: 1.7655\n",
            "Epoch 87/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9644 - loss: 0.1140 - val_accuracy: 0.6458 - val_loss: 1.7846\n",
            "Epoch 88/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9659 - loss: 0.1127 - val_accuracy: 0.6597 - val_loss: 1.8140\n",
            "Epoch 89/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9555 - loss: 0.1241 - val_accuracy: 0.6424 - val_loss: 1.9299\n",
            "Epoch 90/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9604 - loss: 0.1269 - val_accuracy: 0.6389 - val_loss: 1.7645\n",
            "Epoch 91/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9443 - loss: 0.1620 - val_accuracy: 0.6250 - val_loss: 1.8755\n",
            "Epoch 92/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9625 - loss: 0.1098 - val_accuracy: 0.6111 - val_loss: 1.9167\n",
            "Epoch 93/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9582 - loss: 0.1328 - val_accuracy: 0.6493 - val_loss: 1.8270\n",
            "Epoch 94/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9624 - loss: 0.1247 - val_accuracy: 0.6319 - val_loss: 1.9818\n",
            "Epoch 95/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9642 - loss: 0.1240 - val_accuracy: 0.6354 - val_loss: 1.9316\n",
            "Epoch 96/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9660 - loss: 0.1018 - val_accuracy: 0.6597 - val_loss: 1.8611\n",
            "Epoch 97/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9751 - loss: 0.0868 - val_accuracy: 0.6458 - val_loss: 1.9268\n",
            "Epoch 98/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9454 - loss: 0.1612 - val_accuracy: 0.6181 - val_loss: 1.9980\n",
            "Epoch 99/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9492 - loss: 0.1424 - val_accuracy: 0.6285 - val_loss: 1.7500\n",
            "Epoch 100/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9605 - loss: 0.1273 - val_accuracy: 0.6111 - val_loss: 1.9001\n",
            "Epoch 101/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9591 - loss: 0.1136 - val_accuracy: 0.6354 - val_loss: 1.7459\n",
            "Epoch 102/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9695 - loss: 0.1004 - val_accuracy: 0.6458 - val_loss: 1.7497\n",
            "Epoch 103/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9591 - loss: 0.1194 - val_accuracy: 0.6354 - val_loss: 1.8373\n",
            "Epoch 104/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9512 - loss: 0.1554 - val_accuracy: 0.6562 - val_loss: 1.6910\n",
            "Epoch 105/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9532 - loss: 0.1597 - val_accuracy: 0.6424 - val_loss: 1.8018\n",
            "Epoch 106/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9643 - loss: 0.0933 - val_accuracy: 0.6493 - val_loss: 1.8574\n",
            "Epoch 107/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9668 - loss: 0.1183 - val_accuracy: 0.6632 - val_loss: 1.9307\n",
            "Epoch 108/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9553 - loss: 0.1220 - val_accuracy: 0.6215 - val_loss: 1.9838\n",
            "Epoch 109/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9658 - loss: 0.1033 - val_accuracy: 0.6319 - val_loss: 1.9577\n",
            "Epoch 110/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9554 - loss: 0.1267 - val_accuracy: 0.6389 - val_loss: 1.8321\n",
            "Epoch 111/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9531 - loss: 0.1230 - val_accuracy: 0.6597 - val_loss: 1.9091\n",
            "Epoch 112/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9590 - loss: 0.1089 - val_accuracy: 0.6701 - val_loss: 1.8457\n",
            "Epoch 113/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9706 - loss: 0.1012 - val_accuracy: 0.6319 - val_loss: 1.9674\n",
            "Epoch 114/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9551 - loss: 0.1202 - val_accuracy: 0.6562 - val_loss: 1.8164\n",
            "Epoch 115/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9588 - loss: 0.1324 - val_accuracy: 0.6458 - val_loss: 1.7568\n",
            "Epoch 116/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9617 - loss: 0.1374 - val_accuracy: 0.6424 - val_loss: 1.7913\n",
            "Epoch 117/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9675 - loss: 0.1087 - val_accuracy: 0.6424 - val_loss: 1.7345\n",
            "Epoch 118/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9772 - loss: 0.0623 - val_accuracy: 0.6389 - val_loss: 1.9283\n",
            "Epoch 119/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9682 - loss: 0.0892 - val_accuracy: 0.6354 - val_loss: 2.1113\n",
            "Epoch 120/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9864 - loss: 0.0475 - val_accuracy: 0.6458 - val_loss: 2.0581\n",
            "Epoch 121/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9817 - loss: 0.0598 - val_accuracy: 0.6493 - val_loss: 2.1509\n",
            "Epoch 122/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9806 - loss: 0.0487 - val_accuracy: 0.6528 - val_loss: 2.1762\n",
            "Epoch 123/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9589 - loss: 0.1148 - val_accuracy: 0.6215 - val_loss: 2.1244\n",
            "Epoch 124/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9478 - loss: 0.1628 - val_accuracy: 0.6389 - val_loss: 2.0467\n",
            "Epoch 125/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9615 - loss: 0.1191 - val_accuracy: 0.6354 - val_loss: 2.0551\n",
            "Epoch 126/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9539 - loss: 0.1611 - val_accuracy: 0.6389 - val_loss: 1.8441\n",
            "Epoch 127/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9716 - loss: 0.0898 - val_accuracy: 0.6528 - val_loss: 1.7918\n",
            "Epoch 128/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9705 - loss: 0.0897 - val_accuracy: 0.6181 - val_loss: 2.1492\n",
            "Epoch 129/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9704 - loss: 0.1450 - val_accuracy: 0.6424 - val_loss: 1.7237\n",
            "Epoch 130/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9755 - loss: 0.0851 - val_accuracy: 0.6562 - val_loss: 1.7294\n",
            "Epoch 131/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9770 - loss: 0.0643 - val_accuracy: 0.6597 - val_loss: 1.9912\n",
            "Epoch 132/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9725 - loss: 0.0853 - val_accuracy: 0.6493 - val_loss: 2.1229\n",
            "Epoch 133/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9697 - loss: 0.1007 - val_accuracy: 0.6389 - val_loss: 2.0624\n",
            "Epoch 134/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9661 - loss: 0.0969 - val_accuracy: 0.6632 - val_loss: 1.9937\n",
            "Epoch 135/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9669 - loss: 0.1166 - val_accuracy: 0.6181 - val_loss: 1.9722\n",
            "Epoch 136/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9717 - loss: 0.0816 - val_accuracy: 0.6597 - val_loss: 1.9685\n",
            "Epoch 137/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9724 - loss: 0.0764 - val_accuracy: 0.6562 - val_loss: 1.9804\n",
            "Epoch 138/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9643 - loss: 0.0980 - val_accuracy: 0.6354 - val_loss: 1.9446\n",
            "Epoch 139/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9704 - loss: 0.0946 - val_accuracy: 0.6667 - val_loss: 1.9349\n",
            "Epoch 140/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9576 - loss: 0.1191 - val_accuracy: 0.6215 - val_loss: 2.0095\n",
            "Epoch 141/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9753 - loss: 0.0975 - val_accuracy: 0.6701 - val_loss: 1.6635\n",
            "Epoch 142/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9762 - loss: 0.0875 - val_accuracy: 0.6667 - val_loss: 1.8123\n",
            "Epoch 143/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9639 - loss: 0.1163 - val_accuracy: 0.6562 - val_loss: 1.9804\n",
            "Epoch 144/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9722 - loss: 0.0969 - val_accuracy: 0.6597 - val_loss: 1.9026\n",
            "Epoch 145/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9722 - loss: 0.0851 - val_accuracy: 0.6771 - val_loss: 1.9047\n",
            "Epoch 146/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9694 - loss: 0.0805 - val_accuracy: 0.6424 - val_loss: 2.1506\n",
            "Epoch 147/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9798 - loss: 0.0668 - val_accuracy: 0.6667 - val_loss: 1.9480\n",
            "Epoch 148/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9583 - loss: 0.1244 - val_accuracy: 0.6319 - val_loss: 2.0538\n",
            "Epoch 149/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9643 - loss: 0.1034 - val_accuracy: 0.6285 - val_loss: 2.0038\n",
            "Epoch 150/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9678 - loss: 0.0889 - val_accuracy: 0.6701 - val_loss: 1.9664\n",
            "Epoch 151/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9735 - loss: 0.1041 - val_accuracy: 0.6458 - val_loss: 1.9317\n",
            "Epoch 152/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9802 - loss: 0.0728 - val_accuracy: 0.6562 - val_loss: 2.0270\n",
            "Epoch 153/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9740 - loss: 0.0780 - val_accuracy: 0.6354 - val_loss: 2.0992\n",
            "Epoch 154/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9766 - loss: 0.0524 - val_accuracy: 0.6389 - val_loss: 2.2855\n",
            "Epoch 155/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9828 - loss: 0.0542 - val_accuracy: 0.6181 - val_loss: 2.2543\n",
            "Epoch 156/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9795 - loss: 0.0737 - val_accuracy: 0.6424 - val_loss: 2.0673\n",
            "Epoch 157/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9642 - loss: 0.1045 - val_accuracy: 0.6493 - val_loss: 1.9652\n",
            "Epoch 158/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9678 - loss: 0.0814 - val_accuracy: 0.6562 - val_loss: 2.0332\n",
            "Epoch 159/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9813 - loss: 0.0604 - val_accuracy: 0.6458 - val_loss: 1.9003\n",
            "Epoch 160/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9712 - loss: 0.0860 - val_accuracy: 0.6562 - val_loss: 1.9613\n",
            "Epoch 161/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9693 - loss: 0.1057 - val_accuracy: 0.6424 - val_loss: 1.9724\n",
            "Epoch 162/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9749 - loss: 0.0900 - val_accuracy: 0.6632 - val_loss: 1.9686\n",
            "Epoch 163/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9713 - loss: 0.0805 - val_accuracy: 0.6493 - val_loss: 1.9894\n",
            "Epoch 164/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9732 - loss: 0.0660 - val_accuracy: 0.6319 - val_loss: 2.1690\n",
            "Epoch 165/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9784 - loss: 0.0702 - val_accuracy: 0.6354 - val_loss: 1.9825\n",
            "Epoch 166/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9794 - loss: 0.0660 - val_accuracy: 0.6250 - val_loss: 2.0068\n",
            "Epoch 167/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9716 - loss: 0.0837 - val_accuracy: 0.6806 - val_loss: 1.9980\n",
            "Epoch 168/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9660 - loss: 0.0981 - val_accuracy: 0.6806 - val_loss: 1.9252\n",
            "Epoch 169/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9561 - loss: 0.1070 - val_accuracy: 0.6667 - val_loss: 1.8569\n",
            "Epoch 170/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9804 - loss: 0.0600 - val_accuracy: 0.6458 - val_loss: 1.9922\n",
            "Epoch 171/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9776 - loss: 0.0575 - val_accuracy: 0.6667 - val_loss: 2.2242\n",
            "Epoch 172/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9848 - loss: 0.0580 - val_accuracy: 0.6701 - val_loss: 2.0194\n",
            "Epoch 173/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9849 - loss: 0.0514 - val_accuracy: 0.6701 - val_loss: 2.0534\n",
            "Epoch 174/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9788 - loss: 0.0694 - val_accuracy: 0.6632 - val_loss: 2.2762\n",
            "Epoch 175/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9797 - loss: 0.0831 - val_accuracy: 0.6458 - val_loss: 2.0911\n",
            "Epoch 176/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9731 - loss: 0.0930 - val_accuracy: 0.5903 - val_loss: 2.2441\n",
            "Epoch 177/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9626 - loss: 0.1206 - val_accuracy: 0.6146 - val_loss: 1.9867\n",
            "Epoch 178/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9725 - loss: 0.0889 - val_accuracy: 0.6354 - val_loss: 2.1184\n",
            "Epoch 179/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9665 - loss: 0.0937 - val_accuracy: 0.6424 - val_loss: 1.9873\n",
            "Epoch 180/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9564 - loss: 0.1101 - val_accuracy: 0.6632 - val_loss: 1.9322\n",
            "Epoch 181/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9778 - loss: 0.0685 - val_accuracy: 0.6771 - val_loss: 1.8791\n",
            "Epoch 182/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9798 - loss: 0.0577 - val_accuracy: 0.6250 - val_loss: 2.2092\n",
            "Epoch 183/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9826 - loss: 0.0529 - val_accuracy: 0.6424 - val_loss: 2.2600\n",
            "Epoch 184/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9776 - loss: 0.0757 - val_accuracy: 0.6458 - val_loss: 2.2435\n",
            "Epoch 185/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9784 - loss: 0.0548 - val_accuracy: 0.6528 - val_loss: 2.1702\n",
            "Epoch 186/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9843 - loss: 0.0769 - val_accuracy: 0.6528 - val_loss: 2.0434\n",
            "Epoch 187/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9739 - loss: 0.0736 - val_accuracy: 0.6458 - val_loss: 2.1958\n",
            "Epoch 188/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9723 - loss: 0.0814 - val_accuracy: 0.6528 - val_loss: 2.0277\n",
            "Epoch 189/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9850 - loss: 0.0531 - val_accuracy: 0.6771 - val_loss: 1.9039\n",
            "Epoch 190/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9842 - loss: 0.0538 - val_accuracy: 0.6597 - val_loss: 1.9091\n",
            "Epoch 191/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9726 - loss: 0.1042 - val_accuracy: 0.6667 - val_loss: 2.1173\n",
            "Epoch 192/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9777 - loss: 0.0657 - val_accuracy: 0.6562 - val_loss: 2.1776\n",
            "Epoch 193/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9889 - loss: 0.0496 - val_accuracy: 0.6667 - val_loss: 2.2799\n",
            "Epoch 194/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9793 - loss: 0.0640 - val_accuracy: 0.6597 - val_loss: 2.2030\n",
            "Epoch 195/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9802 - loss: 0.0634 - val_accuracy: 0.6667 - val_loss: 2.2303\n",
            "Epoch 196/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9801 - loss: 0.0740 - val_accuracy: 0.6597 - val_loss: 2.3915\n",
            "Epoch 197/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9827 - loss: 0.0584 - val_accuracy: 0.6354 - val_loss: 2.1743\n",
            "Epoch 198/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9750 - loss: 0.0992 - val_accuracy: 0.6528 - val_loss: 2.2069\n",
            "Epoch 199/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9745 - loss: 0.0626 - val_accuracy: 0.6667 - val_loss: 2.0317\n",
            "Epoch 200/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9810 - loss: 0.0593 - val_accuracy: 0.6319 - val_loss: 2.1613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP model parameters\n",
        "num_layers = 6\n",
        "units = [65, 260, 364, 52, 52,26]\n",
        "activations = ['relu', 'relu', 'relu', 'relu', 'relu','relu']\n",
        "dropouts = [0.1, 0.3, 0.4, 0.1, 0.1,0.5]\n",
        "learning_rate = 0.0010343990168720423\n",
        "loss_function = 'sparse_categorical_crossentropy'\n",
        "input_shape=(13,)"
      ],
      "metadata": {
        "id": "3skBYyxTBMcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the MLP model\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Input layer\n",
        "model.add(layers.Input(shape=input_shape))\n",
        "\n",
        "# Hidden layers\n",
        "for i in range(num_layers):\n",
        "    model.add(layers.Dense(units[i], activation=activations[i]))\n",
        "    model.add(layers.Dropout(dropouts[i]))\n",
        "\n",
        "# Output layer (adjust the number of units to your number of classes)\n",
        "model.add(layers.Dense(8, activation='softmax'))  # Assuming 8 classes for output\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=loss_function,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "dRGN9LiYBMZc",
        "outputId": "cc3dcc83-8cb1-4a74-e82a-bfc34175b66b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │             \u001b[38;5;34m910\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_23 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │          \u001b[38;5;34m17,160\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │          \u001b[38;5;34m95,004\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │          \u001b[38;5;34m18,980\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │           \u001b[38;5;34m2,756\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_27 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_32 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m)                  │           \u001b[38;5;34m1,378\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_28 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_33 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │             \u001b[38;5;34m216\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">910</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,160</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">95,004</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,980</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,756</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,378</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">216</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m136,404\u001b[0m (532.83 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">136,404</span> (532.83 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m136,404\u001b[0m (532.83 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">136,404</span> (532.83 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=200,batch_size=32, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Avd-CrMGBMWw",
        "outputId": "d93364dd-e66d-4fce-8872-b424df936a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.1260 - loss: 2.0862 - val_accuracy: 0.2222 - val_loss: 2.0089\n",
            "Epoch 2/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2054 - loss: 2.0185 - val_accuracy: 0.3333 - val_loss: 1.9350\n",
            "Epoch 3/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2043 - loss: 1.9862 - val_accuracy: 0.3090 - val_loss: 1.8928\n",
            "Epoch 4/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2339 - loss: 1.9377 - val_accuracy: 0.3403 - val_loss: 1.8407\n",
            "Epoch 5/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2571 - loss: 1.9415 - val_accuracy: 0.3611 - val_loss: 1.8076\n",
            "Epoch 6/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2469 - loss: 1.8711 - val_accuracy: 0.3438 - val_loss: 1.6984\n",
            "Epoch 7/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2791 - loss: 1.8499 - val_accuracy: 0.3507 - val_loss: 1.6802\n",
            "Epoch 8/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3113 - loss: 1.7885 - val_accuracy: 0.4028 - val_loss: 1.6379\n",
            "Epoch 9/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2906 - loss: 1.7879 - val_accuracy: 0.4201 - val_loss: 1.6183\n",
            "Epoch 10/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3184 - loss: 1.7886 - val_accuracy: 0.4062 - val_loss: 1.5869\n",
            "Epoch 11/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3411 - loss: 1.7225 - val_accuracy: 0.4514 - val_loss: 1.5675\n",
            "Epoch 12/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3623 - loss: 1.6938 - val_accuracy: 0.4444 - val_loss: 1.5152\n",
            "Epoch 13/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3731 - loss: 1.6609 - val_accuracy: 0.4549 - val_loss: 1.5062\n",
            "Epoch 14/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3924 - loss: 1.6221 - val_accuracy: 0.5000 - val_loss: 1.4793\n",
            "Epoch 15/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4053 - loss: 1.6353 - val_accuracy: 0.4479 - val_loss: 1.4885\n",
            "Epoch 16/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3984 - loss: 1.6164 - val_accuracy: 0.4618 - val_loss: 1.4606\n",
            "Epoch 17/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4560 - loss: 1.4931 - val_accuracy: 0.5035 - val_loss: 1.3999\n",
            "Epoch 18/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4328 - loss: 1.5540 - val_accuracy: 0.5243 - val_loss: 1.4230\n",
            "Epoch 19/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4640 - loss: 1.5092 - val_accuracy: 0.4965 - val_loss: 1.4117\n",
            "Epoch 20/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4725 - loss: 1.4647 - val_accuracy: 0.5139 - val_loss: 1.3673\n",
            "Epoch 21/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4275 - loss: 1.4781 - val_accuracy: 0.5069 - val_loss: 1.3662\n",
            "Epoch 22/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4845 - loss: 1.4244 - val_accuracy: 0.5000 - val_loss: 1.3884\n",
            "Epoch 23/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4611 - loss: 1.4522 - val_accuracy: 0.4861 - val_loss: 1.3905\n",
            "Epoch 24/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4928 - loss: 1.4148 - val_accuracy: 0.5035 - val_loss: 1.3680\n",
            "Epoch 25/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5164 - loss: 1.3352 - val_accuracy: 0.5174 - val_loss: 1.3308\n",
            "Epoch 26/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5031 - loss: 1.3479 - val_accuracy: 0.5174 - val_loss: 1.3056\n",
            "Epoch 27/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5457 - loss: 1.2691 - val_accuracy: 0.5486 - val_loss: 1.2677\n",
            "Epoch 28/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5448 - loss: 1.3056 - val_accuracy: 0.5417 - val_loss: 1.2647\n",
            "Epoch 29/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5360 - loss: 1.2727 - val_accuracy: 0.5451 - val_loss: 1.2799\n",
            "Epoch 30/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5686 - loss: 1.2308 - val_accuracy: 0.5417 - val_loss: 1.3049\n",
            "Epoch 31/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5601 - loss: 1.2623 - val_accuracy: 0.5451 - val_loss: 1.2687\n",
            "Epoch 32/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5696 - loss: 1.2934 - val_accuracy: 0.5347 - val_loss: 1.2444\n",
            "Epoch 33/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5983 - loss: 1.1865 - val_accuracy: 0.5382 - val_loss: 1.3293\n",
            "Epoch 34/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5730 - loss: 1.1669 - val_accuracy: 0.5451 - val_loss: 1.3348\n",
            "Epoch 35/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5677 - loss: 1.1957 - val_accuracy: 0.5278 - val_loss: 1.2741\n",
            "Epoch 36/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5659 - loss: 1.1466 - val_accuracy: 0.5521 - val_loss: 1.2557\n",
            "Epoch 37/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5959 - loss: 1.1314 - val_accuracy: 0.5486 - val_loss: 1.2665\n",
            "Epoch 38/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6336 - loss: 1.1112 - val_accuracy: 0.5417 - val_loss: 1.2356\n",
            "Epoch 39/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6383 - loss: 1.0861 - val_accuracy: 0.5625 - val_loss: 1.2305\n",
            "Epoch 40/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6222 - loss: 1.0671 - val_accuracy: 0.5521 - val_loss: 1.2812\n",
            "Epoch 41/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6083 - loss: 1.1275 - val_accuracy: 0.5660 - val_loss: 1.2314\n",
            "Epoch 42/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6436 - loss: 1.0353 - val_accuracy: 0.5451 - val_loss: 1.2412\n",
            "Epoch 43/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6303 - loss: 1.0187 - val_accuracy: 0.5729 - val_loss: 1.2414\n",
            "Epoch 44/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6479 - loss: 1.0511 - val_accuracy: 0.5833 - val_loss: 1.2021\n",
            "Epoch 45/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6549 - loss: 0.9988 - val_accuracy: 0.5625 - val_loss: 1.2321\n",
            "Epoch 46/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6713 - loss: 0.9882 - val_accuracy: 0.5694 - val_loss: 1.3137\n",
            "Epoch 47/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6376 - loss: 1.0605 - val_accuracy: 0.5694 - val_loss: 1.2772\n",
            "Epoch 48/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6371 - loss: 1.0527 - val_accuracy: 0.5694 - val_loss: 1.2777\n",
            "Epoch 49/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6628 - loss: 1.0286 - val_accuracy: 0.5799 - val_loss: 1.3287\n",
            "Epoch 50/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6743 - loss: 0.9485 - val_accuracy: 0.5556 - val_loss: 1.3034\n",
            "Epoch 51/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6623 - loss: 1.0219 - val_accuracy: 0.5903 - val_loss: 1.2903\n",
            "Epoch 52/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6814 - loss: 0.9353 - val_accuracy: 0.5833 - val_loss: 1.2923\n",
            "Epoch 53/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6568 - loss: 0.9784 - val_accuracy: 0.5729 - val_loss: 1.2853\n",
            "Epoch 54/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6832 - loss: 0.9275 - val_accuracy: 0.5833 - val_loss: 1.2259\n",
            "Epoch 55/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7082 - loss: 0.8823 - val_accuracy: 0.5694 - val_loss: 1.2370\n",
            "Epoch 56/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6719 - loss: 0.9122 - val_accuracy: 0.5660 - val_loss: 1.3354\n",
            "Epoch 57/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6956 - loss: 0.8930 - val_accuracy: 0.5833 - val_loss: 1.3133\n",
            "Epoch 58/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7000 - loss: 0.8989 - val_accuracy: 0.5729 - val_loss: 1.2610\n",
            "Epoch 59/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7141 - loss: 0.8897 - val_accuracy: 0.5729 - val_loss: 1.3281\n",
            "Epoch 60/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7035 - loss: 0.9037 - val_accuracy: 0.5833 - val_loss: 1.3393\n",
            "Epoch 61/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6978 - loss: 0.9287 - val_accuracy: 0.5729 - val_loss: 1.3809\n",
            "Epoch 62/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7112 - loss: 0.8435 - val_accuracy: 0.6007 - val_loss: 1.3107\n",
            "Epoch 63/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7187 - loss: 0.8119 - val_accuracy: 0.5694 - val_loss: 1.3330\n",
            "Epoch 64/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7292 - loss: 0.8618 - val_accuracy: 0.5694 - val_loss: 1.3297\n",
            "Epoch 65/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7205 - loss: 0.7801 - val_accuracy: 0.5868 - val_loss: 1.4518\n",
            "Epoch 66/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7209 - loss: 0.7823 - val_accuracy: 0.5764 - val_loss: 1.4112\n",
            "Epoch 67/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7420 - loss: 0.8287 - val_accuracy: 0.5799 - val_loss: 1.3366\n",
            "Epoch 68/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7545 - loss: 0.7538 - val_accuracy: 0.5972 - val_loss: 1.3281\n",
            "Epoch 69/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7030 - loss: 0.8541 - val_accuracy: 0.5729 - val_loss: 1.3531\n",
            "Epoch 70/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7450 - loss: 0.7722 - val_accuracy: 0.5903 - val_loss: 1.3158\n",
            "Epoch 71/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7223 - loss: 0.8631 - val_accuracy: 0.6007 - val_loss: 1.2764\n",
            "Epoch 72/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7523 - loss: 0.7507 - val_accuracy: 0.6007 - val_loss: 1.3822\n",
            "Epoch 73/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7326 - loss: 0.8034 - val_accuracy: 0.6042 - val_loss: 1.3072\n",
            "Epoch 74/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7435 - loss: 0.7272 - val_accuracy: 0.6007 - val_loss: 1.3703\n",
            "Epoch 75/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7546 - loss: 0.7444 - val_accuracy: 0.6042 - val_loss: 1.3585\n",
            "Epoch 76/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7617 - loss: 0.7198 - val_accuracy: 0.5764 - val_loss: 1.4177\n",
            "Epoch 77/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7386 - loss: 0.7852 - val_accuracy: 0.6319 - val_loss: 1.3215\n",
            "Epoch 78/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7667 - loss: 0.7391 - val_accuracy: 0.6042 - val_loss: 1.2834\n",
            "Epoch 79/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7934 - loss: 0.6665 - val_accuracy: 0.5938 - val_loss: 1.4086\n",
            "Epoch 80/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7827 - loss: 0.7821 - val_accuracy: 0.5972 - val_loss: 1.3564\n",
            "Epoch 81/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7530 - loss: 0.7324 - val_accuracy: 0.6111 - val_loss: 1.4150\n",
            "Epoch 82/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7492 - loss: 0.8158 - val_accuracy: 0.5660 - val_loss: 1.3017\n",
            "Epoch 83/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7652 - loss: 0.6847 - val_accuracy: 0.5764 - val_loss: 1.3288\n",
            "Epoch 84/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7455 - loss: 0.7608 - val_accuracy: 0.5903 - val_loss: 1.3696\n",
            "Epoch 85/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7835 - loss: 0.6820 - val_accuracy: 0.6354 - val_loss: 1.3784\n",
            "Epoch 86/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8179 - loss: 0.6383 - val_accuracy: 0.6146 - val_loss: 1.3747\n",
            "Epoch 87/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7869 - loss: 0.7064 - val_accuracy: 0.5833 - val_loss: 1.4129\n",
            "Epoch 88/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7994 - loss: 0.6284 - val_accuracy: 0.5694 - val_loss: 1.4562\n",
            "Epoch 89/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8093 - loss: 0.6419 - val_accuracy: 0.5799 - val_loss: 1.4143\n",
            "Epoch 90/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7990 - loss: 0.5972 - val_accuracy: 0.5868 - val_loss: 1.3991\n",
            "Epoch 91/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8086 - loss: 0.6405 - val_accuracy: 0.6146 - val_loss: 1.4062\n",
            "Epoch 92/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8162 - loss: 0.5572 - val_accuracy: 0.6042 - val_loss: 1.4581\n",
            "Epoch 93/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8163 - loss: 0.5499 - val_accuracy: 0.6042 - val_loss: 1.4411\n",
            "Epoch 94/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7963 - loss: 0.6869 - val_accuracy: 0.6181 - val_loss: 1.3647\n",
            "Epoch 95/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7928 - loss: 0.6474 - val_accuracy: 0.5972 - val_loss: 1.4133\n",
            "Epoch 96/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8416 - loss: 0.6141 - val_accuracy: 0.5938 - val_loss: 1.4181\n",
            "Epoch 97/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8105 - loss: 0.6744 - val_accuracy: 0.6042 - val_loss: 1.4090\n",
            "Epoch 98/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8257 - loss: 0.5397 - val_accuracy: 0.6042 - val_loss: 1.4246\n",
            "Epoch 99/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8036 - loss: 0.5839 - val_accuracy: 0.6250 - val_loss: 1.4079\n",
            "Epoch 100/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8531 - loss: 0.4741 - val_accuracy: 0.6146 - val_loss: 1.4153\n",
            "Epoch 101/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8315 - loss: 0.5859 - val_accuracy: 0.6146 - val_loss: 1.3734\n",
            "Epoch 102/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8176 - loss: 0.5992 - val_accuracy: 0.6111 - val_loss: 1.4473\n",
            "Epoch 103/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8303 - loss: 0.5118 - val_accuracy: 0.5938 - val_loss: 1.5144\n",
            "Epoch 104/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8243 - loss: 0.5878 - val_accuracy: 0.6076 - val_loss: 1.3816\n",
            "Epoch 105/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8466 - loss: 0.5034 - val_accuracy: 0.6285 - val_loss: 1.4048\n",
            "Epoch 106/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7987 - loss: 0.6611 - val_accuracy: 0.6181 - val_loss: 1.4164\n",
            "Epoch 107/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8246 - loss: 0.5307 - val_accuracy: 0.6389 - val_loss: 1.4289\n",
            "Epoch 108/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8302 - loss: 0.5116 - val_accuracy: 0.6528 - val_loss: 1.4177\n",
            "Epoch 109/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8382 - loss: 0.5576 - val_accuracy: 0.6250 - val_loss: 1.4225\n",
            "Epoch 110/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8149 - loss: 0.6041 - val_accuracy: 0.6146 - val_loss: 1.3647\n",
            "Epoch 111/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8261 - loss: 0.6300 - val_accuracy: 0.6389 - val_loss: 1.3839\n",
            "Epoch 112/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8621 - loss: 0.4683 - val_accuracy: 0.6319 - val_loss: 1.4182\n",
            "Epoch 113/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8402 - loss: 0.5267 - val_accuracy: 0.6319 - val_loss: 1.3564\n",
            "Epoch 114/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8257 - loss: 0.5487 - val_accuracy: 0.5972 - val_loss: 1.4675\n",
            "Epoch 115/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8066 - loss: 0.5535 - val_accuracy: 0.6181 - val_loss: 1.3984\n",
            "Epoch 116/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8516 - loss: 0.4935 - val_accuracy: 0.6111 - val_loss: 1.3968\n",
            "Epoch 117/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8291 - loss: 0.5470 - val_accuracy: 0.6250 - val_loss: 1.4023\n",
            "Epoch 118/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8267 - loss: 0.5867 - val_accuracy: 0.6285 - val_loss: 1.3047\n",
            "Epoch 119/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8385 - loss: 0.4925 - val_accuracy: 0.6250 - val_loss: 1.3771\n",
            "Epoch 120/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8608 - loss: 0.4304 - val_accuracy: 0.6389 - val_loss: 1.4717\n",
            "Epoch 121/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8715 - loss: 0.4582 - val_accuracy: 0.6146 - val_loss: 1.5585\n",
            "Epoch 122/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8552 - loss: 0.4833 - val_accuracy: 0.6250 - val_loss: 1.4953\n",
            "Epoch 123/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8749 - loss: 0.4517 - val_accuracy: 0.6042 - val_loss: 1.5513\n",
            "Epoch 124/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8482 - loss: 0.5240 - val_accuracy: 0.6042 - val_loss: 1.5427\n",
            "Epoch 125/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8316 - loss: 0.5881 - val_accuracy: 0.6146 - val_loss: 1.4684\n",
            "Epoch 126/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8415 - loss: 0.5437 - val_accuracy: 0.6076 - val_loss: 1.5112\n",
            "Epoch 127/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8500 - loss: 0.4859 - val_accuracy: 0.6215 - val_loss: 1.4334\n",
            "Epoch 128/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8295 - loss: 0.5391 - val_accuracy: 0.6285 - val_loss: 1.4291\n",
            "Epoch 129/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8405 - loss: 0.5288 - val_accuracy: 0.6354 - val_loss: 1.4651\n",
            "Epoch 130/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8561 - loss: 0.4899 - val_accuracy: 0.6389 - val_loss: 1.3902\n",
            "Epoch 131/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8729 - loss: 0.4074 - val_accuracy: 0.6215 - val_loss: 1.4990\n",
            "Epoch 132/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8607 - loss: 0.4787 - val_accuracy: 0.6250 - val_loss: 1.5181\n",
            "Epoch 133/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8498 - loss: 0.4730 - val_accuracy: 0.6215 - val_loss: 1.5386\n",
            "Epoch 134/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8510 - loss: 0.5253 - val_accuracy: 0.6285 - val_loss: 1.5036\n",
            "Epoch 135/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8496 - loss: 0.4749 - val_accuracy: 0.6250 - val_loss: 1.5670\n",
            "Epoch 136/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8746 - loss: 0.4464 - val_accuracy: 0.6146 - val_loss: 1.6246\n",
            "Epoch 137/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8477 - loss: 0.5088 - val_accuracy: 0.6181 - val_loss: 1.5166\n",
            "Epoch 138/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8575 - loss: 0.4425 - val_accuracy: 0.6181 - val_loss: 1.5927\n",
            "Epoch 139/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8564 - loss: 0.5146 - val_accuracy: 0.6250 - val_loss: 1.6429\n",
            "Epoch 140/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8574 - loss: 0.4800 - val_accuracy: 0.6250 - val_loss: 1.6080\n",
            "Epoch 141/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8718 - loss: 0.4133 - val_accuracy: 0.6215 - val_loss: 1.6083\n",
            "Epoch 142/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8629 - loss: 0.4236 - val_accuracy: 0.6250 - val_loss: 1.6225\n",
            "Epoch 143/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8661 - loss: 0.4537 - val_accuracy: 0.6424 - val_loss: 1.6422\n",
            "Epoch 144/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8607 - loss: 0.4437 - val_accuracy: 0.6146 - val_loss: 1.6774\n",
            "Epoch 145/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8672 - loss: 0.4488 - val_accuracy: 0.6181 - val_loss: 1.6439\n",
            "Epoch 146/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8614 - loss: 0.4461 - val_accuracy: 0.6597 - val_loss: 1.4219\n",
            "Epoch 147/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8721 - loss: 0.4462 - val_accuracy: 0.6319 - val_loss: 1.4559\n",
            "Epoch 148/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8753 - loss: 0.4699 - val_accuracy: 0.6354 - val_loss: 1.5080\n",
            "Epoch 149/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8707 - loss: 0.4384 - val_accuracy: 0.6250 - val_loss: 1.5397\n",
            "Epoch 150/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8746 - loss: 0.4361 - val_accuracy: 0.6493 - val_loss: 1.4878\n",
            "Epoch 151/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8761 - loss: 0.3871 - val_accuracy: 0.6285 - val_loss: 1.5435\n",
            "Epoch 152/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8752 - loss: 0.4399 - val_accuracy: 0.6319 - val_loss: 1.6502\n",
            "Epoch 153/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8869 - loss: 0.4240 - val_accuracy: 0.6424 - val_loss: 1.4657\n",
            "Epoch 154/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8857 - loss: 0.4200 - val_accuracy: 0.6493 - val_loss: 1.4683\n",
            "Epoch 155/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8707 - loss: 0.4213 - val_accuracy: 0.6562 - val_loss: 1.4907\n",
            "Epoch 156/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8910 - loss: 0.4014 - val_accuracy: 0.6458 - val_loss: 1.5662\n",
            "Epoch 157/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8776 - loss: 0.4243 - val_accuracy: 0.6111 - val_loss: 1.6888\n",
            "Epoch 158/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8608 - loss: 0.4849 - val_accuracy: 0.6146 - val_loss: 1.5892\n",
            "Epoch 159/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8655 - loss: 0.4595 - val_accuracy: 0.6493 - val_loss: 1.5541\n",
            "Epoch 160/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8829 - loss: 0.3833 - val_accuracy: 0.6181 - val_loss: 1.6992\n",
            "Epoch 161/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8839 - loss: 0.4011 - val_accuracy: 0.6528 - val_loss: 1.6883\n",
            "Epoch 162/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8761 - loss: 0.4468 - val_accuracy: 0.6354 - val_loss: 1.5357\n",
            "Epoch 163/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8583 - loss: 0.4164 - val_accuracy: 0.6215 - val_loss: 1.5230\n",
            "Epoch 164/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8813 - loss: 0.4571 - val_accuracy: 0.6285 - val_loss: 1.6021\n",
            "Epoch 165/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8787 - loss: 0.3988 - val_accuracy: 0.6146 - val_loss: 1.5578\n",
            "Epoch 166/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8832 - loss: 0.3921 - val_accuracy: 0.6389 - val_loss: 1.5835\n",
            "Epoch 167/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8987 - loss: 0.4221 - val_accuracy: 0.6528 - val_loss: 1.5737\n",
            "Epoch 168/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8704 - loss: 0.4318 - val_accuracy: 0.6597 - val_loss: 1.5687\n",
            "Epoch 169/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8735 - loss: 0.4491 - val_accuracy: 0.6458 - val_loss: 1.5845\n",
            "Epoch 170/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8617 - loss: 0.4500 - val_accuracy: 0.6354 - val_loss: 1.5525\n",
            "Epoch 171/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8857 - loss: 0.4160 - val_accuracy: 0.6389 - val_loss: 1.5835\n",
            "Epoch 172/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8752 - loss: 0.4121 - val_accuracy: 0.6285 - val_loss: 1.6688\n",
            "Epoch 173/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8946 - loss: 0.3804 - val_accuracy: 0.6424 - val_loss: 1.6011\n",
            "Epoch 174/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8804 - loss: 0.4820 - val_accuracy: 0.6215 - val_loss: 1.5969\n",
            "Epoch 175/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8881 - loss: 0.3631 - val_accuracy: 0.6319 - val_loss: 1.6396\n",
            "Epoch 176/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8998 - loss: 0.3424 - val_accuracy: 0.6458 - val_loss: 1.6953\n",
            "Epoch 177/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9119 - loss: 0.3395 - val_accuracy: 0.6285 - val_loss: 1.7329\n",
            "Epoch 178/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8827 - loss: 0.3551 - val_accuracy: 0.6458 - val_loss: 1.6962\n",
            "Epoch 179/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8835 - loss: 0.4116 - val_accuracy: 0.6285 - val_loss: 1.7283\n",
            "Epoch 180/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8824 - loss: 0.3972 - val_accuracy: 0.6285 - val_loss: 1.7253\n",
            "Epoch 181/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8718 - loss: 0.4092 - val_accuracy: 0.6285 - val_loss: 1.6866\n",
            "Epoch 182/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8848 - loss: 0.3903 - val_accuracy: 0.6250 - val_loss: 1.7384\n",
            "Epoch 183/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9001 - loss: 0.3790 - val_accuracy: 0.6389 - val_loss: 1.6725\n",
            "Epoch 184/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9080 - loss: 0.3918 - val_accuracy: 0.6319 - val_loss: 1.7824\n",
            "Epoch 185/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8769 - loss: 0.3967 - val_accuracy: 0.6111 - val_loss: 1.7483\n",
            "Epoch 186/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8829 - loss: 0.4043 - val_accuracy: 0.6667 - val_loss: 1.6869\n",
            "Epoch 187/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8928 - loss: 0.3576 - val_accuracy: 0.6354 - val_loss: 1.7251\n",
            "Epoch 188/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8786 - loss: 0.4462 - val_accuracy: 0.6597 - val_loss: 1.5404\n",
            "Epoch 189/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8915 - loss: 0.3763 - val_accuracy: 0.6597 - val_loss: 1.5812\n",
            "Epoch 190/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8982 - loss: 0.3722 - val_accuracy: 0.6701 - val_loss: 1.5630\n",
            "Epoch 191/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8848 - loss: 0.3811 - val_accuracy: 0.6667 - val_loss: 1.6279\n",
            "Epoch 192/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9169 - loss: 0.2672 - val_accuracy: 0.6597 - val_loss: 1.7669\n",
            "Epoch 193/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9096 - loss: 0.3220 - val_accuracy: 0.6493 - val_loss: 1.7047\n",
            "Epoch 194/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9144 - loss: 0.3138 - val_accuracy: 0.6493 - val_loss: 1.6277\n",
            "Epoch 195/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9018 - loss: 0.3834 - val_accuracy: 0.6215 - val_loss: 1.6810\n",
            "Epoch 196/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8943 - loss: 0.3757 - val_accuracy: 0.6354 - val_loss: 1.6715\n",
            "Epoch 197/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8939 - loss: 0.3760 - val_accuracy: 0.6458 - val_loss: 1.6230\n",
            "Epoch 198/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9003 - loss: 0.3931 - val_accuracy: 0.6562 - val_loss: 1.6676\n",
            "Epoch 199/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8913 - loss: 0.3598 - val_accuracy: 0.6319 - val_loss: 1.6388\n",
            "Epoch 200/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8952 - loss: 0.3347 - val_accuracy: 0.6319 - val_loss: 1.6103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vs5mOqUCBL_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP model parameters\n",
        "num_layers = 5\n",
        "units = [65, 260, 364, 52, 52]\n",
        "activations = ['relu', 'relu', 'relu', 'relu', 'relu']\n",
        "dropouts = [0.1, 0.3, 0.4, 0.1, 0.1]\n",
        "learning_rate = 0.0010343990168720423\n",
        "loss_function = 'sparse_categorical_crossentropy'\n",
        "input_shape = (13,)"
      ],
      "metadata": {
        "id": "vXt7MiZBA6hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the MLP model\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Input layer\n",
        "model.add(layers.Input(shape=input_shape))\n",
        "\n",
        "# Hidden layers with batch normalization\n",
        "for i in range(num_layers):\n",
        "    model.add(layers.Dense(units[i]))\n",
        "    model.add(layers.BatchNormalization())  # Add batch normalization\n",
        "    model.add(layers.Activation(activations[i]))  # Apply activation after batch normalization\n",
        "    model.add(layers.Dropout(dropouts[i]))  # Add dropout for regularization\n",
        "\n",
        "# Output layer (adjust the number of units to your number of classes)\n",
        "model.add(layers.Dense(8, activation='softmax'))  # Assuming 8 classes for output\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=loss_function,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        },
        "id": "R8b4OW6tA6eC",
        "outputId": "156322ff-0357-42d5-d710-ef4673ad8848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │             \u001b[38;5;34m910\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │             \u001b[38;5;34m260\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │          \u001b[38;5;34m17,160\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_8                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │           \u001b[38;5;34m1,040\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │          \u001b[38;5;34m95,004\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_9                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │           \u001b[38;5;34m1,456\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │          \u001b[38;5;34m18,980\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_10               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │             \u001b[38;5;34m208\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │           \u001b[38;5;34m2,756\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_11               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │             \u001b[38;5;34m208\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │             \u001b[38;5;34m424\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">910</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,160</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_8                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">95,004</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_9                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,456</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,980</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_10               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,756</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_11               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">424</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m138,406\u001b[0m (540.65 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">138,406</span> (540.65 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m136,820\u001b[0m (534.45 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">136,820</span> (534.45 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,586\u001b[0m (6.20 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,586</span> (6.20 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=200,batch_size=32, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl7wACRlA6P7",
        "outputId": "76379ab0-2605-42f2-b523-5d77e2631c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.1416 - loss: 2.3471 - val_accuracy: 0.2604 - val_loss: 1.9943\n",
            "Epoch 2/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.2369 - loss: 1.9926 - val_accuracy: 0.3160 - val_loss: 1.9399\n",
            "Epoch 3/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2904 - loss: 1.8482 - val_accuracy: 0.3472 - val_loss: 1.8477\n",
            "Epoch 4/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3435 - loss: 1.8061 - val_accuracy: 0.3472 - val_loss: 1.7936\n",
            "Epoch 5/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3339 - loss: 1.7242 - val_accuracy: 0.3958 - val_loss: 1.7294\n",
            "Epoch 6/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3592 - loss: 1.6830 - val_accuracy: 0.4340 - val_loss: 1.6619\n",
            "Epoch 7/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3588 - loss: 1.6768 - val_accuracy: 0.4236 - val_loss: 1.6048\n",
            "Epoch 8/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3647 - loss: 1.6591 - val_accuracy: 0.4375 - val_loss: 1.5589\n",
            "Epoch 9/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4131 - loss: 1.6197 - val_accuracy: 0.4514 - val_loss: 1.5120\n",
            "Epoch 10/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4139 - loss: 1.5175 - val_accuracy: 0.4722 - val_loss: 1.5145\n",
            "Epoch 11/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4171 - loss: 1.5370 - val_accuracy: 0.4688 - val_loss: 1.4601\n",
            "Epoch 12/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4338 - loss: 1.4735 - val_accuracy: 0.4861 - val_loss: 1.4125\n",
            "Epoch 13/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4530 - loss: 1.5197 - val_accuracy: 0.5035 - val_loss: 1.3791\n",
            "Epoch 14/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4765 - loss: 1.4352 - val_accuracy: 0.5312 - val_loss: 1.3463\n",
            "Epoch 15/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4331 - loss: 1.4781 - val_accuracy: 0.5069 - val_loss: 1.3221\n",
            "Epoch 16/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4758 - loss: 1.3803 - val_accuracy: 0.5382 - val_loss: 1.3093\n",
            "Epoch 17/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4908 - loss: 1.3794 - val_accuracy: 0.5382 - val_loss: 1.3124\n",
            "Epoch 18/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4647 - loss: 1.3580 - val_accuracy: 0.5347 - val_loss: 1.2953\n",
            "Epoch 19/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4755 - loss: 1.3791 - val_accuracy: 0.5347 - val_loss: 1.2876\n",
            "Epoch 20/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4923 - loss: 1.3606 - val_accuracy: 0.5556 - val_loss: 1.2621\n",
            "Epoch 21/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5146 - loss: 1.3020 - val_accuracy: 0.5590 - val_loss: 1.2441\n",
            "Epoch 22/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5009 - loss: 1.3051 - val_accuracy: 0.5694 - val_loss: 1.2557\n",
            "Epoch 23/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5345 - loss: 1.3097 - val_accuracy: 0.5903 - val_loss: 1.2212\n",
            "Epoch 24/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5243 - loss: 1.2977 - val_accuracy: 0.5938 - val_loss: 1.2136\n",
            "Epoch 25/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5216 - loss: 1.2816 - val_accuracy: 0.5521 - val_loss: 1.2136\n",
            "Epoch 26/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5284 - loss: 1.2691 - val_accuracy: 0.5833 - val_loss: 1.1939\n",
            "Epoch 27/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5251 - loss: 1.2772 - val_accuracy: 0.5486 - val_loss: 1.1895\n",
            "Epoch 28/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5230 - loss: 1.2923 - val_accuracy: 0.5868 - val_loss: 1.1909\n",
            "Epoch 29/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5343 - loss: 1.2231 - val_accuracy: 0.5556 - val_loss: 1.2063\n",
            "Epoch 30/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5719 - loss: 1.1932 - val_accuracy: 0.5660 - val_loss: 1.1766\n",
            "Epoch 31/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5446 - loss: 1.1893 - val_accuracy: 0.5625 - val_loss: 1.1780\n",
            "Epoch 32/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5662 - loss: 1.1759 - val_accuracy: 0.5694 - val_loss: 1.1629\n",
            "Epoch 33/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5538 - loss: 1.2175 - val_accuracy: 0.5764 - val_loss: 1.1352\n",
            "Epoch 34/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5811 - loss: 1.1515 - val_accuracy: 0.5903 - val_loss: 1.1316\n",
            "Epoch 35/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5560 - loss: 1.2105 - val_accuracy: 0.5764 - val_loss: 1.1375\n",
            "Epoch 36/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5711 - loss: 1.1607 - val_accuracy: 0.5660 - val_loss: 1.1491\n",
            "Epoch 37/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5831 - loss: 1.1704 - val_accuracy: 0.5694 - val_loss: 1.1126\n",
            "Epoch 38/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5538 - loss: 1.1446 - val_accuracy: 0.5694 - val_loss: 1.1321\n",
            "Epoch 39/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5530 - loss: 1.1642 - val_accuracy: 0.5694 - val_loss: 1.1274\n",
            "Epoch 40/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5941 - loss: 1.1367 - val_accuracy: 0.5833 - val_loss: 1.1216\n",
            "Epoch 41/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5948 - loss: 1.1348 - val_accuracy: 0.6111 - val_loss: 1.1001\n",
            "Epoch 42/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6048 - loss: 1.1075 - val_accuracy: 0.5938 - val_loss: 1.0946\n",
            "Epoch 43/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5811 - loss: 1.1158 - val_accuracy: 0.5938 - val_loss: 1.1024\n",
            "Epoch 44/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6093 - loss: 1.0325 - val_accuracy: 0.5764 - val_loss: 1.1182\n",
            "Epoch 45/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6040 - loss: 1.0816 - val_accuracy: 0.5938 - val_loss: 1.0924\n",
            "Epoch 46/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5941 - loss: 1.1642 - val_accuracy: 0.6007 - val_loss: 1.0883\n",
            "Epoch 47/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6232 - loss: 1.0452 - val_accuracy: 0.6319 - val_loss: 1.0684\n",
            "Epoch 48/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6007 - loss: 1.0905 - val_accuracy: 0.6111 - val_loss: 1.0796\n",
            "Epoch 49/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6183 - loss: 1.0577 - val_accuracy: 0.6285 - val_loss: 1.0598\n",
            "Epoch 50/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5914 - loss: 1.1127 - val_accuracy: 0.6215 - val_loss: 1.0349\n",
            "Epoch 51/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6080 - loss: 1.0229 - val_accuracy: 0.6458 - val_loss: 1.0202\n",
            "Epoch 52/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6141 - loss: 1.0660 - val_accuracy: 0.6215 - val_loss: 1.0217\n",
            "Epoch 53/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6124 - loss: 1.0687 - val_accuracy: 0.6250 - val_loss: 1.0298\n",
            "Epoch 54/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6379 - loss: 0.9793 - val_accuracy: 0.6215 - val_loss: 1.0420\n",
            "Epoch 55/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6291 - loss: 0.9766 - val_accuracy: 0.6181 - val_loss: 1.0436\n",
            "Epoch 56/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6041 - loss: 1.0625 - val_accuracy: 0.6458 - val_loss: 1.0382\n",
            "Epoch 57/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6485 - loss: 1.0011 - val_accuracy: 0.6042 - val_loss: 1.0706\n",
            "Epoch 58/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6077 - loss: 1.0463 - val_accuracy: 0.5972 - val_loss: 1.0541\n",
            "Epoch 59/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6248 - loss: 0.9550 - val_accuracy: 0.6319 - val_loss: 1.0223\n",
            "Epoch 60/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6492 - loss: 0.9476 - val_accuracy: 0.6215 - val_loss: 1.0494\n",
            "Epoch 61/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6096 - loss: 1.0344 - val_accuracy: 0.6354 - val_loss: 1.0109\n",
            "Epoch 62/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6128 - loss: 1.0404 - val_accuracy: 0.6319 - val_loss: 1.0327\n",
            "Epoch 63/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6630 - loss: 0.9418 - val_accuracy: 0.6424 - val_loss: 1.0244\n",
            "Epoch 64/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6799 - loss: 0.9231 - val_accuracy: 0.6111 - val_loss: 1.0196\n",
            "Epoch 65/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6560 - loss: 0.9305 - val_accuracy: 0.6319 - val_loss: 1.0028\n",
            "Epoch 66/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6391 - loss: 0.9301 - val_accuracy: 0.6528 - val_loss: 1.0029\n",
            "Epoch 67/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6710 - loss: 0.9200 - val_accuracy: 0.6424 - val_loss: 1.0227\n",
            "Epoch 68/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6756 - loss: 0.9077 - val_accuracy: 0.6458 - val_loss: 1.0045\n",
            "Epoch 69/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6904 - loss: 0.8457 - val_accuracy: 0.6354 - val_loss: 1.0207\n",
            "Epoch 70/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6131 - loss: 0.9997 - val_accuracy: 0.6354 - val_loss: 1.0165\n",
            "Epoch 71/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6759 - loss: 0.8817 - val_accuracy: 0.6424 - val_loss: 0.9881\n",
            "Epoch 72/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6558 - loss: 0.9040 - val_accuracy: 0.6389 - val_loss: 0.9816\n",
            "Epoch 73/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6546 - loss: 0.9311 - val_accuracy: 0.6389 - val_loss: 1.0045\n",
            "Epoch 74/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7002 - loss: 0.8528 - val_accuracy: 0.6632 - val_loss: 0.9764\n",
            "Epoch 75/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6720 - loss: 0.8792 - val_accuracy: 0.6528 - val_loss: 0.9891\n",
            "Epoch 76/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6515 - loss: 0.9258 - val_accuracy: 0.6632 - val_loss: 0.9873\n",
            "Epoch 77/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6692 - loss: 0.9937 - val_accuracy: 0.6771 - val_loss: 0.9539\n",
            "Epoch 78/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6393 - loss: 0.9797 - val_accuracy: 0.6632 - val_loss: 0.9930\n",
            "Epoch 79/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6696 - loss: 0.8784 - val_accuracy: 0.6458 - val_loss: 1.0354\n",
            "Epoch 80/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6557 - loss: 0.9003 - val_accuracy: 0.6806 - val_loss: 0.9639\n",
            "Epoch 81/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6827 - loss: 0.8697 - val_accuracy: 0.6840 - val_loss: 0.9673\n",
            "Epoch 82/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7207 - loss: 0.8104 - val_accuracy: 0.6493 - val_loss: 0.9741\n",
            "Epoch 83/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6581 - loss: 0.8824 - val_accuracy: 0.6701 - val_loss: 0.9786\n",
            "Epoch 84/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7017 - loss: 0.8231 - val_accuracy: 0.6806 - val_loss: 0.9647\n",
            "Epoch 85/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6724 - loss: 0.9061 - val_accuracy: 0.6632 - val_loss: 0.9919\n",
            "Epoch 86/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6794 - loss: 0.8439 - val_accuracy: 0.6701 - val_loss: 0.9635\n",
            "Epoch 87/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6816 - loss: 0.8559 - val_accuracy: 0.6424 - val_loss: 0.9949\n",
            "Epoch 88/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6969 - loss: 0.8517 - val_accuracy: 0.6493 - val_loss: 0.9795\n",
            "Epoch 89/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6785 - loss: 0.9233 - val_accuracy: 0.6806 - val_loss: 0.9675\n",
            "Epoch 90/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6806 - loss: 0.8743 - val_accuracy: 0.6771 - val_loss: 0.9580\n",
            "Epoch 91/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6955 - loss: 0.9234 - val_accuracy: 0.6771 - val_loss: 0.9476\n",
            "Epoch 92/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7312 - loss: 0.8412 - val_accuracy: 0.6771 - val_loss: 0.9345\n",
            "Epoch 93/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6928 - loss: 0.8414 - val_accuracy: 0.6701 - val_loss: 0.9601\n",
            "Epoch 94/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7038 - loss: 0.8287 - val_accuracy: 0.6424 - val_loss: 0.9750\n",
            "Epoch 95/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7093 - loss: 0.8313 - val_accuracy: 0.6701 - val_loss: 0.9671\n",
            "Epoch 96/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6877 - loss: 0.8780 - val_accuracy: 0.6597 - val_loss: 0.9776\n",
            "Epoch 97/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6953 - loss: 0.8388 - val_accuracy: 0.6806 - val_loss: 0.9675\n",
            "Epoch 98/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7085 - loss: 0.8142 - val_accuracy: 0.6736 - val_loss: 0.9677\n",
            "Epoch 99/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6824 - loss: 0.8383 - val_accuracy: 0.6562 - val_loss: 0.9887\n",
            "Epoch 100/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6801 - loss: 0.8488 - val_accuracy: 0.6667 - val_loss: 0.9329\n",
            "Epoch 101/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7028 - loss: 0.8583 - val_accuracy: 0.6771 - val_loss: 0.9568\n",
            "Epoch 102/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6866 - loss: 0.8274 - val_accuracy: 0.6840 - val_loss: 0.9227\n",
            "Epoch 103/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7060 - loss: 0.8179 - val_accuracy: 0.6771 - val_loss: 0.9378\n",
            "Epoch 104/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7218 - loss: 0.8080 - val_accuracy: 0.6979 - val_loss: 0.9531\n",
            "Epoch 105/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7375 - loss: 0.7327 - val_accuracy: 0.6736 - val_loss: 0.9596\n",
            "Epoch 106/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7050 - loss: 0.7983 - val_accuracy: 0.6771 - val_loss: 1.0076\n",
            "Epoch 107/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7316 - loss: 0.7694 - val_accuracy: 0.6910 - val_loss: 0.9843\n",
            "Epoch 108/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6947 - loss: 0.8398 - val_accuracy: 0.6493 - val_loss: 0.9745\n",
            "Epoch 109/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7338 - loss: 0.7143 - val_accuracy: 0.6667 - val_loss: 0.9704\n",
            "Epoch 110/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7022 - loss: 0.8147 - val_accuracy: 0.6667 - val_loss: 0.9540\n",
            "Epoch 111/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7223 - loss: 0.7415 - val_accuracy: 0.6806 - val_loss: 0.9504\n",
            "Epoch 112/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7301 - loss: 0.7448 - val_accuracy: 0.6597 - val_loss: 0.9751\n",
            "Epoch 113/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7589 - loss: 0.7150 - val_accuracy: 0.6806 - val_loss: 0.9794\n",
            "Epoch 114/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7297 - loss: 0.7794 - val_accuracy: 0.6840 - val_loss: 0.9353\n",
            "Epoch 115/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7085 - loss: 0.7966 - val_accuracy: 0.6806 - val_loss: 0.9738\n",
            "Epoch 116/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7181 - loss: 0.7858 - val_accuracy: 0.6806 - val_loss: 0.9242\n",
            "Epoch 117/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7426 - loss: 0.7151 - val_accuracy: 0.6875 - val_loss: 0.9651\n",
            "Epoch 118/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7481 - loss: 0.7595 - val_accuracy: 0.6910 - val_loss: 0.9362\n",
            "Epoch 119/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7111 - loss: 0.7628 - val_accuracy: 0.6667 - val_loss: 0.9468\n",
            "Epoch 120/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7142 - loss: 0.7898 - val_accuracy: 0.6840 - val_loss: 0.9224\n",
            "Epoch 121/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7170 - loss: 0.7797 - val_accuracy: 0.6979 - val_loss: 0.9207\n",
            "Epoch 122/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7679 - loss: 0.6694 - val_accuracy: 0.6840 - val_loss: 0.9099\n",
            "Epoch 123/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7256 - loss: 0.7547 - val_accuracy: 0.6667 - val_loss: 0.9505\n",
            "Epoch 124/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7156 - loss: 0.7491 - val_accuracy: 0.6632 - val_loss: 0.9484\n",
            "Epoch 125/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7151 - loss: 0.7660 - val_accuracy: 0.6736 - val_loss: 0.9326\n",
            "Epoch 126/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7288 - loss: 0.7617 - val_accuracy: 0.6597 - val_loss: 0.9580\n",
            "Epoch 127/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7233 - loss: 0.7610 - val_accuracy: 0.6806 - val_loss: 0.9627\n",
            "Epoch 128/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7197 - loss: 0.7399 - val_accuracy: 0.6701 - val_loss: 0.9414\n",
            "Epoch 129/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7657 - loss: 0.6638 - val_accuracy: 0.6632 - val_loss: 0.9512\n",
            "Epoch 130/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7178 - loss: 0.7548 - val_accuracy: 0.6736 - val_loss: 0.9648\n",
            "Epoch 131/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7306 - loss: 0.7244 - val_accuracy: 0.6701 - val_loss: 0.9702\n",
            "Epoch 132/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7395 - loss: 0.7072 - val_accuracy: 0.6806 - val_loss: 0.9658\n",
            "Epoch 133/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7528 - loss: 0.7012 - val_accuracy: 0.6875 - val_loss: 0.9367\n",
            "Epoch 134/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7555 - loss: 0.6966 - val_accuracy: 0.6667 - val_loss: 0.9389\n",
            "Epoch 135/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7477 - loss: 0.6915 - val_accuracy: 0.6667 - val_loss: 0.9423\n",
            "Epoch 136/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7299 - loss: 0.7128 - val_accuracy: 0.6701 - val_loss: 0.9473\n",
            "Epoch 137/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7883 - loss: 0.6273 - val_accuracy: 0.6493 - val_loss: 0.9783\n",
            "Epoch 138/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7464 - loss: 0.7395 - val_accuracy: 0.6736 - val_loss: 0.9599\n",
            "Epoch 139/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7476 - loss: 0.7028 - val_accuracy: 0.6597 - val_loss: 0.9808\n",
            "Epoch 140/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7530 - loss: 0.6953 - val_accuracy: 0.6597 - val_loss: 0.9677\n",
            "Epoch 141/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7526 - loss: 0.6643 - val_accuracy: 0.6597 - val_loss: 0.9644\n",
            "Epoch 142/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7276 - loss: 0.7553 - val_accuracy: 0.6632 - val_loss: 0.9625\n",
            "Epoch 143/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7775 - loss: 0.6576 - val_accuracy: 0.6424 - val_loss: 0.9544\n",
            "Epoch 144/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7062 - loss: 0.7920 - val_accuracy: 0.6493 - val_loss: 0.9771\n",
            "Epoch 145/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7919 - loss: 0.6117 - val_accuracy: 0.6875 - val_loss: 0.9451\n",
            "Epoch 146/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7197 - loss: 0.7205 - val_accuracy: 0.6701 - val_loss: 0.9645\n",
            "Epoch 147/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7440 - loss: 0.7357 - val_accuracy: 0.6632 - val_loss: 0.9749\n",
            "Epoch 148/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7485 - loss: 0.6630 - val_accuracy: 0.6701 - val_loss: 0.9754\n",
            "Epoch 149/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7473 - loss: 0.7257 - val_accuracy: 0.6597 - val_loss: 0.9973\n",
            "Epoch 150/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7424 - loss: 0.6896 - val_accuracy: 0.6424 - val_loss: 1.0443\n",
            "Epoch 151/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7714 - loss: 0.6365 - val_accuracy: 0.6736 - val_loss: 1.0166\n",
            "Epoch 152/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7521 - loss: 0.7014 - val_accuracy: 0.6736 - val_loss: 0.9752\n",
            "Epoch 153/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7456 - loss: 0.6861 - val_accuracy: 0.6840 - val_loss: 0.9632\n",
            "Epoch 154/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7762 - loss: 0.6407 - val_accuracy: 0.7083 - val_loss: 0.9698\n",
            "Epoch 155/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7705 - loss: 0.6270 - val_accuracy: 0.6910 - val_loss: 0.9550\n",
            "Epoch 156/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7590 - loss: 0.6481 - val_accuracy: 0.6840 - val_loss: 0.9914\n",
            "Epoch 157/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7539 - loss: 0.6738 - val_accuracy: 0.6771 - val_loss: 1.0018\n",
            "Epoch 158/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7550 - loss: 0.6966 - val_accuracy: 0.6771 - val_loss: 0.9422\n",
            "Epoch 159/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7747 - loss: 0.6823 - val_accuracy: 0.6910 - val_loss: 0.9349\n",
            "Epoch 160/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7609 - loss: 0.6856 - val_accuracy: 0.6528 - val_loss: 0.9950\n",
            "Epoch 161/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7517 - loss: 0.7108 - val_accuracy: 0.6562 - val_loss: 0.9746\n",
            "Epoch 162/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7845 - loss: 0.5997 - val_accuracy: 0.6701 - val_loss: 1.0098\n",
            "Epoch 163/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7526 - loss: 0.6603 - val_accuracy: 0.6701 - val_loss: 1.0175\n",
            "Epoch 164/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7506 - loss: 0.6991 - val_accuracy: 0.6944 - val_loss: 1.0048\n",
            "Epoch 165/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7820 - loss: 0.6594 - val_accuracy: 0.6910 - val_loss: 1.0240\n",
            "Epoch 166/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7792 - loss: 0.6274 - val_accuracy: 0.6701 - val_loss: 1.0107\n",
            "Epoch 167/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7474 - loss: 0.7066 - val_accuracy: 0.6771 - val_loss: 0.9991\n",
            "Epoch 168/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7501 - loss: 0.6628 - val_accuracy: 0.6771 - val_loss: 0.9965\n",
            "Epoch 169/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7665 - loss: 0.6556 - val_accuracy: 0.6736 - val_loss: 1.0151\n",
            "Epoch 170/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7407 - loss: 0.6901 - val_accuracy: 0.6806 - val_loss: 0.9834\n",
            "Epoch 171/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7817 - loss: 0.6161 - val_accuracy: 0.6667 - val_loss: 0.9873\n",
            "Epoch 172/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7671 - loss: 0.6283 - val_accuracy: 0.6667 - val_loss: 0.9782\n",
            "Epoch 173/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7794 - loss: 0.6534 - val_accuracy: 0.6910 - val_loss: 0.9991\n",
            "Epoch 174/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7693 - loss: 0.6748 - val_accuracy: 0.7188 - val_loss: 0.9754\n",
            "Epoch 175/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7936 - loss: 0.6206 - val_accuracy: 0.6632 - val_loss: 1.0045\n",
            "Epoch 176/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7982 - loss: 0.5638 - val_accuracy: 0.6667 - val_loss: 1.0115\n",
            "Epoch 177/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7375 - loss: 0.7444 - val_accuracy: 0.6771 - val_loss: 0.9988\n",
            "Epoch 178/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7547 - loss: 0.6625 - val_accuracy: 0.7118 - val_loss: 0.9557\n",
            "Epoch 179/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7585 - loss: 0.6491 - val_accuracy: 0.6875 - val_loss: 0.9818\n",
            "Epoch 180/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7693 - loss: 0.6334 - val_accuracy: 0.6806 - val_loss: 0.9475\n",
            "Epoch 181/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8127 - loss: 0.5814 - val_accuracy: 0.6736 - val_loss: 0.9687\n",
            "Epoch 182/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7743 - loss: 0.6979 - val_accuracy: 0.6910 - val_loss: 0.9860\n",
            "Epoch 183/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7597 - loss: 0.6573 - val_accuracy: 0.7083 - val_loss: 0.9439\n",
            "Epoch 184/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7962 - loss: 0.6226 - val_accuracy: 0.7014 - val_loss: 0.9661\n",
            "Epoch 185/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7995 - loss: 0.5837 - val_accuracy: 0.6771 - val_loss: 0.9934\n",
            "Epoch 186/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7656 - loss: 0.6311 - val_accuracy: 0.6944 - val_loss: 0.9965\n",
            "Epoch 187/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7967 - loss: 0.5590 - val_accuracy: 0.6736 - val_loss: 1.0113\n",
            "Epoch 188/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7850 - loss: 0.5851 - val_accuracy: 0.6806 - val_loss: 1.0167\n",
            "Epoch 189/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7885 - loss: 0.5972 - val_accuracy: 0.6736 - val_loss: 1.0090\n",
            "Epoch 190/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7693 - loss: 0.6336 - val_accuracy: 0.6910 - val_loss: 1.0223\n",
            "Epoch 191/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7687 - loss: 0.6206 - val_accuracy: 0.6597 - val_loss: 1.0084\n",
            "Epoch 192/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7852 - loss: 0.6088 - val_accuracy: 0.6840 - val_loss: 1.0252\n",
            "Epoch 193/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7837 - loss: 0.6591 - val_accuracy: 0.6771 - val_loss: 0.9831\n",
            "Epoch 194/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7676 - loss: 0.6739 - val_accuracy: 0.6597 - val_loss: 1.0357\n",
            "Epoch 195/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7880 - loss: 0.5573 - val_accuracy: 0.6667 - val_loss: 1.0183\n",
            "Epoch 196/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7960 - loss: 0.5901 - val_accuracy: 0.6806 - val_loss: 1.0303\n",
            "Epoch 197/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7677 - loss: 0.6279 - val_accuracy: 0.6875 - val_loss: 1.0146\n",
            "Epoch 198/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7858 - loss: 0.5988 - val_accuracy: 0.6806 - val_loss: 0.9888\n",
            "Epoch 199/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7772 - loss: 0.5986 - val_accuracy: 0.6944 - val_loss: 1.0137\n",
            "Epoch 200/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7725 - loss: 0.6475 - val_accuracy: 0.6979 - val_loss: 0.9965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP model parameters\n",
        "num_layers = 5\n",
        "units = [65, 260, 364, 52, 52]\n",
        "activations = ['relu', 'relu', 'relu', 'relu', 'relu']\n",
        "dropouts = [0.1, 0.3, 0.4, 0.1, 0.1]\n",
        "learning_rate = 0.0010343990168720423\n",
        "loss_function = 'sparse_categorical_crossentropy'\n",
        "input_shape = (13,)"
      ],
      "metadata": {
        "id": "vOfxIWVCK26s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3rd\n"
      ],
      "metadata": {
        "id": "myjioHMCKE0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the MLP model\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Input layer\n",
        "model.add(layers.Input(shape=input_shape))\n",
        "\n",
        "# Hidden layers with batch normalization\n",
        "for i in range(num_layers):\n",
        "    model.add(layers.Dense(units[i]))\n",
        "    model.add(layers.BatchNormalization())  # Add batch normalization\n",
        "    model.add(layers.Activation(activations[i]))  # Apply activation after batch normalization\n",
        "    model.add(layers.Dropout(dropouts[i]))  # Add dropout for regularization\n",
        "\n",
        "# Output layer (adjust the number of units to your number of classes)\n",
        "model.add(layers.Dense(8, activation='softmax'))  # Assuming 8 classes for output\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=loss_function,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        },
        "id": "jXOye_S_K9ih",
        "outputId": "6e6b77ae-503b-465c-dad2-b7ccb5efd971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_70 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │             \u001b[38;5;34m910\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_42               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │             \u001b[38;5;34m260\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_30 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_59 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_71 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │          \u001b[38;5;34m17,160\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_43               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │           \u001b[38;5;34m1,040\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_31 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_60 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_72 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │          \u001b[38;5;34m95,004\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_44               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │           \u001b[38;5;34m1,456\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_32 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_61 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_73 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │          \u001b[38;5;34m18,980\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_45               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │             \u001b[38;5;34m208\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_33 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_62 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_74 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │           \u001b[38;5;34m2,756\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_46               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │             \u001b[38;5;34m208\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_34 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_63 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_75 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │             \u001b[38;5;34m424\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">910</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_42               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,160</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_43               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">95,004</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_44               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,456</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,980</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_45               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,756</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_46               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">424</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m138,406\u001b[0m (540.65 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">138,406</span> (540.65 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m136,820\u001b[0m (534.45 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">136,820</span> (534.45 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,586\u001b[0m (6.20 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,586</span> (6.20 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=184,batch_size=32, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYm-VgDJLFkG",
        "outputId": "15144921-00c5-4061-a1f9-fb2ac4c0d54e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.1476 - loss: 2.2927 - val_accuracy: 0.2674 - val_loss: 1.9855\n",
            "Epoch 2/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2413 - loss: 1.9276 - val_accuracy: 0.2917 - val_loss: 1.9125\n",
            "Epoch 3/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3031 - loss: 1.7990 - val_accuracy: 0.3368 - val_loss: 1.8301\n",
            "Epoch 4/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3752 - loss: 1.6884 - val_accuracy: 0.3542 - val_loss: 1.7429\n",
            "Epoch 5/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3674 - loss: 1.6580 - val_accuracy: 0.3854 - val_loss: 1.6698\n",
            "Epoch 6/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3893 - loss: 1.6570 - val_accuracy: 0.4375 - val_loss: 1.5930\n",
            "Epoch 7/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4174 - loss: 1.5846 - val_accuracy: 0.4722 - val_loss: 1.5286\n",
            "Epoch 8/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4141 - loss: 1.5328 - val_accuracy: 0.4896 - val_loss: 1.4601\n",
            "Epoch 9/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4491 - loss: 1.5106 - val_accuracy: 0.5000 - val_loss: 1.4313\n",
            "Epoch 10/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4545 - loss: 1.4690 - val_accuracy: 0.4931 - val_loss: 1.3973\n",
            "Epoch 11/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4579 - loss: 1.4683 - val_accuracy: 0.4896 - val_loss: 1.3896\n",
            "Epoch 12/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4423 - loss: 1.4920 - val_accuracy: 0.5069 - val_loss: 1.3786\n",
            "Epoch 13/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4961 - loss: 1.3869 - val_accuracy: 0.5139 - val_loss: 1.3583\n",
            "Epoch 14/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4867 - loss: 1.3957 - val_accuracy: 0.5208 - val_loss: 1.3474\n",
            "Epoch 15/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4784 - loss: 1.3818 - val_accuracy: 0.5000 - val_loss: 1.3457\n",
            "Epoch 16/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5095 - loss: 1.3336 - val_accuracy: 0.5035 - val_loss: 1.3132\n",
            "Epoch 17/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4762 - loss: 1.4017 - val_accuracy: 0.5417 - val_loss: 1.2903\n",
            "Epoch 18/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5234 - loss: 1.3010 - val_accuracy: 0.5417 - val_loss: 1.2826\n",
            "Epoch 19/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4970 - loss: 1.3380 - val_accuracy: 0.5382 - val_loss: 1.2592\n",
            "Epoch 20/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5422 - loss: 1.2801 - val_accuracy: 0.5243 - val_loss: 1.2516\n",
            "Epoch 21/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5030 - loss: 1.2564 - val_accuracy: 0.5174 - val_loss: 1.2369\n",
            "Epoch 22/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5427 - loss: 1.2797 - val_accuracy: 0.5625 - val_loss: 1.2181\n",
            "Epoch 23/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5370 - loss: 1.2832 - val_accuracy: 0.5486 - val_loss: 1.2273\n",
            "Epoch 24/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5094 - loss: 1.3210 - val_accuracy: 0.5417 - val_loss: 1.2170\n",
            "Epoch 25/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5360 - loss: 1.2297 - val_accuracy: 0.5347 - val_loss: 1.2249\n",
            "Epoch 26/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5775 - loss: 1.1676 - val_accuracy: 0.5382 - val_loss: 1.1964\n",
            "Epoch 27/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5578 - loss: 1.1703 - val_accuracy: 0.5486 - val_loss: 1.1973\n",
            "Epoch 28/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5475 - loss: 1.2659 - val_accuracy: 0.5556 - val_loss: 1.1813\n",
            "Epoch 29/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5636 - loss: 1.1883 - val_accuracy: 0.5799 - val_loss: 1.1699\n",
            "Epoch 30/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5814 - loss: 1.1618 - val_accuracy: 0.5903 - val_loss: 1.1667\n",
            "Epoch 31/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6130 - loss: 1.1169 - val_accuracy: 0.5764 - val_loss: 1.1540\n",
            "Epoch 32/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5604 - loss: 1.1493 - val_accuracy: 0.5660 - val_loss: 1.1723\n",
            "Epoch 33/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5945 - loss: 1.0775 - val_accuracy: 0.5938 - val_loss: 1.1581\n",
            "Epoch 34/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5707 - loss: 1.1500 - val_accuracy: 0.5451 - val_loss: 1.1652\n",
            "Epoch 35/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5919 - loss: 1.1092 - val_accuracy: 0.5903 - val_loss: 1.1240\n",
            "Epoch 36/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6015 - loss: 1.0820 - val_accuracy: 0.5660 - val_loss: 1.1250\n",
            "Epoch 37/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5912 - loss: 1.1432 - val_accuracy: 0.5417 - val_loss: 1.1340\n",
            "Epoch 38/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5860 - loss: 1.0986 - val_accuracy: 0.5417 - val_loss: 1.1429\n",
            "Epoch 39/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5925 - loss: 1.0717 - val_accuracy: 0.5625 - val_loss: 1.1116\n",
            "Epoch 40/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6236 - loss: 1.0481 - val_accuracy: 0.5521 - val_loss: 1.1319\n",
            "Epoch 41/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6245 - loss: 1.0770 - val_accuracy: 0.5903 - val_loss: 1.1050\n",
            "Epoch 42/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6487 - loss: 0.9747 - val_accuracy: 0.5799 - val_loss: 1.1244\n",
            "Epoch 43/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5980 - loss: 1.0800 - val_accuracy: 0.5694 - val_loss: 1.1072\n",
            "Epoch 44/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6329 - loss: 1.0114 - val_accuracy: 0.5729 - val_loss: 1.1095\n",
            "Epoch 45/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6163 - loss: 1.0565 - val_accuracy: 0.5903 - val_loss: 1.0881\n",
            "Epoch 46/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6069 - loss: 1.0091 - val_accuracy: 0.5729 - val_loss: 1.1177\n",
            "Epoch 47/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6680 - loss: 0.9772 - val_accuracy: 0.5868 - val_loss: 1.0887\n",
            "Epoch 48/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6381 - loss: 1.0069 - val_accuracy: 0.5833 - val_loss: 1.0952\n",
            "Epoch 49/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6030 - loss: 1.0650 - val_accuracy: 0.6042 - val_loss: 1.0663\n",
            "Epoch 50/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6206 - loss: 1.0650 - val_accuracy: 0.6250 - val_loss: 1.0773\n",
            "Epoch 51/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6241 - loss: 1.0267 - val_accuracy: 0.6146 - val_loss: 1.0804\n",
            "Epoch 52/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6255 - loss: 0.9742 - val_accuracy: 0.5833 - val_loss: 1.0938\n",
            "Epoch 53/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6163 - loss: 1.0064 - val_accuracy: 0.6007 - val_loss: 1.0713\n",
            "Epoch 54/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6194 - loss: 1.0088 - val_accuracy: 0.6007 - val_loss: 1.0840\n",
            "Epoch 55/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6874 - loss: 0.9106 - val_accuracy: 0.6146 - val_loss: 1.0373\n",
            "Epoch 56/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6240 - loss: 1.0109 - val_accuracy: 0.6250 - val_loss: 1.0496\n",
            "Epoch 57/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.6078 - loss: 1.0569 - val_accuracy: 0.6111 - val_loss: 1.0476\n",
            "Epoch 58/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6292 - loss: 0.9698 - val_accuracy: 0.6042 - val_loss: 1.0592\n",
            "Epoch 59/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6687 - loss: 0.9776 - val_accuracy: 0.6042 - val_loss: 1.0665\n",
            "Epoch 60/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6674 - loss: 0.9683 - val_accuracy: 0.6076 - val_loss: 1.0767\n",
            "Epoch 61/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6609 - loss: 0.9439 - val_accuracy: 0.6111 - val_loss: 1.0510\n",
            "Epoch 62/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6239 - loss: 1.0267 - val_accuracy: 0.6181 - val_loss: 1.0573\n",
            "Epoch 63/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6979 - loss: 0.8557 - val_accuracy: 0.6111 - val_loss: 1.0476\n",
            "Epoch 64/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6650 - loss: 0.9444 - val_accuracy: 0.6111 - val_loss: 1.0805\n",
            "Epoch 65/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6670 - loss: 0.9605 - val_accuracy: 0.6215 - val_loss: 1.0260\n",
            "Epoch 66/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6453 - loss: 0.9469 - val_accuracy: 0.6250 - val_loss: 1.0367\n",
            "Epoch 67/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6434 - loss: 0.8988 - val_accuracy: 0.6146 - val_loss: 1.0522\n",
            "Epoch 68/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6685 - loss: 0.8935 - val_accuracy: 0.6354 - val_loss: 1.0451\n",
            "Epoch 69/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6614 - loss: 0.9245 - val_accuracy: 0.5938 - val_loss: 1.0649\n",
            "Epoch 70/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6950 - loss: 0.9150 - val_accuracy: 0.6250 - val_loss: 1.0892\n",
            "Epoch 71/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6820 - loss: 0.8827 - val_accuracy: 0.5972 - val_loss: 1.0946\n",
            "Epoch 72/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6611 - loss: 0.8947 - val_accuracy: 0.5833 - val_loss: 1.1076\n",
            "Epoch 73/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6973 - loss: 0.8712 - val_accuracy: 0.6181 - val_loss: 1.0441\n",
            "Epoch 74/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6937 - loss: 0.8672 - val_accuracy: 0.6042 - val_loss: 1.0675\n",
            "Epoch 75/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6802 - loss: 0.8766 - val_accuracy: 0.6285 - val_loss: 1.0255\n",
            "Epoch 76/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6727 - loss: 0.8752 - val_accuracy: 0.6215 - val_loss: 1.0508\n",
            "Epoch 77/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6803 - loss: 0.8653 - val_accuracy: 0.6042 - val_loss: 1.0785\n",
            "Epoch 78/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7081 - loss: 0.8260 - val_accuracy: 0.6042 - val_loss: 1.0737\n",
            "Epoch 79/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7179 - loss: 0.7970 - val_accuracy: 0.6250 - val_loss: 1.0356\n",
            "Epoch 80/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7143 - loss: 0.7936 - val_accuracy: 0.6597 - val_loss: 1.0034\n",
            "Epoch 81/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6836 - loss: 0.8876 - val_accuracy: 0.6493 - val_loss: 1.0330\n",
            "Epoch 82/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6785 - loss: 0.8407 - val_accuracy: 0.6354 - val_loss: 1.0619\n",
            "Epoch 83/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6762 - loss: 0.8654 - val_accuracy: 0.6250 - val_loss: 1.0385\n",
            "Epoch 84/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6974 - loss: 0.8392 - val_accuracy: 0.6285 - val_loss: 1.0363\n",
            "Epoch 85/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6745 - loss: 0.8798 - val_accuracy: 0.6458 - val_loss: 1.0037\n",
            "Epoch 86/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6639 - loss: 0.8851 - val_accuracy: 0.6424 - val_loss: 1.0090\n",
            "Epoch 87/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7137 - loss: 0.8198 - val_accuracy: 0.6250 - val_loss: 1.0467\n",
            "Epoch 88/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6929 - loss: 0.7950 - val_accuracy: 0.6146 - val_loss: 1.0354\n",
            "Epoch 89/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6823 - loss: 0.8256 - val_accuracy: 0.5868 - val_loss: 1.0442\n",
            "Epoch 90/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7267 - loss: 0.7536 - val_accuracy: 0.6354 - val_loss: 1.0401\n",
            "Epoch 91/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6826 - loss: 0.8185 - val_accuracy: 0.6215 - val_loss: 1.0419\n",
            "Epoch 92/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7160 - loss: 0.7873 - val_accuracy: 0.6042 - val_loss: 1.0459\n",
            "Epoch 93/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7156 - loss: 0.7794 - val_accuracy: 0.6146 - val_loss: 1.0311\n",
            "Epoch 94/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7109 - loss: 0.8150 - val_accuracy: 0.6319 - val_loss: 1.0435\n",
            "Epoch 95/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.6950 - loss: 0.8375 - val_accuracy: 0.6354 - val_loss: 1.0399\n",
            "Epoch 96/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7332 - loss: 0.7551 - val_accuracy: 0.6250 - val_loss: 1.0264\n",
            "Epoch 97/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7143 - loss: 0.7946 - val_accuracy: 0.6285 - val_loss: 1.0404\n",
            "Epoch 98/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7078 - loss: 0.7981 - val_accuracy: 0.6111 - val_loss: 1.0348\n",
            "Epoch 99/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7235 - loss: 0.7407 - val_accuracy: 0.5938 - val_loss: 1.0531\n",
            "Epoch 100/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7183 - loss: 0.7843 - val_accuracy: 0.6181 - val_loss: 1.0599\n",
            "Epoch 101/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7012 - loss: 0.8298 - val_accuracy: 0.6181 - val_loss: 1.0729\n",
            "Epoch 102/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7054 - loss: 0.7662 - val_accuracy: 0.6076 - val_loss: 1.0635\n",
            "Epoch 103/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7280 - loss: 0.7216 - val_accuracy: 0.6319 - val_loss: 1.0269\n",
            "Epoch 104/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6888 - loss: 0.8582 - val_accuracy: 0.6319 - val_loss: 1.0487\n",
            "Epoch 105/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6950 - loss: 0.8231 - val_accuracy: 0.6215 - val_loss: 1.0490\n",
            "Epoch 106/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7409 - loss: 0.7409 - val_accuracy: 0.6319 - val_loss: 1.0383\n",
            "Epoch 107/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7458 - loss: 0.7260 - val_accuracy: 0.6250 - val_loss: 0.9922\n",
            "Epoch 108/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6930 - loss: 0.8006 - val_accuracy: 0.6458 - val_loss: 1.0032\n",
            "Epoch 109/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7184 - loss: 0.7485 - val_accuracy: 0.6424 - val_loss: 0.9918\n",
            "Epoch 110/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7566 - loss: 0.7211 - val_accuracy: 0.6354 - val_loss: 1.0087\n",
            "Epoch 111/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6944 - loss: 0.8207 - val_accuracy: 0.6562 - val_loss: 1.0184\n",
            "Epoch 112/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7140 - loss: 0.7792 - val_accuracy: 0.6285 - val_loss: 1.0097\n",
            "Epoch 113/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7335 - loss: 0.7697 - val_accuracy: 0.6354 - val_loss: 1.0520\n",
            "Epoch 114/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7316 - loss: 0.7480 - val_accuracy: 0.6319 - val_loss: 1.0410\n",
            "Epoch 115/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7080 - loss: 0.7957 - val_accuracy: 0.6250 - val_loss: 1.0342\n",
            "Epoch 116/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7126 - loss: 0.7648 - val_accuracy: 0.6215 - val_loss: 1.0288\n",
            "Epoch 117/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7651 - loss: 0.7209 - val_accuracy: 0.6424 - val_loss: 1.0059\n",
            "Epoch 118/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7223 - loss: 0.7596 - val_accuracy: 0.6146 - val_loss: 1.0195\n",
            "Epoch 119/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7275 - loss: 0.7660 - val_accuracy: 0.6528 - val_loss: 0.9614\n",
            "Epoch 120/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7506 - loss: 0.7030 - val_accuracy: 0.6597 - val_loss: 0.9939\n",
            "Epoch 121/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7529 - loss: 0.7051 - val_accuracy: 0.6354 - val_loss: 0.9988\n",
            "Epoch 122/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7375 - loss: 0.7135 - val_accuracy: 0.6528 - val_loss: 0.9855\n",
            "Epoch 123/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7275 - loss: 0.7898 - val_accuracy: 0.6354 - val_loss: 1.0350\n",
            "Epoch 124/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7343 - loss: 0.7316 - val_accuracy: 0.6389 - val_loss: 1.0233\n",
            "Epoch 125/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7208 - loss: 0.7584 - val_accuracy: 0.6181 - val_loss: 1.0232\n",
            "Epoch 126/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7388 - loss: 0.7182 - val_accuracy: 0.6181 - val_loss: 1.0254\n",
            "Epoch 127/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7715 - loss: 0.6813 - val_accuracy: 0.6181 - val_loss: 1.0427\n",
            "Epoch 128/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7339 - loss: 0.7324 - val_accuracy: 0.6389 - val_loss: 1.0185\n",
            "Epoch 129/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7411 - loss: 0.7142 - val_accuracy: 0.6528 - val_loss: 1.0102\n",
            "Epoch 130/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7394 - loss: 0.7486 - val_accuracy: 0.6285 - val_loss: 1.0272\n",
            "Epoch 131/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7085 - loss: 0.7612 - val_accuracy: 0.6562 - val_loss: 1.0362\n",
            "Epoch 132/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7412 - loss: 0.7314 - val_accuracy: 0.6458 - val_loss: 1.0269\n",
            "Epoch 133/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7251 - loss: 0.7260 - val_accuracy: 0.6632 - val_loss: 0.9950\n",
            "Epoch 134/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7255 - loss: 0.7453 - val_accuracy: 0.6562 - val_loss: 0.9965\n",
            "Epoch 135/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7682 - loss: 0.6444 - val_accuracy: 0.6389 - val_loss: 1.0014\n",
            "Epoch 136/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7855 - loss: 0.6322 - val_accuracy: 0.6215 - val_loss: 1.0249\n",
            "Epoch 137/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7546 - loss: 0.7243 - val_accuracy: 0.6597 - val_loss: 1.0587\n",
            "Epoch 138/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7341 - loss: 0.7520 - val_accuracy: 0.6458 - val_loss: 1.0276\n",
            "Epoch 139/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7560 - loss: 0.6945 - val_accuracy: 0.6632 - val_loss: 1.0055\n",
            "Epoch 140/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7480 - loss: 0.7394 - val_accuracy: 0.6632 - val_loss: 0.9800\n",
            "Epoch 141/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7547 - loss: 0.6836 - val_accuracy: 0.6701 - val_loss: 0.9981\n",
            "Epoch 142/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7174 - loss: 0.7879 - val_accuracy: 0.6667 - val_loss: 1.0347\n",
            "Epoch 143/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7455 - loss: 0.7482 - val_accuracy: 0.6528 - val_loss: 1.0477\n",
            "Epoch 144/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7500 - loss: 0.6595 - val_accuracy: 0.6389 - val_loss: 1.0567\n",
            "Epoch 145/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7686 - loss: 0.6695 - val_accuracy: 0.6215 - val_loss: 1.0518\n",
            "Epoch 146/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7486 - loss: 0.6893 - val_accuracy: 0.6597 - val_loss: 1.0187\n",
            "Epoch 147/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7380 - loss: 0.7018 - val_accuracy: 0.6424 - val_loss: 1.0370\n",
            "Epoch 148/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7751 - loss: 0.6643 - val_accuracy: 0.6667 - val_loss: 1.0103\n",
            "Epoch 149/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7634 - loss: 0.6477 - val_accuracy: 0.6528 - val_loss: 1.0095\n",
            "Epoch 150/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7744 - loss: 0.6339 - val_accuracy: 0.6493 - val_loss: 1.0118\n",
            "Epoch 151/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7448 - loss: 0.7011 - val_accuracy: 0.6424 - val_loss: 1.0177\n",
            "Epoch 152/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7489 - loss: 0.6699 - val_accuracy: 0.6493 - val_loss: 1.0021\n",
            "Epoch 153/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7436 - loss: 0.7107 - val_accuracy: 0.6354 - val_loss: 1.0080\n",
            "Epoch 154/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7429 - loss: 0.7006 - val_accuracy: 0.6528 - val_loss: 1.0176\n",
            "Epoch 155/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7671 - loss: 0.6781 - val_accuracy: 0.6562 - val_loss: 1.0148\n",
            "Epoch 156/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7684 - loss: 0.6241 - val_accuracy: 0.6597 - val_loss: 1.0098\n",
            "Epoch 157/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7732 - loss: 0.6065 - val_accuracy: 0.6424 - val_loss: 0.9967\n",
            "Epoch 158/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7593 - loss: 0.6615 - val_accuracy: 0.6806 - val_loss: 1.0000\n",
            "Epoch 159/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7438 - loss: 0.7138 - val_accuracy: 0.6667 - val_loss: 1.0068\n",
            "Epoch 160/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7620 - loss: 0.6846 - val_accuracy: 0.6424 - val_loss: 1.0001\n",
            "Epoch 161/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7810 - loss: 0.5911 - val_accuracy: 0.6424 - val_loss: 0.9882\n",
            "Epoch 162/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7692 - loss: 0.6747 - val_accuracy: 0.6389 - val_loss: 1.0064\n",
            "Epoch 163/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7645 - loss: 0.6766 - val_accuracy: 0.6250 - val_loss: 1.0016\n",
            "Epoch 164/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7662 - loss: 0.6504 - val_accuracy: 0.6528 - val_loss: 1.0340\n",
            "Epoch 165/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7595 - loss: 0.6783 - val_accuracy: 0.6424 - val_loss: 1.0207\n",
            "Epoch 166/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7988 - loss: 0.5510 - val_accuracy: 0.6562 - val_loss: 0.9973\n",
            "Epoch 167/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7991 - loss: 0.5581 - val_accuracy: 0.6736 - val_loss: 1.0013\n",
            "Epoch 168/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7604 - loss: 0.6468 - val_accuracy: 0.6562 - val_loss: 1.0415\n",
            "Epoch 169/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7597 - loss: 0.6929 - val_accuracy: 0.6458 - val_loss: 1.0145\n",
            "Epoch 170/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7483 - loss: 0.6782 - val_accuracy: 0.6528 - val_loss: 1.0076\n",
            "Epoch 171/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7726 - loss: 0.6053 - val_accuracy: 0.6354 - val_loss: 1.0431\n",
            "Epoch 172/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7761 - loss: 0.6317 - val_accuracy: 0.6285 - val_loss: 1.0289\n",
            "Epoch 173/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7915 - loss: 0.5932 - val_accuracy: 0.6389 - val_loss: 1.0358\n",
            "Epoch 174/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7667 - loss: 0.6771 - val_accuracy: 0.6424 - val_loss: 1.0305\n",
            "Epoch 175/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7498 - loss: 0.6497 - val_accuracy: 0.6389 - val_loss: 1.0010\n",
            "Epoch 176/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7797 - loss: 0.6066 - val_accuracy: 0.6458 - val_loss: 1.0352\n",
            "Epoch 177/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7662 - loss: 0.6147 - val_accuracy: 0.6319 - val_loss: 1.0564\n",
            "Epoch 178/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7831 - loss: 0.6070 - val_accuracy: 0.6424 - val_loss: 1.0639\n",
            "Epoch 179/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7815 - loss: 0.6538 - val_accuracy: 0.6250 - val_loss: 1.0249\n",
            "Epoch 180/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7856 - loss: 0.6046 - val_accuracy: 0.6424 - val_loss: 1.0092\n",
            "Epoch 181/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7703 - loss: 0.6466 - val_accuracy: 0.6389 - val_loss: 1.0160\n",
            "Epoch 182/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8013 - loss: 0.5783 - val_accuracy: 0.6354 - val_loss: 0.9995\n",
            "Epoch 183/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7753 - loss: 0.6462 - val_accuracy: 0.6667 - val_loss: 1.0095\n",
            "Epoch 184/184\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8148 - loss: 0.5513 - val_accuracy: 0.6424 - val_loss: 1.0207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP model parameters\n",
        "num_layers = 5\n",
        "units = [65, 364, 364, 260, 52]\n",
        "activations = ['relu', 'relu', 'relu', 'relu', 'relu']\n",
        "dropouts = [0.1, 0.3, 0.4, 0.1, 0.1]\n",
        "learning_rate = 0.0010343990168720423\n",
        "loss_function = 'sparse_categorical_crossentropy'\n",
        "input_shape = (13,)"
      ],
      "metadata": {
        "id": "l9wHsc1jC3Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the MLP model\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Input layer\n",
        "model.add(layers.Input(shape=input_shape))\n",
        "\n",
        "# Hidden layers with batch normalization\n",
        "for i in range(num_layers):\n",
        "    model.add(layers.Dense(units[i]))\n",
        "    model.add(layers.BatchNormalization())  # Add batch normalization\n",
        "    model.add(layers.Activation(activations[i]))  # Apply activation after batch normalization\n",
        "    model.add(layers.Dropout(dropouts[i]))  # Add dropout for regularization\n",
        "\n",
        "# Output layer (adjust the number of units to your number of classes)\n",
        "model.add(layers.Dense(8, activation='softmax'))  # Assuming 8 classes for output\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=loss_function,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        },
        "id": "6ZiCxgT_C95V",
        "outputId": "2bf8f767-4006-40dc-b249-c2d1231d2ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_34 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │             \u001b[38;5;34m910\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_12               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │             \u001b[38;5;34m260\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_29 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_35 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │          \u001b[38;5;34m24,024\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_13               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │           \u001b[38;5;34m1,456\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_30 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │         \u001b[38;5;34m132,860\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_14               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │           \u001b[38;5;34m1,456\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_31 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │          \u001b[38;5;34m94,900\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_15               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │           \u001b[38;5;34m1,040\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_32 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_38 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │          \u001b[38;5;34m13,572\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_16               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │             \u001b[38;5;34m208\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_33 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │             \u001b[38;5;34m424\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">910</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_12               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">24,024</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_13               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,456</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">132,860</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_14               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,456</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">94,900</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_15               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">13,572</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_16               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">424</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m271,110\u001b[0m (1.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">271,110</span> (1.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m268,900\u001b[0m (1.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,900</span> (1.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,210\u001b[0m (8.63 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,210</span> (8.63 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=200,batch_size=32, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eK1TsVbcC91z",
        "outputId": "91236bde-ac63-4d07-baae-f4ca7c6f2036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.1711 - loss: 2.3047 - val_accuracy: 0.3403 - val_loss: 1.9131\n",
            "Epoch 2/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2703 - loss: 1.9560 - val_accuracy: 0.2917 - val_loss: 1.8520\n",
            "Epoch 3/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3700 - loss: 1.7096 - val_accuracy: 0.3194 - val_loss: 1.7919\n",
            "Epoch 4/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.3686 - loss: 1.6573 - val_accuracy: 0.3889 - val_loss: 1.6897\n",
            "Epoch 5/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.3905 - loss: 1.6165 - val_accuracy: 0.4132 - val_loss: 1.5885\n",
            "Epoch 6/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4466 - loss: 1.5268 - val_accuracy: 0.4340 - val_loss: 1.5336\n",
            "Epoch 7/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4437 - loss: 1.5160 - val_accuracy: 0.4583 - val_loss: 1.4867\n",
            "Epoch 8/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4515 - loss: 1.4650 - val_accuracy: 0.4306 - val_loss: 1.4661\n",
            "Epoch 9/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4620 - loss: 1.4098 - val_accuracy: 0.4653 - val_loss: 1.4170\n",
            "Epoch 10/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.5108 - loss: 1.3585 - val_accuracy: 0.4861 - val_loss: 1.4065\n",
            "Epoch 11/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.4883 - loss: 1.3632 - val_accuracy: 0.4896 - val_loss: 1.3874\n",
            "Epoch 12/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.4914 - loss: 1.3508 - val_accuracy: 0.5417 - val_loss: 1.3195\n",
            "Epoch 13/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5327 - loss: 1.3042 - val_accuracy: 0.5104 - val_loss: 1.3222\n",
            "Epoch 14/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4983 - loss: 1.3348 - val_accuracy: 0.5069 - val_loss: 1.3088\n",
            "Epoch 15/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4961 - loss: 1.3342 - val_accuracy: 0.5347 - val_loss: 1.2859\n",
            "Epoch 16/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5153 - loss: 1.3034 - val_accuracy: 0.5486 - val_loss: 1.2663\n",
            "Epoch 17/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5100 - loss: 1.2989 - val_accuracy: 0.5382 - val_loss: 1.2678\n",
            "Epoch 18/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5337 - loss: 1.2477 - val_accuracy: 0.5417 - val_loss: 1.2937\n",
            "Epoch 19/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5423 - loss: 1.2126 - val_accuracy: 0.5660 - val_loss: 1.2633\n",
            "Epoch 20/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5912 - loss: 1.1551 - val_accuracy: 0.5556 - val_loss: 1.2582\n",
            "Epoch 21/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5818 - loss: 1.1527 - val_accuracy: 0.5347 - val_loss: 1.2865\n",
            "Epoch 22/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5574 - loss: 1.1576 - val_accuracy: 0.5694 - val_loss: 1.2371\n",
            "Epoch 23/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5637 - loss: 1.1722 - val_accuracy: 0.5660 - val_loss: 1.2567\n",
            "Epoch 24/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5842 - loss: 1.1651 - val_accuracy: 0.5694 - val_loss: 1.2477\n",
            "Epoch 25/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5575 - loss: 1.1937 - val_accuracy: 0.5799 - val_loss: 1.2258\n",
            "Epoch 26/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6079 - loss: 1.0737 - val_accuracy: 0.5764 - val_loss: 1.2405\n",
            "Epoch 27/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5985 - loss: 1.0706 - val_accuracy: 0.5486 - val_loss: 1.2725\n",
            "Epoch 28/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6166 - loss: 1.0268 - val_accuracy: 0.5556 - val_loss: 1.2286\n",
            "Epoch 29/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5968 - loss: 1.1116 - val_accuracy: 0.5694 - val_loss: 1.2250\n",
            "Epoch 30/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5905 - loss: 1.1279 - val_accuracy: 0.5660 - val_loss: 1.2393\n",
            "Epoch 31/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6131 - loss: 1.0705 - val_accuracy: 0.5590 - val_loss: 1.2096\n",
            "Epoch 32/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6213 - loss: 1.0022 - val_accuracy: 0.5521 - val_loss: 1.2481\n",
            "Epoch 33/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.5902 - loss: 1.0779 - val_accuracy: 0.5451 - val_loss: 1.2183\n",
            "Epoch 34/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6107 - loss: 1.0026 - val_accuracy: 0.5903 - val_loss: 1.2047\n",
            "Epoch 35/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6203 - loss: 1.0102 - val_accuracy: 0.5694 - val_loss: 1.1987\n",
            "Epoch 36/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6311 - loss: 1.0076 - val_accuracy: 0.5556 - val_loss: 1.2215\n",
            "Epoch 37/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6562 - loss: 0.9760 - val_accuracy: 0.5694 - val_loss: 1.1743\n",
            "Epoch 38/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6419 - loss: 0.9790 - val_accuracy: 0.5868 - val_loss: 1.1633\n",
            "Epoch 39/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6340 - loss: 0.9741 - val_accuracy: 0.5590 - val_loss: 1.2171\n",
            "Epoch 40/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6415 - loss: 0.9740 - val_accuracy: 0.5938 - val_loss: 1.2132\n",
            "Epoch 41/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6488 - loss: 0.9793 - val_accuracy: 0.5833 - val_loss: 1.1812\n",
            "Epoch 42/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6135 - loss: 0.9757 - val_accuracy: 0.5764 - val_loss: 1.1929\n",
            "Epoch 43/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6446 - loss: 0.9824 - val_accuracy: 0.5694 - val_loss: 1.2151\n",
            "Epoch 44/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6650 - loss: 0.9797 - val_accuracy: 0.5868 - val_loss: 1.1866\n",
            "Epoch 45/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6340 - loss: 0.9472 - val_accuracy: 0.5938 - val_loss: 1.2106\n",
            "Epoch 46/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6491 - loss: 0.9242 - val_accuracy: 0.5868 - val_loss: 1.1715\n",
            "Epoch 47/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6654 - loss: 0.9317 - val_accuracy: 0.5833 - val_loss: 1.1731\n",
            "Epoch 48/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6221 - loss: 0.9804 - val_accuracy: 0.5660 - val_loss: 1.1924\n",
            "Epoch 49/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6548 - loss: 0.9562 - val_accuracy: 0.5799 - val_loss: 1.2164\n",
            "Epoch 50/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6540 - loss: 0.9137 - val_accuracy: 0.6007 - val_loss: 1.1657\n",
            "Epoch 51/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6888 - loss: 0.8783 - val_accuracy: 0.5833 - val_loss: 1.1381\n",
            "Epoch 52/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6659 - loss: 0.9106 - val_accuracy: 0.5868 - val_loss: 1.1831\n",
            "Epoch 53/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6844 - loss: 0.8560 - val_accuracy: 0.6042 - val_loss: 1.1574\n",
            "Epoch 54/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6957 - loss: 0.8473 - val_accuracy: 0.5833 - val_loss: 1.1793\n",
            "Epoch 55/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.6723 - loss: 0.8937 - val_accuracy: 0.5903 - val_loss: 1.2018\n",
            "Epoch 56/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.6864 - loss: 0.8629 - val_accuracy: 0.5938 - val_loss: 1.1722\n",
            "Epoch 57/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.7149 - loss: 0.8167 - val_accuracy: 0.5833 - val_loss: 1.1940\n",
            "Epoch 58/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7043 - loss: 0.8552 - val_accuracy: 0.5868 - val_loss: 1.2119\n",
            "Epoch 59/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6818 - loss: 0.8360 - val_accuracy: 0.5799 - val_loss: 1.2118\n",
            "Epoch 60/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7127 - loss: 0.7990 - val_accuracy: 0.5972 - val_loss: 1.1860\n",
            "Epoch 61/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6868 - loss: 0.8359 - val_accuracy: 0.5729 - val_loss: 1.2191\n",
            "Epoch 62/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7126 - loss: 0.8124 - val_accuracy: 0.5833 - val_loss: 1.1932\n",
            "Epoch 63/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7116 - loss: 0.8319 - val_accuracy: 0.5521 - val_loss: 1.2120\n",
            "Epoch 64/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7267 - loss: 0.7672 - val_accuracy: 0.5625 - val_loss: 1.2108\n",
            "Epoch 65/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7146 - loss: 0.7582 - val_accuracy: 0.5660 - val_loss: 1.2095\n",
            "Epoch 66/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6844 - loss: 0.8367 - val_accuracy: 0.6007 - val_loss: 1.1886\n",
            "Epoch 67/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7017 - loss: 0.8003 - val_accuracy: 0.5972 - val_loss: 1.1351\n",
            "Epoch 68/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7129 - loss: 0.7837 - val_accuracy: 0.5938 - val_loss: 1.2166\n",
            "Epoch 69/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7255 - loss: 0.7914 - val_accuracy: 0.5799 - val_loss: 1.1913\n",
            "Epoch 70/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6846 - loss: 0.8253 - val_accuracy: 0.6076 - val_loss: 1.1757\n",
            "Epoch 71/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7127 - loss: 0.7499 - val_accuracy: 0.6111 - val_loss: 1.1913\n",
            "Epoch 72/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7096 - loss: 0.7676 - val_accuracy: 0.6042 - val_loss: 1.1950\n",
            "Epoch 73/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7229 - loss: 0.7481 - val_accuracy: 0.6042 - val_loss: 1.1695\n",
            "Epoch 74/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6866 - loss: 0.8282 - val_accuracy: 0.5903 - val_loss: 1.2105\n",
            "Epoch 75/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7181 - loss: 0.7336 - val_accuracy: 0.6007 - val_loss: 1.1861\n",
            "Epoch 76/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6988 - loss: 0.7859 - val_accuracy: 0.5938 - val_loss: 1.2102\n",
            "Epoch 77/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7091 - loss: 0.7447 - val_accuracy: 0.5938 - val_loss: 1.1996\n",
            "Epoch 78/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7048 - loss: 0.7937 - val_accuracy: 0.6042 - val_loss: 1.2138\n",
            "Epoch 79/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7177 - loss: 0.7479 - val_accuracy: 0.5903 - val_loss: 1.2006\n",
            "Epoch 80/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7057 - loss: 0.7254 - val_accuracy: 0.5868 - val_loss: 1.2145\n",
            "Epoch 81/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7508 - loss: 0.6722 - val_accuracy: 0.5833 - val_loss: 1.2165\n",
            "Epoch 82/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7267 - loss: 0.7220 - val_accuracy: 0.6007 - val_loss: 1.1966\n",
            "Epoch 83/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.7572 - loss: 0.6595 - val_accuracy: 0.5868 - val_loss: 1.2208\n",
            "Epoch 84/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7525 - loss: 0.7110 - val_accuracy: 0.6042 - val_loss: 1.2101\n",
            "Epoch 85/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7105 - loss: 0.7254 - val_accuracy: 0.6042 - val_loss: 1.1553\n",
            "Epoch 86/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7491 - loss: 0.7253 - val_accuracy: 0.6250 - val_loss: 1.1992\n",
            "Epoch 87/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7345 - loss: 0.7190 - val_accuracy: 0.6042 - val_loss: 1.1718\n",
            "Epoch 88/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7516 - loss: 0.6801 - val_accuracy: 0.6042 - val_loss: 1.1381\n",
            "Epoch 89/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7605 - loss: 0.6688 - val_accuracy: 0.6389 - val_loss: 1.1256\n",
            "Epoch 90/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7613 - loss: 0.6518 - val_accuracy: 0.6285 - val_loss: 1.1849\n",
            "Epoch 91/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7319 - loss: 0.8072 - val_accuracy: 0.6215 - val_loss: 1.2099\n",
            "Epoch 92/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7233 - loss: 0.7606 - val_accuracy: 0.6250 - val_loss: 1.1566\n",
            "Epoch 93/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7342 - loss: 0.7024 - val_accuracy: 0.6111 - val_loss: 1.1648\n",
            "Epoch 94/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7755 - loss: 0.6252 - val_accuracy: 0.6111 - val_loss: 1.1669\n",
            "Epoch 95/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7581 - loss: 0.6998 - val_accuracy: 0.6215 - val_loss: 1.1405\n",
            "Epoch 96/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7513 - loss: 0.6809 - val_accuracy: 0.5938 - val_loss: 1.1435\n",
            "Epoch 97/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7546 - loss: 0.6731 - val_accuracy: 0.6111 - val_loss: 1.1233\n",
            "Epoch 98/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7313 - loss: 0.6883 - val_accuracy: 0.6319 - val_loss: 1.1069\n",
            "Epoch 99/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7749 - loss: 0.6378 - val_accuracy: 0.6354 - val_loss: 1.1236\n",
            "Epoch 100/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7379 - loss: 0.7071 - val_accuracy: 0.6319 - val_loss: 1.1238\n",
            "Epoch 101/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7906 - loss: 0.6287 - val_accuracy: 0.6007 - val_loss: 1.1820\n",
            "Epoch 102/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7648 - loss: 0.6487 - val_accuracy: 0.6181 - val_loss: 1.1974\n",
            "Epoch 103/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7914 - loss: 0.6215 - val_accuracy: 0.6181 - val_loss: 1.1794\n",
            "Epoch 104/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7576 - loss: 0.6248 - val_accuracy: 0.6354 - val_loss: 1.1858\n",
            "Epoch 105/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7548 - loss: 0.6756 - val_accuracy: 0.6215 - val_loss: 1.2529\n",
            "Epoch 106/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7529 - loss: 0.6628 - val_accuracy: 0.6146 - val_loss: 1.1728\n",
            "Epoch 107/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7488 - loss: 0.6706 - val_accuracy: 0.6007 - val_loss: 1.2244\n",
            "Epoch 108/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7595 - loss: 0.6684 - val_accuracy: 0.6076 - val_loss: 1.1934\n",
            "Epoch 109/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7651 - loss: 0.6500 - val_accuracy: 0.6146 - val_loss: 1.1989\n",
            "Epoch 110/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7902 - loss: 0.5853 - val_accuracy: 0.6111 - val_loss: 1.2241\n",
            "Epoch 111/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8152 - loss: 0.5811 - val_accuracy: 0.6042 - val_loss: 1.1762\n",
            "Epoch 112/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7579 - loss: 0.6911 - val_accuracy: 0.6181 - val_loss: 1.1589\n",
            "Epoch 113/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7815 - loss: 0.6367 - val_accuracy: 0.6111 - val_loss: 1.1497\n",
            "Epoch 114/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7762 - loss: 0.6491 - val_accuracy: 0.6354 - val_loss: 1.1560\n",
            "Epoch 115/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7284 - loss: 0.7009 - val_accuracy: 0.6250 - val_loss: 1.1072\n",
            "Epoch 116/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7856 - loss: 0.6176 - val_accuracy: 0.6215 - val_loss: 1.1624\n",
            "Epoch 117/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7534 - loss: 0.6869 - val_accuracy: 0.6458 - val_loss: 1.1337\n",
            "Epoch 118/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7819 - loss: 0.5952 - val_accuracy: 0.6389 - val_loss: 1.1500\n",
            "Epoch 119/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7646 - loss: 0.6532 - val_accuracy: 0.6285 - val_loss: 1.1260\n",
            "Epoch 120/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7850 - loss: 0.6002 - val_accuracy: 0.6319 - val_loss: 1.1163\n",
            "Epoch 121/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7984 - loss: 0.5714 - val_accuracy: 0.6285 - val_loss: 1.1621\n",
            "Epoch 122/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7570 - loss: 0.6909 - val_accuracy: 0.6285 - val_loss: 1.1424\n",
            "Epoch 123/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8015 - loss: 0.5637 - val_accuracy: 0.6215 - val_loss: 1.1498\n",
            "Epoch 124/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8048 - loss: 0.5501 - val_accuracy: 0.6181 - val_loss: 1.1844\n",
            "Epoch 125/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8071 - loss: 0.5533 - val_accuracy: 0.6354 - val_loss: 1.1851\n",
            "Epoch 126/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8033 - loss: 0.5744 - val_accuracy: 0.6528 - val_loss: 1.1649\n",
            "Epoch 127/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7623 - loss: 0.5882 - val_accuracy: 0.6215 - val_loss: 1.1837\n",
            "Epoch 128/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7943 - loss: 0.5834 - val_accuracy: 0.6458 - val_loss: 1.1855\n",
            "Epoch 129/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7752 - loss: 0.6192 - val_accuracy: 0.6215 - val_loss: 1.2139\n",
            "Epoch 130/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7832 - loss: 0.6216 - val_accuracy: 0.6215 - val_loss: 1.2191\n",
            "Epoch 131/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7875 - loss: 0.5554 - val_accuracy: 0.6319 - val_loss: 1.1883\n",
            "Epoch 132/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7795 - loss: 0.6211 - val_accuracy: 0.6250 - val_loss: 1.1628\n",
            "Epoch 133/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7829 - loss: 0.5640 - val_accuracy: 0.6285 - val_loss: 1.2076\n",
            "Epoch 134/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7968 - loss: 0.5579 - val_accuracy: 0.6354 - val_loss: 1.2188\n",
            "Epoch 135/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7976 - loss: 0.5442 - val_accuracy: 0.6493 - val_loss: 1.1734\n",
            "Epoch 136/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7966 - loss: 0.5588 - val_accuracy: 0.6597 - val_loss: 1.1271\n",
            "Epoch 137/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8077 - loss: 0.5306 - val_accuracy: 0.6701 - val_loss: 1.1147\n",
            "Epoch 138/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8181 - loss: 0.5322 - val_accuracy: 0.6528 - val_loss: 1.1398\n",
            "Epoch 139/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8047 - loss: 0.5885 - val_accuracy: 0.6181 - val_loss: 1.1855\n",
            "Epoch 140/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7857 - loss: 0.5774 - val_accuracy: 0.6007 - val_loss: 1.1980\n",
            "Epoch 141/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7908 - loss: 0.6026 - val_accuracy: 0.6562 - val_loss: 1.1592\n",
            "Epoch 142/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8180 - loss: 0.5030 - val_accuracy: 0.6389 - val_loss: 1.1681\n",
            "Epoch 143/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7974 - loss: 0.5678 - val_accuracy: 0.6354 - val_loss: 1.1569\n",
            "Epoch 144/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7992 - loss: 0.5166 - val_accuracy: 0.6354 - val_loss: 1.2052\n",
            "Epoch 145/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7901 - loss: 0.5694 - val_accuracy: 0.6389 - val_loss: 1.1624\n",
            "Epoch 146/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7710 - loss: 0.6112 - val_accuracy: 0.6389 - val_loss: 1.1536\n",
            "Epoch 147/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8017 - loss: 0.5411 - val_accuracy: 0.6458 - val_loss: 1.1344\n",
            "Epoch 148/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8166 - loss: 0.5166 - val_accuracy: 0.6597 - val_loss: 1.1566\n",
            "Epoch 149/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8300 - loss: 0.5214 - val_accuracy: 0.6597 - val_loss: 1.1672\n",
            "Epoch 150/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8226 - loss: 0.5126 - val_accuracy: 0.6389 - val_loss: 1.1625\n",
            "Epoch 151/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8029 - loss: 0.5371 - val_accuracy: 0.6319 - val_loss: 1.1475\n",
            "Epoch 152/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8227 - loss: 0.5088 - val_accuracy: 0.6319 - val_loss: 1.1846\n",
            "Epoch 153/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7949 - loss: 0.5710 - val_accuracy: 0.6562 - val_loss: 1.2078\n",
            "Epoch 154/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8321 - loss: 0.5233 - val_accuracy: 0.6424 - val_loss: 1.2294\n",
            "Epoch 155/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8211 - loss: 0.4774 - val_accuracy: 0.6250 - val_loss: 1.1931\n",
            "Epoch 156/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8078 - loss: 0.5182 - val_accuracy: 0.6181 - val_loss: 1.1699\n",
            "Epoch 157/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8220 - loss: 0.5059 - val_accuracy: 0.6424 - val_loss: 1.1400\n",
            "Epoch 158/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7719 - loss: 0.6490 - val_accuracy: 0.6181 - val_loss: 1.2172\n",
            "Epoch 159/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8101 - loss: 0.5608 - val_accuracy: 0.6389 - val_loss: 1.2015\n",
            "Epoch 160/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7815 - loss: 0.5122 - val_accuracy: 0.6319 - val_loss: 1.1949\n",
            "Epoch 161/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7936 - loss: 0.5664 - val_accuracy: 0.6181 - val_loss: 1.2316\n",
            "Epoch 162/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8141 - loss: 0.5182 - val_accuracy: 0.6076 - val_loss: 1.2919\n",
            "Epoch 163/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8299 - loss: 0.4570 - val_accuracy: 0.6250 - val_loss: 1.2172\n",
            "Epoch 164/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8080 - loss: 0.5456 - val_accuracy: 0.6181 - val_loss: 1.2535\n",
            "Epoch 165/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8061 - loss: 0.5275 - val_accuracy: 0.6250 - val_loss: 1.2407\n",
            "Epoch 166/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8343 - loss: 0.4944 - val_accuracy: 0.6181 - val_loss: 1.1859\n",
            "Epoch 167/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8033 - loss: 0.5104 - val_accuracy: 0.6354 - val_loss: 1.1506\n",
            "Epoch 168/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8104 - loss: 0.5187 - val_accuracy: 0.6215 - val_loss: 1.2243\n",
            "Epoch 169/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7918 - loss: 0.5740 - val_accuracy: 0.6111 - val_loss: 1.2694\n",
            "Epoch 170/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8163 - loss: 0.4832 - val_accuracy: 0.6250 - val_loss: 1.2682\n",
            "Epoch 171/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8047 - loss: 0.5198 - val_accuracy: 0.6424 - val_loss: 1.2218\n",
            "Epoch 172/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8330 - loss: 0.4732 - val_accuracy: 0.6319 - val_loss: 1.2507\n",
            "Epoch 173/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8131 - loss: 0.5008 - val_accuracy: 0.6285 - val_loss: 1.2411\n",
            "Epoch 174/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8338 - loss: 0.5121 - val_accuracy: 0.6424 - val_loss: 1.2383\n",
            "Epoch 175/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8248 - loss: 0.5159 - val_accuracy: 0.6250 - val_loss: 1.2646\n",
            "Epoch 176/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8072 - loss: 0.5240 - val_accuracy: 0.6424 - val_loss: 1.2410\n",
            "Epoch 177/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8157 - loss: 0.4901 - val_accuracy: 0.6528 - val_loss: 1.2187\n",
            "Epoch 178/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8239 - loss: 0.4930 - val_accuracy: 0.6285 - val_loss: 1.2406\n",
            "Epoch 179/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8467 - loss: 0.4778 - val_accuracy: 0.6389 - val_loss: 1.2269\n",
            "Epoch 180/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8265 - loss: 0.4937 - val_accuracy: 0.6111 - val_loss: 1.2274\n",
            "Epoch 181/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7978 - loss: 0.5739 - val_accuracy: 0.6146 - val_loss: 1.1897\n",
            "Epoch 182/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8318 - loss: 0.4741 - val_accuracy: 0.6354 - val_loss: 1.1214\n",
            "Epoch 183/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8287 - loss: 0.5141 - val_accuracy: 0.6736 - val_loss: 1.0911\n",
            "Epoch 184/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8029 - loss: 0.5603 - val_accuracy: 0.6562 - val_loss: 1.1392\n",
            "Epoch 185/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8279 - loss: 0.4910 - val_accuracy: 0.6319 - val_loss: 1.2118\n",
            "Epoch 186/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8137 - loss: 0.4841 - val_accuracy: 0.6076 - val_loss: 1.2004\n",
            "Epoch 187/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8077 - loss: 0.5157 - val_accuracy: 0.6389 - val_loss: 1.1738\n",
            "Epoch 188/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8133 - loss: 0.5173 - val_accuracy: 0.6458 - val_loss: 1.1862\n",
            "Epoch 189/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8212 - loss: 0.4842 - val_accuracy: 0.6250 - val_loss: 1.2005\n",
            "Epoch 190/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8220 - loss: 0.4782 - val_accuracy: 0.6250 - val_loss: 1.2285\n",
            "Epoch 191/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8213 - loss: 0.5130 - val_accuracy: 0.6285 - val_loss: 1.2067\n",
            "Epoch 192/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7967 - loss: 0.5563 - val_accuracy: 0.6250 - val_loss: 1.2876\n",
            "Epoch 193/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8230 - loss: 0.5033 - val_accuracy: 0.6285 - val_loss: 1.2555\n",
            "Epoch 194/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8472 - loss: 0.4241 - val_accuracy: 0.6354 - val_loss: 1.2263\n",
            "Epoch 195/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8444 - loss: 0.4020 - val_accuracy: 0.6319 - val_loss: 1.1656\n",
            "Epoch 196/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7933 - loss: 0.5324 - val_accuracy: 0.6042 - val_loss: 1.2353\n",
            "Epoch 197/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8026 - loss: 0.4985 - val_accuracy: 0.6076 - val_loss: 1.2768\n",
            "Epoch 198/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8048 - loss: 0.5496 - val_accuracy: 0.6215 - val_loss: 1.2345\n",
            "Epoch 199/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8654 - loss: 0.3839 - val_accuracy: 0.6250 - val_loss: 1.1932\n",
            "Epoch 200/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8113 - loss: 0.5240 - val_accuracy: 0.6319 - val_loss: 1.2384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP model parameters\n",
        "num_layers = 5\n",
        "units = [65, 260, 364, 52, 52]\n",
        "activations = ['relu', 'relu', 'relu', 'relu', 'relu']\n",
        "dropouts = [0.1, 0.3, 0.4, 0.1, 0.1]\n",
        "learning_rate = 0.0010343990168720423\n",
        "loss_function = 'sparse_categorical_crossentropy'\n",
        "input_shape = (13,)"
      ],
      "metadata": {
        "id": "e3C1aVWRC3LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the MLP model\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Input layer\n",
        "model.add(layers.Input(shape=input_shape))\n",
        "\n",
        "# Hidden layers with batch normalization\n",
        "for i in range(num_layers):\n",
        "    model.add(layers.Dense(units[i]))\n",
        "    model.add(layers.BatchNormalization())  # Add batch normalization\n",
        "    model.add(layers.Activation(activations[i]))  # Apply activation after batch normalization\n",
        "    model.add(layers.Dropout(dropouts[i]))  # Add dropout for regularization\n",
        "\n",
        "# Output layer (adjust the number of units to your number of classes)\n",
        "model.add(layers.Dense(8, activation='softmax'))  # Assuming 8 classes for output\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
        "              loss=loss_function,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        },
        "id": "JGB_ZVLdC3BK",
        "outputId": "cba7c383-156a-4559-abef-6ff8434e0560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_40 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │             \u001b[38;5;34m910\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_17               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │             \u001b[38;5;34m260\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_34 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │          \u001b[38;5;34m17,160\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_18               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │           \u001b[38;5;34m1,040\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_6 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_35 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_42 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │          \u001b[38;5;34m95,004\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_19               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │           \u001b[38;5;34m1,456\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_7 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_36 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_43 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │          \u001b[38;5;34m18,980\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_20               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │             \u001b[38;5;34m208\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_8 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_37 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_44 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │           \u001b[38;5;34m2,756\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_21               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │             \u001b[38;5;34m208\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_9 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_38 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_45 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │             \u001b[38;5;34m424\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">910</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_17               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,160</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_18               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">95,004</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_19               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,456</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,980</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_20               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,756</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_21               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">424</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m138,406\u001b[0m (540.65 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">138,406</span> (540.65 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m136,820\u001b[0m (534.45 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">136,820</span> (534.45 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,586\u001b[0m (6.20 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,586</span> (6.20 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=200,batch_size=32, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFfhtr07DHK7",
        "outputId": "491f98ef-aa38-439f-81a4-ece8ed13be4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.1567 - loss: 2.2879 - val_accuracy: 0.2812 - val_loss: 1.9944\n",
            "Epoch 2/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2290 - loss: 1.9659 - val_accuracy: 0.2743 - val_loss: 1.9487\n",
            "Epoch 3/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.2866 - loss: 1.8746 - val_accuracy: 0.3368 - val_loss: 1.8808\n",
            "Epoch 4/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2984 - loss: 1.8286 - val_accuracy: 0.3785 - val_loss: 1.7888\n",
            "Epoch 5/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3457 - loss: 1.7021 - val_accuracy: 0.4097 - val_loss: 1.7089\n",
            "Epoch 6/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3626 - loss: 1.7048 - val_accuracy: 0.4340 - val_loss: 1.6317\n",
            "Epoch 7/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4066 - loss: 1.6312 - val_accuracy: 0.4444 - val_loss: 1.5629\n",
            "Epoch 8/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3820 - loss: 1.6449 - val_accuracy: 0.4653 - val_loss: 1.4997\n",
            "Epoch 9/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4322 - loss: 1.4926 - val_accuracy: 0.4722 - val_loss: 1.4527\n",
            "Epoch 10/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4058 - loss: 1.5552 - val_accuracy: 0.4826 - val_loss: 1.4267\n",
            "Epoch 11/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4548 - loss: 1.4692 - val_accuracy: 0.4965 - val_loss: 1.3742\n",
            "Epoch 12/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4699 - loss: 1.4579 - val_accuracy: 0.4965 - val_loss: 1.3710\n",
            "Epoch 13/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4710 - loss: 1.4553 - val_accuracy: 0.5035 - val_loss: 1.3529\n",
            "Epoch 14/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4899 - loss: 1.3981 - val_accuracy: 0.5035 - val_loss: 1.3237\n",
            "Epoch 15/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4953 - loss: 1.3750 - val_accuracy: 0.4826 - val_loss: 1.3079\n",
            "Epoch 16/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4943 - loss: 1.3730 - val_accuracy: 0.4965 - val_loss: 1.3050\n",
            "Epoch 17/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4811 - loss: 1.3594 - val_accuracy: 0.5174 - val_loss: 1.2824\n",
            "Epoch 18/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4938 - loss: 1.3502 - val_accuracy: 0.5278 - val_loss: 1.2707\n",
            "Epoch 19/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5146 - loss: 1.3403 - val_accuracy: 0.5278 - val_loss: 1.2569\n",
            "Epoch 20/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5042 - loss: 1.3422 - val_accuracy: 0.5243 - val_loss: 1.2446\n",
            "Epoch 21/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5057 - loss: 1.3331 - val_accuracy: 0.5104 - val_loss: 1.2537\n",
            "Epoch 22/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5311 - loss: 1.2550 - val_accuracy: 0.5208 - val_loss: 1.2332\n",
            "Epoch 23/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5477 - loss: 1.2448 - val_accuracy: 0.5208 - val_loss: 1.2180\n",
            "Epoch 24/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5471 - loss: 1.2425 - val_accuracy: 0.5660 - val_loss: 1.2105\n",
            "Epoch 25/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5652 - loss: 1.1964 - val_accuracy: 0.5556 - val_loss: 1.2041\n",
            "Epoch 26/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5384 - loss: 1.2206 - val_accuracy: 0.5451 - val_loss: 1.1787\n",
            "Epoch 27/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5223 - loss: 1.2433 - val_accuracy: 0.5451 - val_loss: 1.1852\n",
            "Epoch 28/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5426 - loss: 1.2783 - val_accuracy: 0.5417 - val_loss: 1.2065\n",
            "Epoch 29/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5172 - loss: 1.2366 - val_accuracy: 0.5417 - val_loss: 1.1777\n",
            "Epoch 30/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5363 - loss: 1.2318 - val_accuracy: 0.5521 - val_loss: 1.1958\n",
            "Epoch 31/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5573 - loss: 1.1773 - val_accuracy: 0.5625 - val_loss: 1.1877\n",
            "Epoch 32/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5550 - loss: 1.2516 - val_accuracy: 0.5660 - val_loss: 1.1740\n",
            "Epoch 33/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5669 - loss: 1.1320 - val_accuracy: 0.5486 - val_loss: 1.1748\n",
            "Epoch 34/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5516 - loss: 1.1764 - val_accuracy: 0.5903 - val_loss: 1.1518\n",
            "Epoch 35/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5661 - loss: 1.1483 - val_accuracy: 0.5799 - val_loss: 1.1236\n",
            "Epoch 36/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5696 - loss: 1.1736 - val_accuracy: 0.5833 - val_loss: 1.1336\n",
            "Epoch 37/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5856 - loss: 1.1281 - val_accuracy: 0.5660 - val_loss: 1.1497\n",
            "Epoch 38/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6099 - loss: 1.0841 - val_accuracy: 0.5660 - val_loss: 1.1559\n",
            "Epoch 39/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5774 - loss: 1.1668 - val_accuracy: 0.5764 - val_loss: 1.1519\n",
            "Epoch 40/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5543 - loss: 1.1820 - val_accuracy: 0.5694 - val_loss: 1.1153\n",
            "Epoch 41/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5890 - loss: 1.1210 - val_accuracy: 0.5764 - val_loss: 1.1146\n",
            "Epoch 42/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5698 - loss: 1.1892 - val_accuracy: 0.5972 - val_loss: 1.0981\n",
            "Epoch 43/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6469 - loss: 0.9974 - val_accuracy: 0.5729 - val_loss: 1.1100\n",
            "Epoch 44/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6130 - loss: 1.0667 - val_accuracy: 0.5729 - val_loss: 1.0855\n",
            "Epoch 45/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6093 - loss: 1.0896 - val_accuracy: 0.5764 - val_loss: 1.0909\n",
            "Epoch 46/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5876 - loss: 1.0982 - val_accuracy: 0.6076 - val_loss: 1.0882\n",
            "Epoch 47/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6108 - loss: 1.0563 - val_accuracy: 0.5833 - val_loss: 1.1280\n",
            "Epoch 48/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6372 - loss: 1.0040 - val_accuracy: 0.6076 - val_loss: 1.1021\n",
            "Epoch 49/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6227 - loss: 1.0086 - val_accuracy: 0.6042 - val_loss: 1.0753\n",
            "Epoch 50/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6292 - loss: 1.0022 - val_accuracy: 0.5972 - val_loss: 1.0650\n",
            "Epoch 51/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6212 - loss: 1.0062 - val_accuracy: 0.5799 - val_loss: 1.0873\n",
            "Epoch 52/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6111 - loss: 1.1081 - val_accuracy: 0.6042 - val_loss: 1.0599\n",
            "Epoch 53/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5868 - loss: 1.0670 - val_accuracy: 0.6076 - val_loss: 1.0603\n",
            "Epoch 54/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6245 - loss: 1.0075 - val_accuracy: 0.6076 - val_loss: 1.0488\n",
            "Epoch 55/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6354 - loss: 1.0549 - val_accuracy: 0.6181 - val_loss: 1.0321\n",
            "Epoch 56/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6423 - loss: 0.9900 - val_accuracy: 0.5938 - val_loss: 1.0480\n",
            "Epoch 57/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6266 - loss: 1.0228 - val_accuracy: 0.5903 - val_loss: 1.0473\n",
            "Epoch 58/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6584 - loss: 0.9484 - val_accuracy: 0.5799 - val_loss: 1.0743\n",
            "Epoch 59/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6678 - loss: 0.9629 - val_accuracy: 0.6076 - val_loss: 1.0734\n",
            "Epoch 60/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6548 - loss: 0.8836 - val_accuracy: 0.6250 - val_loss: 1.0531\n",
            "Epoch 61/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6288 - loss: 0.9890 - val_accuracy: 0.6076 - val_loss: 1.0733\n",
            "Epoch 62/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6339 - loss: 0.9900 - val_accuracy: 0.5938 - val_loss: 1.0568\n",
            "Epoch 63/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6593 - loss: 0.9210 - val_accuracy: 0.6042 - val_loss: 1.0381\n",
            "Epoch 64/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6517 - loss: 0.9247 - val_accuracy: 0.5972 - val_loss: 1.0582\n",
            "Epoch 65/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6557 - loss: 0.9674 - val_accuracy: 0.5972 - val_loss: 1.0487\n",
            "Epoch 66/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6821 - loss: 0.8756 - val_accuracy: 0.6146 - val_loss: 1.0534\n",
            "Epoch 67/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6495 - loss: 0.9474 - val_accuracy: 0.5972 - val_loss: 1.0464\n",
            "Epoch 68/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6668 - loss: 0.8777 - val_accuracy: 0.6181 - val_loss: 1.0305\n",
            "Epoch 69/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6490 - loss: 0.9496 - val_accuracy: 0.6042 - val_loss: 1.0675\n",
            "Epoch 70/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6542 - loss: 0.9676 - val_accuracy: 0.5868 - val_loss: 1.0683\n",
            "Epoch 71/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6811 - loss: 0.8948 - val_accuracy: 0.6076 - val_loss: 1.0719\n",
            "Epoch 72/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6535 - loss: 0.9306 - val_accuracy: 0.5903 - val_loss: 1.0616\n",
            "Epoch 73/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6659 - loss: 0.8917 - val_accuracy: 0.6076 - val_loss: 1.0430\n",
            "Epoch 74/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6698 - loss: 0.9397 - val_accuracy: 0.6076 - val_loss: 1.0861\n",
            "Epoch 75/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6703 - loss: 0.9351 - val_accuracy: 0.6111 - val_loss: 1.0647\n",
            "Epoch 76/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6429 - loss: 0.9484 - val_accuracy: 0.6146 - val_loss: 1.0800\n",
            "Epoch 77/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6645 - loss: 0.9059 - val_accuracy: 0.6181 - val_loss: 1.0517\n",
            "Epoch 78/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6909 - loss: 0.8452 - val_accuracy: 0.6250 - val_loss: 1.0445\n",
            "Epoch 79/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6820 - loss: 0.8975 - val_accuracy: 0.6042 - val_loss: 1.0619\n",
            "Epoch 80/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6857 - loss: 0.8749 - val_accuracy: 0.6111 - val_loss: 1.0521\n",
            "Epoch 81/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6711 - loss: 0.9252 - val_accuracy: 0.6285 - val_loss: 1.0682\n",
            "Epoch 82/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6858 - loss: 0.8899 - val_accuracy: 0.6250 - val_loss: 1.0255\n",
            "Epoch 83/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7260 - loss: 0.7891 - val_accuracy: 0.6389 - val_loss: 1.0150\n",
            "Epoch 84/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6625 - loss: 0.9241 - val_accuracy: 0.6562 - val_loss: 1.0165\n",
            "Epoch 85/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6678 - loss: 0.8696 - val_accuracy: 0.6215 - val_loss: 1.0007\n",
            "Epoch 86/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7017 - loss: 0.8664 - val_accuracy: 0.6389 - val_loss: 1.0099\n",
            "Epoch 87/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6941 - loss: 0.8491 - val_accuracy: 0.6215 - val_loss: 1.0189\n",
            "Epoch 88/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6760 - loss: 0.8221 - val_accuracy: 0.6181 - val_loss: 1.0400\n",
            "Epoch 89/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6861 - loss: 0.9067 - val_accuracy: 0.6528 - val_loss: 1.0029\n",
            "Epoch 90/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6551 - loss: 0.9008 - val_accuracy: 0.6354 - val_loss: 1.0381\n",
            "Epoch 91/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6867 - loss: 0.8291 - val_accuracy: 0.6146 - val_loss: 1.0536\n",
            "Epoch 92/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6964 - loss: 0.8108 - val_accuracy: 0.6458 - val_loss: 0.9994\n",
            "Epoch 93/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6844 - loss: 0.8766 - val_accuracy: 0.6181 - val_loss: 1.0212\n",
            "Epoch 94/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7467 - loss: 0.7608 - val_accuracy: 0.6319 - val_loss: 1.0104\n",
            "Epoch 95/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6993 - loss: 0.8302 - val_accuracy: 0.6285 - val_loss: 1.0495\n",
            "Epoch 96/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7102 - loss: 0.8148 - val_accuracy: 0.6042 - val_loss: 1.0758\n",
            "Epoch 97/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6914 - loss: 0.8573 - val_accuracy: 0.6250 - val_loss: 1.0492\n",
            "Epoch 98/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7104 - loss: 0.8368 - val_accuracy: 0.6597 - val_loss: 1.0261\n",
            "Epoch 99/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7051 - loss: 0.8296 - val_accuracy: 0.6215 - val_loss: 1.0511\n",
            "Epoch 100/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7024 - loss: 0.8215 - val_accuracy: 0.6285 - val_loss: 1.0823\n",
            "Epoch 101/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6968 - loss: 0.8650 - val_accuracy: 0.6250 - val_loss: 1.0861\n",
            "Epoch 102/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7229 - loss: 0.7762 - val_accuracy: 0.6181 - val_loss: 1.0682\n",
            "Epoch 103/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6913 - loss: 0.8132 - val_accuracy: 0.6042 - val_loss: 1.0718\n",
            "Epoch 104/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6960 - loss: 0.8254 - val_accuracy: 0.6250 - val_loss: 1.0295\n",
            "Epoch 105/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7140 - loss: 0.7821 - val_accuracy: 0.6319 - val_loss: 1.0561\n",
            "Epoch 106/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7118 - loss: 0.7875 - val_accuracy: 0.6076 - val_loss: 1.0609\n",
            "Epoch 107/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6764 - loss: 0.8521 - val_accuracy: 0.6319 - val_loss: 1.0586\n",
            "Epoch 108/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7175 - loss: 0.7614 - val_accuracy: 0.6354 - val_loss: 1.0816\n",
            "Epoch 109/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7340 - loss: 0.7727 - val_accuracy: 0.6319 - val_loss: 1.0601\n",
            "Epoch 110/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7026 - loss: 0.8510 - val_accuracy: 0.6319 - val_loss: 1.0270\n",
            "Epoch 111/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6869 - loss: 0.8497 - val_accuracy: 0.6424 - val_loss: 1.0437\n",
            "Epoch 112/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7200 - loss: 0.7739 - val_accuracy: 0.6528 - val_loss: 1.0291\n",
            "Epoch 113/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6674 - loss: 0.8869 - val_accuracy: 0.6285 - val_loss: 1.0283\n",
            "Epoch 114/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7083 - loss: 0.8712 - val_accuracy: 0.6181 - val_loss: 1.0593\n",
            "Epoch 115/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7094 - loss: 0.7974 - val_accuracy: 0.6458 - val_loss: 1.0196\n",
            "Epoch 116/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7027 - loss: 0.7940 - val_accuracy: 0.6493 - val_loss: 0.9849\n",
            "Epoch 117/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7084 - loss: 0.8116 - val_accuracy: 0.6875 - val_loss: 0.9785\n",
            "Epoch 118/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7313 - loss: 0.8031 - val_accuracy: 0.6632 - val_loss: 0.9793\n",
            "Epoch 119/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7174 - loss: 0.8781 - val_accuracy: 0.6562 - val_loss: 1.0037\n",
            "Epoch 120/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7367 - loss: 0.7612 - val_accuracy: 0.6597 - val_loss: 1.0215\n",
            "Epoch 121/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7055 - loss: 0.7900 - val_accuracy: 0.6562 - val_loss: 1.0076\n",
            "Epoch 122/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7198 - loss: 0.7911 - val_accuracy: 0.6528 - val_loss: 1.0237\n",
            "Epoch 123/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7172 - loss: 0.8032 - val_accuracy: 0.6667 - val_loss: 1.0289\n",
            "Epoch 124/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7183 - loss: 0.7705 - val_accuracy: 0.6597 - val_loss: 1.0221\n",
            "Epoch 125/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7499 - loss: 0.6517 - val_accuracy: 0.6562 - val_loss: 1.0303\n",
            "Epoch 126/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7320 - loss: 0.7354 - val_accuracy: 0.6493 - val_loss: 1.0387\n",
            "Epoch 127/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7243 - loss: 0.7509 - val_accuracy: 0.6319 - val_loss: 1.0177\n",
            "Epoch 128/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7531 - loss: 0.7090 - val_accuracy: 0.6354 - val_loss: 1.0521\n",
            "Epoch 129/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7095 - loss: 0.7903 - val_accuracy: 0.6319 - val_loss: 1.0365\n",
            "Epoch 130/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7347 - loss: 0.7488 - val_accuracy: 0.6319 - val_loss: 1.0439\n",
            "Epoch 131/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7545 - loss: 0.6906 - val_accuracy: 0.6424 - val_loss: 1.0237\n",
            "Epoch 132/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7066 - loss: 0.7850 - val_accuracy: 0.6562 - val_loss: 1.0142\n",
            "Epoch 133/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7549 - loss: 0.6895 - val_accuracy: 0.6181 - val_loss: 1.0551\n",
            "Epoch 134/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7417 - loss: 0.7393 - val_accuracy: 0.6493 - val_loss: 1.0848\n",
            "Epoch 135/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7212 - loss: 0.7376 - val_accuracy: 0.6493 - val_loss: 1.0814\n",
            "Epoch 136/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7315 - loss: 0.7688 - val_accuracy: 0.6354 - val_loss: 1.0634\n",
            "Epoch 137/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7307 - loss: 0.7470 - val_accuracy: 0.6181 - val_loss: 1.0848\n",
            "Epoch 138/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7669 - loss: 0.6867 - val_accuracy: 0.6389 - val_loss: 1.0309\n",
            "Epoch 139/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7492 - loss: 0.7543 - val_accuracy: 0.6215 - val_loss: 1.0149\n",
            "Epoch 140/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7278 - loss: 0.7520 - val_accuracy: 0.6458 - val_loss: 1.0459\n",
            "Epoch 141/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7374 - loss: 0.7514 - val_accuracy: 0.6250 - val_loss: 1.0521\n",
            "Epoch 142/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7378 - loss: 0.7144 - val_accuracy: 0.6458 - val_loss: 1.0375\n",
            "Epoch 143/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7635 - loss: 0.6991 - val_accuracy: 0.6389 - val_loss: 1.0440\n",
            "Epoch 144/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7358 - loss: 0.7256 - val_accuracy: 0.6319 - val_loss: 1.0429\n",
            "Epoch 145/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7271 - loss: 0.7354 - val_accuracy: 0.6458 - val_loss: 1.0495\n",
            "Epoch 146/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7410 - loss: 0.6987 - val_accuracy: 0.6458 - val_loss: 1.0408\n",
            "Epoch 147/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7513 - loss: 0.7101 - val_accuracy: 0.6389 - val_loss: 0.9997\n",
            "Epoch 148/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7749 - loss: 0.6769 - val_accuracy: 0.6528 - val_loss: 1.0321\n",
            "Epoch 149/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7408 - loss: 0.7220 - val_accuracy: 0.6424 - val_loss: 1.0437\n",
            "Epoch 150/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7338 - loss: 0.7249 - val_accuracy: 0.6597 - val_loss: 1.0357\n",
            "Epoch 151/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7431 - loss: 0.6995 - val_accuracy: 0.6528 - val_loss: 1.0206\n",
            "Epoch 152/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7438 - loss: 0.7204 - val_accuracy: 0.6528 - val_loss: 1.0189\n",
            "Epoch 153/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7641 - loss: 0.6700 - val_accuracy: 0.6389 - val_loss: 1.0408\n",
            "Epoch 154/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7072 - loss: 0.7960 - val_accuracy: 0.6528 - val_loss: 1.0272\n",
            "Epoch 155/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7028 - loss: 0.7673 - val_accuracy: 0.6562 - val_loss: 1.0394\n",
            "Epoch 156/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7484 - loss: 0.6965 - val_accuracy: 0.6389 - val_loss: 1.0211\n",
            "Epoch 157/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7442 - loss: 0.6903 - val_accuracy: 0.6493 - val_loss: 1.0238\n",
            "Epoch 158/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7093 - loss: 0.7821 - val_accuracy: 0.6424 - val_loss: 1.0151\n",
            "Epoch 159/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7466 - loss: 0.7091 - val_accuracy: 0.6458 - val_loss: 1.0151\n",
            "Epoch 160/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7236 - loss: 0.7790 - val_accuracy: 0.6597 - val_loss: 1.0499\n",
            "Epoch 161/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7362 - loss: 0.7057 - val_accuracy: 0.6528 - val_loss: 1.0065\n",
            "Epoch 162/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7294 - loss: 0.7196 - val_accuracy: 0.6597 - val_loss: 1.0409\n",
            "Epoch 163/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7565 - loss: 0.6713 - val_accuracy: 0.6528 - val_loss: 1.0536\n",
            "Epoch 164/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7566 - loss: 0.6733 - val_accuracy: 0.6424 - val_loss: 1.0190\n",
            "Epoch 165/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7335 - loss: 0.7666 - val_accuracy: 0.6528 - val_loss: 1.0050\n",
            "Epoch 166/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7526 - loss: 0.6604 - val_accuracy: 0.6493 - val_loss: 1.0194\n",
            "Epoch 167/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7553 - loss: 0.7172 - val_accuracy: 0.6736 - val_loss: 0.9825\n",
            "Epoch 168/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8083 - loss: 0.5933 - val_accuracy: 0.6667 - val_loss: 1.0083\n",
            "Epoch 169/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7592 - loss: 0.6866 - val_accuracy: 0.6354 - val_loss: 1.0251\n",
            "Epoch 170/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7633 - loss: 0.6873 - val_accuracy: 0.6528 - val_loss: 0.9995\n",
            "Epoch 171/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7385 - loss: 0.7768 - val_accuracy: 0.6771 - val_loss: 0.9768\n",
            "Epoch 172/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7614 - loss: 0.6616 - val_accuracy: 0.6528 - val_loss: 1.0208\n",
            "Epoch 173/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7327 - loss: 0.7443 - val_accuracy: 0.6632 - val_loss: 1.0068\n",
            "Epoch 174/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7686 - loss: 0.6410 - val_accuracy: 0.6424 - val_loss: 1.0415\n",
            "Epoch 175/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7523 - loss: 0.7136 - val_accuracy: 0.6632 - val_loss: 1.0414\n",
            "Epoch 176/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7476 - loss: 0.6828 - val_accuracy: 0.6493 - val_loss: 1.0563\n",
            "Epoch 177/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7663 - loss: 0.6930 - val_accuracy: 0.6597 - val_loss: 0.9922\n",
            "Epoch 178/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7631 - loss: 0.6772 - val_accuracy: 0.6597 - val_loss: 1.0387\n",
            "Epoch 179/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7400 - loss: 0.6831 - val_accuracy: 0.6701 - val_loss: 1.0096\n",
            "Epoch 180/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7487 - loss: 0.6994 - val_accuracy: 0.6528 - val_loss: 1.0457\n",
            "Epoch 181/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7507 - loss: 0.6411 - val_accuracy: 0.6562 - val_loss: 1.0540\n",
            "Epoch 182/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7662 - loss: 0.6962 - val_accuracy: 0.6458 - val_loss: 1.0054\n",
            "Epoch 183/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7389 - loss: 0.6932 - val_accuracy: 0.6493 - val_loss: 0.9990\n",
            "Epoch 184/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7447 - loss: 0.6847 - val_accuracy: 0.6562 - val_loss: 1.0345\n",
            "Epoch 185/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7300 - loss: 0.7604 - val_accuracy: 0.6875 - val_loss: 0.9813\n",
            "Epoch 186/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7437 - loss: 0.7216 - val_accuracy: 0.6736 - val_loss: 0.9926\n",
            "Epoch 187/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7629 - loss: 0.6611 - val_accuracy: 0.6493 - val_loss: 0.9993\n",
            "Epoch 188/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7823 - loss: 0.6373 - val_accuracy: 0.6632 - val_loss: 1.0110\n",
            "Epoch 189/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7792 - loss: 0.6417 - val_accuracy: 0.6701 - val_loss: 1.0171\n",
            "Epoch 190/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7534 - loss: 0.7172 - val_accuracy: 0.6597 - val_loss: 1.0504\n",
            "Epoch 191/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7847 - loss: 0.6594 - val_accuracy: 0.6493 - val_loss: 1.0485\n",
            "Epoch 192/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7498 - loss: 0.6563 - val_accuracy: 0.6667 - val_loss: 1.0314\n",
            "Epoch 193/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7718 - loss: 0.5976 - val_accuracy: 0.6632 - val_loss: 1.0456\n",
            "Epoch 194/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7713 - loss: 0.6254 - val_accuracy: 0.6736 - val_loss: 0.9950\n",
            "Epoch 195/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7315 - loss: 0.6982 - val_accuracy: 0.6632 - val_loss: 1.0300\n",
            "Epoch 196/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7472 - loss: 0.6867 - val_accuracy: 0.6910 - val_loss: 1.0274\n",
            "Epoch 197/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7729 - loss: 0.6044 - val_accuracy: 0.6597 - val_loss: 1.0198\n",
            "Epoch 198/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7694 - loss: 0.6459 - val_accuracy: 0.6493 - val_loss: 1.0428\n",
            "Epoch 199/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7789 - loss: 0.6484 - val_accuracy: 0.6458 - val_loss: 1.0694\n",
            "Epoch 200/200\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7782 - loss: 0.6413 - val_accuracy: 0.6597 - val_loss: 1.0370\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP model parameters\n",
        "num_layers = 5\n",
        "units = [65, 260, 364, 364, 52]\n",
        "activations = ['relu', 'relu', 'relu', 'relu', 'relu']\n",
        "dropouts = [0.1, 0.3, 0.4, 0.1, 0.1]\n",
        "learning_rate = 0.0010343990168720423\n",
        "loss_function = 'sparse_categorical_crossentropy'\n",
        "input_shape = (13,)"
      ],
      "metadata": {
        "id": "IN1ffNliA6MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the MLP model\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Input layer\n",
        "model.add(layers.Input(shape=input_shape))\n",
        "\n",
        "# Hidden layers with batch normalization\n",
        "for i in range(num_layers):\n",
        "    model.add(layers.Dense(units[i]))\n",
        "    model.add(layers.BatchNormalization())  # Add batch normalization\n",
        "    model.add(layers.Activation(activations[i]))  # Apply activation after batch normalization\n",
        "    model.add(layers.Dropout(dropouts[i]))  # Add dropout for regularization\n",
        "\n",
        "# Output layer (adjust the number of units to your number of classes)\n",
        "model.add(layers.Dense(8, activation='softmax'))  # Assuming 8 classes for output\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=loss_function,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        },
        "id": "XlAnDxTzBu0V",
        "outputId": "ddb2cefd-2b1b-4851-c132-f47b752a0e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_46 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │             \u001b[38;5;34m910\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_22               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │             \u001b[38;5;34m260\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_10 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_39 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_47 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │          \u001b[38;5;34m17,160\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_23               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │           \u001b[38;5;34m1,040\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_11 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_40 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │          \u001b[38;5;34m95,004\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_24               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │           \u001b[38;5;34m1,456\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_12 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_41 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │         \u001b[38;5;34m132,860\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_25               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │           \u001b[38;5;34m1,456\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_13 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_42 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_50 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │          \u001b[38;5;34m18,980\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_26               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │             \u001b[38;5;34m208\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_14 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_43 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_51 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │             \u001b[38;5;34m424\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">910</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_22               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,160</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_23               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">95,004</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_24               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,456</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">132,860</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_25               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,456</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,980</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_26               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">424</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m269,758\u001b[0m (1.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,758</span> (1.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m267,548\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">267,548</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,210\u001b[0m (8.63 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,210</span> (8.63 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=500,batch_size=32, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZqfiEtxB0Dk",
        "outputId": "160dd5ce-23bc-4fda-da1a-40e3895e1591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.1747 - loss: 2.2541 - val_accuracy: 0.2535 - val_loss: 1.9670\n",
            "Epoch 2/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.2976 - loss: 1.8221 - val_accuracy: 0.3403 - val_loss: 1.8610\n",
            "Epoch 3/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.3448 - loss: 1.7558 - val_accuracy: 0.3750 - val_loss: 1.7578\n",
            "Epoch 4/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.3726 - loss: 1.6770 - val_accuracy: 0.4062 - val_loss: 1.6736\n",
            "Epoch 5/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4088 - loss: 1.5719 - val_accuracy: 0.4410 - val_loss: 1.5573\n",
            "Epoch 6/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4397 - loss: 1.5221 - val_accuracy: 0.4618 - val_loss: 1.5127\n",
            "Epoch 7/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4370 - loss: 1.5061 - val_accuracy: 0.4722 - val_loss: 1.4824\n",
            "Epoch 8/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.3898 - loss: 1.5321 - val_accuracy: 0.4861 - val_loss: 1.4278\n",
            "Epoch 9/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4414 - loss: 1.4605 - val_accuracy: 0.4826 - val_loss: 1.4164\n",
            "Epoch 10/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4662 - loss: 1.3737 - val_accuracy: 0.4757 - val_loss: 1.3984\n",
            "Epoch 11/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4861 - loss: 1.4023 - val_accuracy: 0.5208 - val_loss: 1.3881\n",
            "Epoch 12/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4823 - loss: 1.3179 - val_accuracy: 0.5069 - val_loss: 1.3623\n",
            "Epoch 13/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5049 - loss: 1.3471 - val_accuracy: 0.5104 - val_loss: 1.3362\n",
            "Epoch 14/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4937 - loss: 1.3407 - val_accuracy: 0.5035 - val_loss: 1.3233\n",
            "Epoch 15/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5039 - loss: 1.2777 - val_accuracy: 0.4965 - val_loss: 1.3170\n",
            "Epoch 16/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5293 - loss: 1.3016 - val_accuracy: 0.5139 - val_loss: 1.3251\n",
            "Epoch 17/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5131 - loss: 1.2884 - val_accuracy: 0.5347 - val_loss: 1.2940\n",
            "Epoch 18/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5236 - loss: 1.2438 - val_accuracy: 0.5347 - val_loss: 1.3011\n",
            "Epoch 19/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5431 - loss: 1.2379 - val_accuracy: 0.5208 - val_loss: 1.2861\n",
            "Epoch 20/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5389 - loss: 1.1969 - val_accuracy: 0.5243 - val_loss: 1.2641\n",
            "Epoch 21/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5691 - loss: 1.1771 - val_accuracy: 0.5347 - val_loss: 1.2692\n",
            "Epoch 22/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5438 - loss: 1.2081 - val_accuracy: 0.5556 - val_loss: 1.2391\n",
            "Epoch 23/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6081 - loss: 1.1128 - val_accuracy: 0.5590 - val_loss: 1.2422\n",
            "Epoch 24/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5913 - loss: 1.1455 - val_accuracy: 0.5660 - val_loss: 1.2244\n",
            "Epoch 25/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5947 - loss: 1.1409 - val_accuracy: 0.5660 - val_loss: 1.2190\n",
            "Epoch 26/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5889 - loss: 1.1277 - val_accuracy: 0.5347 - val_loss: 1.2410\n",
            "Epoch 27/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5893 - loss: 1.1561 - val_accuracy: 0.5521 - val_loss: 1.2007\n",
            "Epoch 28/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.5739 - loss: 1.1484 - val_accuracy: 0.5451 - val_loss: 1.2248\n",
            "Epoch 29/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.5803 - loss: 1.1690 - val_accuracy: 0.5764 - val_loss: 1.1983\n",
            "Epoch 30/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6045 - loss: 1.1018 - val_accuracy: 0.5590 - val_loss: 1.1677\n",
            "Epoch 31/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6199 - loss: 1.0375 - val_accuracy: 0.5833 - val_loss: 1.1859\n",
            "Epoch 32/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6219 - loss: 1.0422 - val_accuracy: 0.5625 - val_loss: 1.1823\n",
            "Epoch 33/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6293 - loss: 1.0230 - val_accuracy: 0.5833 - val_loss: 1.1540\n",
            "Epoch 34/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6031 - loss: 1.0756 - val_accuracy: 0.5799 - val_loss: 1.1813\n",
            "Epoch 35/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5896 - loss: 1.0760 - val_accuracy: 0.5799 - val_loss: 1.1651\n",
            "Epoch 36/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5987 - loss: 1.0546 - val_accuracy: 0.5903 - val_loss: 1.1682\n",
            "Epoch 37/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6373 - loss: 1.0472 - val_accuracy: 0.5938 - val_loss: 1.1317\n",
            "Epoch 38/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6237 - loss: 1.0391 - val_accuracy: 0.5972 - val_loss: 1.1598\n",
            "Epoch 39/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6061 - loss: 1.0281 - val_accuracy: 0.5799 - val_loss: 1.1645\n",
            "Epoch 40/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6485 - loss: 0.9632 - val_accuracy: 0.5903 - val_loss: 1.1488\n",
            "Epoch 41/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6296 - loss: 1.0164 - val_accuracy: 0.5972 - val_loss: 1.1647\n",
            "Epoch 42/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6062 - loss: 1.0245 - val_accuracy: 0.6076 - val_loss: 1.1298\n",
            "Epoch 43/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6412 - loss: 0.9865 - val_accuracy: 0.5938 - val_loss: 1.1468\n",
            "Epoch 44/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6472 - loss: 0.9654 - val_accuracy: 0.5903 - val_loss: 1.1535\n",
            "Epoch 45/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6462 - loss: 0.9407 - val_accuracy: 0.5694 - val_loss: 1.2134\n",
            "Epoch 46/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6322 - loss: 0.9715 - val_accuracy: 0.5729 - val_loss: 1.1636\n",
            "Epoch 47/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6424 - loss: 0.9409 - val_accuracy: 0.5694 - val_loss: 1.1887\n",
            "Epoch 48/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.6800 - loss: 0.8785 - val_accuracy: 0.5451 - val_loss: 1.1856\n",
            "Epoch 49/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6746 - loss: 0.9026 - val_accuracy: 0.5868 - val_loss: 1.1494\n",
            "Epoch 50/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6459 - loss: 0.9178 - val_accuracy: 0.5833 - val_loss: 1.1898\n",
            "Epoch 51/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6477 - loss: 0.9839 - val_accuracy: 0.6076 - val_loss: 1.1393\n",
            "Epoch 52/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6458 - loss: 0.9481 - val_accuracy: 0.6111 - val_loss: 1.1440\n",
            "Epoch 53/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6744 - loss: 0.8860 - val_accuracy: 0.5972 - val_loss: 1.1411\n",
            "Epoch 54/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6982 - loss: 0.8560 - val_accuracy: 0.5764 - val_loss: 1.1396\n",
            "Epoch 55/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6339 - loss: 0.9419 - val_accuracy: 0.6042 - val_loss: 1.1502\n",
            "Epoch 56/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6671 - loss: 0.9335 - val_accuracy: 0.6042 - val_loss: 1.1509\n",
            "Epoch 57/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6732 - loss: 0.8505 - val_accuracy: 0.5799 - val_loss: 1.1403\n",
            "Epoch 58/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6995 - loss: 0.8308 - val_accuracy: 0.6007 - val_loss: 1.1130\n",
            "Epoch 59/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6909 - loss: 0.8433 - val_accuracy: 0.5938 - val_loss: 1.1711\n",
            "Epoch 60/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7007 - loss: 0.8376 - val_accuracy: 0.6181 - val_loss: 1.0998\n",
            "Epoch 61/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6718 - loss: 0.9182 - val_accuracy: 0.6076 - val_loss: 1.1340\n",
            "Epoch 62/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7197 - loss: 0.8040 - val_accuracy: 0.6111 - val_loss: 1.1499\n",
            "Epoch 63/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7100 - loss: 0.7947 - val_accuracy: 0.6215 - val_loss: 1.1493\n",
            "Epoch 64/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7156 - loss: 0.8056 - val_accuracy: 0.5903 - val_loss: 1.1598\n",
            "Epoch 65/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6998 - loss: 0.8121 - val_accuracy: 0.6146 - val_loss: 1.1523\n",
            "Epoch 66/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6568 - loss: 0.8836 - val_accuracy: 0.6181 - val_loss: 1.1273\n",
            "Epoch 67/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7080 - loss: 0.8354 - val_accuracy: 0.6250 - val_loss: 1.0694\n",
            "Epoch 68/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7177 - loss: 0.7884 - val_accuracy: 0.6111 - val_loss: 1.0862\n",
            "Epoch 69/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6948 - loss: 0.8329 - val_accuracy: 0.6285 - val_loss: 1.0653\n",
            "Epoch 70/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7012 - loss: 0.8415 - val_accuracy: 0.6354 - val_loss: 1.1006\n",
            "Epoch 71/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7166 - loss: 0.8332 - val_accuracy: 0.6076 - val_loss: 1.0873\n",
            "Epoch 72/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6985 - loss: 0.8285 - val_accuracy: 0.6319 - val_loss: 1.0925\n",
            "Epoch 73/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6849 - loss: 0.8353 - val_accuracy: 0.6181 - val_loss: 1.0999\n",
            "Epoch 74/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6859 - loss: 0.8190 - val_accuracy: 0.6319 - val_loss: 1.1057\n",
            "Epoch 75/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6935 - loss: 0.7949 - val_accuracy: 0.6528 - val_loss: 1.1007\n",
            "Epoch 76/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7148 - loss: 0.7578 - val_accuracy: 0.6285 - val_loss: 1.1202\n",
            "Epoch 77/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7414 - loss: 0.7628 - val_accuracy: 0.6285 - val_loss: 1.0944\n",
            "Epoch 78/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7241 - loss: 0.7483 - val_accuracy: 0.6076 - val_loss: 1.1482\n",
            "Epoch 79/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7405 - loss: 0.6944 - val_accuracy: 0.6215 - val_loss: 1.1023\n",
            "Epoch 80/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6831 - loss: 0.8150 - val_accuracy: 0.6250 - val_loss: 1.0953\n",
            "Epoch 81/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7307 - loss: 0.7936 - val_accuracy: 0.6389 - val_loss: 1.1028\n",
            "Epoch 82/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7469 - loss: 0.6973 - val_accuracy: 0.6007 - val_loss: 1.0919\n",
            "Epoch 83/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7174 - loss: 0.8049 - val_accuracy: 0.6250 - val_loss: 1.0754\n",
            "Epoch 84/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7153 - loss: 0.7875 - val_accuracy: 0.6111 - val_loss: 1.0891\n",
            "Epoch 85/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7197 - loss: 0.7772 - val_accuracy: 0.6146 - val_loss: 1.0999\n",
            "Epoch 86/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7462 - loss: 0.7222 - val_accuracy: 0.6250 - val_loss: 1.0991\n",
            "Epoch 87/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7403 - loss: 0.7138 - val_accuracy: 0.6146 - val_loss: 1.0959\n",
            "Epoch 88/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7263 - loss: 0.7493 - val_accuracy: 0.6076 - val_loss: 1.1271\n",
            "Epoch 89/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7402 - loss: 0.7120 - val_accuracy: 0.6215 - val_loss: 1.1106\n",
            "Epoch 90/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7500 - loss: 0.6857 - val_accuracy: 0.6042 - val_loss: 1.1490\n",
            "Epoch 91/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7354 - loss: 0.7203 - val_accuracy: 0.6215 - val_loss: 1.1412\n",
            "Epoch 92/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7284 - loss: 0.7279 - val_accuracy: 0.6181 - val_loss: 1.1421\n",
            "Epoch 93/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7502 - loss: 0.6948 - val_accuracy: 0.6215 - val_loss: 1.1020\n",
            "Epoch 94/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7171 - loss: 0.7640 - val_accuracy: 0.6215 - val_loss: 1.1067\n",
            "Epoch 95/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.7142 - loss: 0.7594 - val_accuracy: 0.6354 - val_loss: 1.1098\n",
            "Epoch 96/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7406 - loss: 0.7103 - val_accuracy: 0.6181 - val_loss: 1.1211\n",
            "Epoch 97/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7389 - loss: 0.7542 - val_accuracy: 0.6215 - val_loss: 1.1133\n",
            "Epoch 98/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7316 - loss: 0.7252 - val_accuracy: 0.6146 - val_loss: 1.0980\n",
            "Epoch 99/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7384 - loss: 0.7549 - val_accuracy: 0.6215 - val_loss: 1.0754\n",
            "Epoch 100/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7638 - loss: 0.6928 - val_accuracy: 0.6042 - val_loss: 1.1173\n",
            "Epoch 101/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7252 - loss: 0.7615 - val_accuracy: 0.6007 - val_loss: 1.1058\n",
            "Epoch 102/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7438 - loss: 0.7469 - val_accuracy: 0.6424 - val_loss: 1.1031\n",
            "Epoch 103/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7528 - loss: 0.6618 - val_accuracy: 0.6319 - val_loss: 1.1173\n",
            "Epoch 104/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7087 - loss: 0.8281 - val_accuracy: 0.6076 - val_loss: 1.1229\n",
            "Epoch 105/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7588 - loss: 0.7009 - val_accuracy: 0.6389 - val_loss: 1.1020\n",
            "Epoch 106/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7539 - loss: 0.7053 - val_accuracy: 0.6354 - val_loss: 1.1090\n",
            "Epoch 107/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7399 - loss: 0.7323 - val_accuracy: 0.6285 - val_loss: 1.1610\n",
            "Epoch 108/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7446 - loss: 0.7049 - val_accuracy: 0.6424 - val_loss: 1.1225\n",
            "Epoch 109/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7419 - loss: 0.6992 - val_accuracy: 0.6111 - val_loss: 1.1295\n",
            "Epoch 110/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7756 - loss: 0.6378 - val_accuracy: 0.6319 - val_loss: 1.1260\n",
            "Epoch 111/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7345 - loss: 0.6882 - val_accuracy: 0.6285 - val_loss: 1.1257\n",
            "Epoch 112/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7578 - loss: 0.6513 - val_accuracy: 0.6076 - val_loss: 1.1166\n",
            "Epoch 113/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7642 - loss: 0.6606 - val_accuracy: 0.6458 - val_loss: 1.0723\n",
            "Epoch 114/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7429 - loss: 0.6925 - val_accuracy: 0.6319 - val_loss: 1.1115\n",
            "Epoch 115/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7399 - loss: 0.6595 - val_accuracy: 0.6319 - val_loss: 1.0983\n",
            "Epoch 116/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7689 - loss: 0.6592 - val_accuracy: 0.6458 - val_loss: 1.0772\n",
            "Epoch 117/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7536 - loss: 0.6906 - val_accuracy: 0.6250 - val_loss: 1.0874\n",
            "Epoch 118/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7413 - loss: 0.6884 - val_accuracy: 0.6285 - val_loss: 1.1006\n",
            "Epoch 119/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7456 - loss: 0.6993 - val_accuracy: 0.6250 - val_loss: 1.1665\n",
            "Epoch 120/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7390 - loss: 0.7225 - val_accuracy: 0.6181 - val_loss: 1.1899\n",
            "Epoch 121/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7878 - loss: 0.6038 - val_accuracy: 0.6319 - val_loss: 1.1380\n",
            "Epoch 122/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7790 - loss: 0.5919 - val_accuracy: 0.6181 - val_loss: 1.1350\n",
            "Epoch 123/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7430 - loss: 0.7003 - val_accuracy: 0.6285 - val_loss: 1.1386\n",
            "Epoch 124/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7721 - loss: 0.6637 - val_accuracy: 0.6458 - val_loss: 1.1233\n",
            "Epoch 125/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7734 - loss: 0.6615 - val_accuracy: 0.6493 - val_loss: 1.1008\n",
            "Epoch 126/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7659 - loss: 0.6712 - val_accuracy: 0.6424 - val_loss: 1.1490\n",
            "Epoch 127/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7540 - loss: 0.6327 - val_accuracy: 0.6354 - val_loss: 1.1171\n",
            "Epoch 128/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7626 - loss: 0.6475 - val_accuracy: 0.6146 - val_loss: 1.1224\n",
            "Epoch 129/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7705 - loss: 0.6632 - val_accuracy: 0.6354 - val_loss: 1.1360\n",
            "Epoch 130/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7823 - loss: 0.6451 - val_accuracy: 0.6285 - val_loss: 1.1502\n",
            "Epoch 131/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7801 - loss: 0.5887 - val_accuracy: 0.6146 - val_loss: 1.1200\n",
            "Epoch 132/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7820 - loss: 0.6120 - val_accuracy: 0.6146 - val_loss: 1.1441\n",
            "Epoch 133/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7800 - loss: 0.5990 - val_accuracy: 0.6528 - val_loss: 1.0962\n",
            "Epoch 134/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7847 - loss: 0.6262 - val_accuracy: 0.6250 - val_loss: 1.1336\n",
            "Epoch 135/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7858 - loss: 0.6271 - val_accuracy: 0.6111 - val_loss: 1.1511\n",
            "Epoch 136/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7743 - loss: 0.6405 - val_accuracy: 0.6354 - val_loss: 1.1606\n",
            "Epoch 137/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7570 - loss: 0.6985 - val_accuracy: 0.6528 - val_loss: 1.0959\n",
            "Epoch 138/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7784 - loss: 0.6429 - val_accuracy: 0.6528 - val_loss: 1.0827\n",
            "Epoch 139/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7918 - loss: 0.5985 - val_accuracy: 0.6493 - val_loss: 1.0699\n",
            "Epoch 140/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7894 - loss: 0.6057 - val_accuracy: 0.6493 - val_loss: 1.0825\n",
            "Epoch 141/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8007 - loss: 0.5387 - val_accuracy: 0.6493 - val_loss: 1.1011\n",
            "Epoch 142/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7499 - loss: 0.6615 - val_accuracy: 0.6354 - val_loss: 1.1300\n",
            "Epoch 143/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7817 - loss: 0.6007 - val_accuracy: 0.6354 - val_loss: 1.1538\n",
            "Epoch 144/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7901 - loss: 0.5494 - val_accuracy: 0.6146 - val_loss: 1.1581\n",
            "Epoch 145/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7933 - loss: 0.5810 - val_accuracy: 0.6215 - val_loss: 1.1266\n",
            "Epoch 146/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7763 - loss: 0.6054 - val_accuracy: 0.6215 - val_loss: 1.1590\n",
            "Epoch 147/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7846 - loss: 0.6084 - val_accuracy: 0.6111 - val_loss: 1.1685\n",
            "Epoch 148/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7682 - loss: 0.6142 - val_accuracy: 0.6111 - val_loss: 1.1460\n",
            "Epoch 149/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8053 - loss: 0.5229 - val_accuracy: 0.6076 - val_loss: 1.1954\n",
            "Epoch 150/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7497 - loss: 0.6280 - val_accuracy: 0.6389 - val_loss: 1.1337\n",
            "Epoch 151/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7691 - loss: 0.6208 - val_accuracy: 0.6354 - val_loss: 1.1168\n",
            "Epoch 152/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8002 - loss: 0.5732 - val_accuracy: 0.6528 - val_loss: 1.1387\n",
            "Epoch 153/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7638 - loss: 0.6373 - val_accuracy: 0.6146 - val_loss: 1.1488\n",
            "Epoch 154/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8158 - loss: 0.5225 - val_accuracy: 0.6250 - val_loss: 1.1197\n",
            "Epoch 155/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7883 - loss: 0.6267 - val_accuracy: 0.5868 - val_loss: 1.1520\n",
            "Epoch 156/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7679 - loss: 0.6103 - val_accuracy: 0.6215 - val_loss: 1.1098\n",
            "Epoch 157/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7784 - loss: 0.6133 - val_accuracy: 0.6285 - val_loss: 1.1163\n",
            "Epoch 158/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7967 - loss: 0.5783 - val_accuracy: 0.6493 - val_loss: 1.0854\n",
            "Epoch 159/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7780 - loss: 0.5989 - val_accuracy: 0.6424 - val_loss: 1.1071\n",
            "Epoch 160/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7906 - loss: 0.6015 - val_accuracy: 0.6319 - val_loss: 1.1118\n",
            "Epoch 161/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7971 - loss: 0.5595 - val_accuracy: 0.6146 - val_loss: 1.1648\n",
            "Epoch 162/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7906 - loss: 0.5488 - val_accuracy: 0.6458 - val_loss: 1.1398\n",
            "Epoch 163/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7786 - loss: 0.6014 - val_accuracy: 0.6458 - val_loss: 1.1436\n",
            "Epoch 164/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7771 - loss: 0.5963 - val_accuracy: 0.6354 - val_loss: 1.1215\n",
            "Epoch 165/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8162 - loss: 0.5163 - val_accuracy: 0.6493 - val_loss: 1.1101\n",
            "Epoch 166/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7944 - loss: 0.5991 - val_accuracy: 0.6528 - val_loss: 1.0838\n",
            "Epoch 167/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7984 - loss: 0.5663 - val_accuracy: 0.6528 - val_loss: 1.0950\n",
            "Epoch 168/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8062 - loss: 0.5346 - val_accuracy: 0.6424 - val_loss: 1.1084\n",
            "Epoch 169/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8002 - loss: 0.5390 - val_accuracy: 0.6250 - val_loss: 1.1502\n",
            "Epoch 170/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8261 - loss: 0.5015 - val_accuracy: 0.6493 - val_loss: 1.1508\n",
            "Epoch 171/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7763 - loss: 0.5924 - val_accuracy: 0.6632 - val_loss: 1.1164\n",
            "Epoch 172/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7909 - loss: 0.5957 - val_accuracy: 0.6528 - val_loss: 1.1432\n",
            "Epoch 173/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7846 - loss: 0.6292 - val_accuracy: 0.6389 - val_loss: 1.1427\n",
            "Epoch 174/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7698 - loss: 0.6410 - val_accuracy: 0.6389 - val_loss: 1.1509\n",
            "Epoch 175/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7767 - loss: 0.5761 - val_accuracy: 0.6562 - val_loss: 1.1488\n",
            "Epoch 176/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7910 - loss: 0.6009 - val_accuracy: 0.6458 - val_loss: 1.1611\n",
            "Epoch 177/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8010 - loss: 0.5388 - val_accuracy: 0.6285 - val_loss: 1.1194\n",
            "Epoch 178/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8217 - loss: 0.5455 - val_accuracy: 0.6354 - val_loss: 1.1494\n",
            "Epoch 179/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7693 - loss: 0.6384 - val_accuracy: 0.6667 - val_loss: 1.1231\n",
            "Epoch 180/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8404 - loss: 0.4859 - val_accuracy: 0.6701 - val_loss: 1.0822\n",
            "Epoch 181/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7978 - loss: 0.5667 - val_accuracy: 0.6389 - val_loss: 1.1012\n",
            "Epoch 182/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7892 - loss: 0.5546 - val_accuracy: 0.6493 - val_loss: 1.1197\n",
            "Epoch 183/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8206 - loss: 0.5259 - val_accuracy: 0.6111 - val_loss: 1.1274\n",
            "Epoch 184/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8426 - loss: 0.4417 - val_accuracy: 0.6458 - val_loss: 1.1057\n",
            "Epoch 185/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8123 - loss: 0.4934 - val_accuracy: 0.6458 - val_loss: 1.0700\n",
            "Epoch 186/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7847 - loss: 0.5435 - val_accuracy: 0.6424 - val_loss: 1.1011\n",
            "Epoch 187/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7949 - loss: 0.5223 - val_accuracy: 0.6493 - val_loss: 1.1570\n",
            "Epoch 188/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8061 - loss: 0.5015 - val_accuracy: 0.6806 - val_loss: 1.1033\n",
            "Epoch 189/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8236 - loss: 0.5177 - val_accuracy: 0.6597 - val_loss: 1.0775\n",
            "Epoch 190/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8140 - loss: 0.5768 - val_accuracy: 0.6528 - val_loss: 1.1245\n",
            "Epoch 191/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7985 - loss: 0.5546 - val_accuracy: 0.6285 - val_loss: 1.1330\n",
            "Epoch 192/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8108 - loss: 0.5628 - val_accuracy: 0.6285 - val_loss: 1.1551\n",
            "Epoch 193/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8295 - loss: 0.5016 - val_accuracy: 0.6354 - val_loss: 1.1672\n",
            "Epoch 194/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8037 - loss: 0.5344 - val_accuracy: 0.6528 - val_loss: 1.1198\n",
            "Epoch 195/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8131 - loss: 0.5157 - val_accuracy: 0.6562 - val_loss: 1.1001\n",
            "Epoch 196/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8134 - loss: 0.5192 - val_accuracy: 0.6424 - val_loss: 1.0920\n",
            "Epoch 197/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8017 - loss: 0.5665 - val_accuracy: 0.6424 - val_loss: 1.0826\n",
            "Epoch 198/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8154 - loss: 0.5179 - val_accuracy: 0.6493 - val_loss: 1.1041\n",
            "Epoch 199/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8004 - loss: 0.5516 - val_accuracy: 0.6458 - val_loss: 1.1356\n",
            "Epoch 200/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8219 - loss: 0.4803 - val_accuracy: 0.6458 - val_loss: 1.1145\n",
            "Epoch 201/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8252 - loss: 0.4841 - val_accuracy: 0.6424 - val_loss: 1.0983\n",
            "Epoch 202/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8267 - loss: 0.5046 - val_accuracy: 0.6389 - val_loss: 1.0766\n",
            "Epoch 203/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7809 - loss: 0.5694 - val_accuracy: 0.6528 - val_loss: 1.1025\n",
            "Epoch 204/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7955 - loss: 0.5507 - val_accuracy: 0.6215 - val_loss: 1.1198\n",
            "Epoch 205/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8267 - loss: 0.5058 - val_accuracy: 0.6424 - val_loss: 1.1668\n",
            "Epoch 206/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8125 - loss: 0.5084 - val_accuracy: 0.6250 - val_loss: 1.1745\n",
            "Epoch 207/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8291 - loss: 0.5146 - val_accuracy: 0.6354 - val_loss: 1.1750\n",
            "Epoch 208/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7989 - loss: 0.5331 - val_accuracy: 0.6181 - val_loss: 1.1756\n",
            "Epoch 209/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8093 - loss: 0.5204 - val_accuracy: 0.6354 - val_loss: 1.1679\n",
            "Epoch 210/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8091 - loss: 0.5498 - val_accuracy: 0.6319 - val_loss: 1.1661\n",
            "Epoch 211/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8141 - loss: 0.5081 - val_accuracy: 0.6562 - val_loss: 1.1578\n",
            "Epoch 212/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8147 - loss: 0.5241 - val_accuracy: 0.6250 - val_loss: 1.1542\n",
            "Epoch 213/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8491 - loss: 0.4513 - val_accuracy: 0.6458 - val_loss: 1.1333\n",
            "Epoch 214/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8211 - loss: 0.5192 - val_accuracy: 0.6354 - val_loss: 1.1356\n",
            "Epoch 215/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8165 - loss: 0.5217 - val_accuracy: 0.6181 - val_loss: 1.1954\n",
            "Epoch 216/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8174 - loss: 0.5194 - val_accuracy: 0.6285 - val_loss: 1.1664\n",
            "Epoch 217/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8199 - loss: 0.5075 - val_accuracy: 0.6562 - val_loss: 1.1149\n",
            "Epoch 218/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8440 - loss: 0.4558 - val_accuracy: 0.6354 - val_loss: 1.1211\n",
            "Epoch 219/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8236 - loss: 0.4909 - val_accuracy: 0.6389 - val_loss: 1.1329\n",
            "Epoch 220/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8293 - loss: 0.5146 - val_accuracy: 0.6424 - val_loss: 1.1736\n",
            "Epoch 221/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8314 - loss: 0.5238 - val_accuracy: 0.6389 - val_loss: 1.1783\n",
            "Epoch 222/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8352 - loss: 0.5181 - val_accuracy: 0.6458 - val_loss: 1.1227\n",
            "Epoch 223/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8407 - loss: 0.4610 - val_accuracy: 0.6354 - val_loss: 1.1127\n",
            "Epoch 224/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8490 - loss: 0.4129 - val_accuracy: 0.6250 - val_loss: 1.1169\n",
            "Epoch 225/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8324 - loss: 0.4828 - val_accuracy: 0.6389 - val_loss: 1.1278\n",
            "Epoch 226/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8224 - loss: 0.5270 - val_accuracy: 0.6562 - val_loss: 1.0999\n",
            "Epoch 227/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8415 - loss: 0.4441 - val_accuracy: 0.6354 - val_loss: 1.1543\n",
            "Epoch 228/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8198 - loss: 0.5185 - val_accuracy: 0.6250 - val_loss: 1.1558\n",
            "Epoch 229/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8323 - loss: 0.4894 - val_accuracy: 0.6354 - val_loss: 1.1383\n",
            "Epoch 230/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8557 - loss: 0.4894 - val_accuracy: 0.6424 - val_loss: 1.1336\n",
            "Epoch 231/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8134 - loss: 0.5130 - val_accuracy: 0.6181 - val_loss: 1.1856\n",
            "Epoch 232/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8303 - loss: 0.5073 - val_accuracy: 0.6215 - val_loss: 1.1983\n",
            "Epoch 233/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8036 - loss: 0.5356 - val_accuracy: 0.6285 - val_loss: 1.2130\n",
            "Epoch 234/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8237 - loss: 0.5083 - val_accuracy: 0.6319 - val_loss: 1.1635\n",
            "Epoch 235/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8185 - loss: 0.5035 - val_accuracy: 0.6146 - val_loss: 1.1557\n",
            "Epoch 236/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8791 - loss: 0.4037 - val_accuracy: 0.6181 - val_loss: 1.1589\n",
            "Epoch 237/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8263 - loss: 0.5006 - val_accuracy: 0.6319 - val_loss: 1.1708\n",
            "Epoch 238/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8353 - loss: 0.4728 - val_accuracy: 0.6319 - val_loss: 1.1534\n",
            "Epoch 239/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8242 - loss: 0.5106 - val_accuracy: 0.6181 - val_loss: 1.1275\n",
            "Epoch 240/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8382 - loss: 0.4737 - val_accuracy: 0.6354 - val_loss: 1.1003\n",
            "Epoch 241/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8085 - loss: 0.5320 - val_accuracy: 0.6250 - val_loss: 1.1545\n",
            "Epoch 242/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8229 - loss: 0.5619 - val_accuracy: 0.6493 - val_loss: 1.1469\n",
            "Epoch 243/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8566 - loss: 0.4230 - val_accuracy: 0.6354 - val_loss: 1.1491\n",
            "Epoch 244/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8190 - loss: 0.4769 - val_accuracy: 0.6285 - val_loss: 1.1300\n",
            "Epoch 245/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8484 - loss: 0.4201 - val_accuracy: 0.6424 - val_loss: 1.1350\n",
            "Epoch 246/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8159 - loss: 0.5184 - val_accuracy: 0.6354 - val_loss: 1.1479\n",
            "Epoch 247/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8373 - loss: 0.4576 - val_accuracy: 0.6354 - val_loss: 1.1504\n",
            "Epoch 248/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8147 - loss: 0.4958 - val_accuracy: 0.6493 - val_loss: 1.1550\n",
            "Epoch 249/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8149 - loss: 0.5616 - val_accuracy: 0.6424 - val_loss: 1.1581\n",
            "Epoch 250/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8170 - loss: 0.5418 - val_accuracy: 0.6458 - val_loss: 1.2018\n",
            "Epoch 251/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8455 - loss: 0.4479 - val_accuracy: 0.6562 - val_loss: 1.1693\n",
            "Epoch 252/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8133 - loss: 0.4773 - val_accuracy: 0.6562 - val_loss: 1.1158\n",
            "Epoch 253/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7877 - loss: 0.6014 - val_accuracy: 0.6493 - val_loss: 1.1397\n",
            "Epoch 254/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8383 - loss: 0.4427 - val_accuracy: 0.6354 - val_loss: 1.1190\n",
            "Epoch 255/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8150 - loss: 0.5186 - val_accuracy: 0.6562 - val_loss: 1.1308\n",
            "Epoch 256/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8336 - loss: 0.4426 - val_accuracy: 0.6424 - val_loss: 1.1408\n",
            "Epoch 257/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8270 - loss: 0.4904 - val_accuracy: 0.6319 - val_loss: 1.1547\n",
            "Epoch 258/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8342 - loss: 0.4786 - val_accuracy: 0.6146 - val_loss: 1.1796\n",
            "Epoch 259/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8091 - loss: 0.5369 - val_accuracy: 0.6354 - val_loss: 1.1805\n",
            "Epoch 260/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8236 - loss: 0.4954 - val_accuracy: 0.6389 - val_loss: 1.0859\n",
            "Epoch 261/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8271 - loss: 0.4625 - val_accuracy: 0.6285 - val_loss: 1.1100\n",
            "Epoch 262/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8301 - loss: 0.4977 - val_accuracy: 0.6354 - val_loss: 1.0786\n",
            "Epoch 263/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8503 - loss: 0.4528 - val_accuracy: 0.6458 - val_loss: 1.1117\n",
            "Epoch 264/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7996 - loss: 0.4879 - val_accuracy: 0.6389 - val_loss: 1.1520\n",
            "Epoch 265/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8047 - loss: 0.5298 - val_accuracy: 0.6250 - val_loss: 1.1209\n",
            "Epoch 266/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8461 - loss: 0.4943 - val_accuracy: 0.6424 - val_loss: 1.0959\n",
            "Epoch 267/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8499 - loss: 0.4408 - val_accuracy: 0.6181 - val_loss: 1.1557\n",
            "Epoch 268/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8408 - loss: 0.4445 - val_accuracy: 0.6354 - val_loss: 1.1513\n",
            "Epoch 269/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8755 - loss: 0.3774 - val_accuracy: 0.6250 - val_loss: 1.1567\n",
            "Epoch 270/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8502 - loss: 0.4548 - val_accuracy: 0.6250 - val_loss: 1.1510\n",
            "Epoch 271/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8461 - loss: 0.4500 - val_accuracy: 0.6493 - val_loss: 1.1316\n",
            "Epoch 272/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8558 - loss: 0.3953 - val_accuracy: 0.6285 - val_loss: 1.1708\n",
            "Epoch 273/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8330 - loss: 0.4831 - val_accuracy: 0.6285 - val_loss: 1.1721\n",
            "Epoch 274/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8139 - loss: 0.4870 - val_accuracy: 0.6111 - val_loss: 1.2260\n",
            "Epoch 275/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8445 - loss: 0.4386 - val_accuracy: 0.6250 - val_loss: 1.2291\n",
            "Epoch 276/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8656 - loss: 0.4293 - val_accuracy: 0.6285 - val_loss: 1.2114\n",
            "Epoch 277/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8491 - loss: 0.3999 - val_accuracy: 0.5938 - val_loss: 1.2079\n",
            "Epoch 278/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8428 - loss: 0.4505 - val_accuracy: 0.6146 - val_loss: 1.1955\n",
            "Epoch 279/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8230 - loss: 0.5104 - val_accuracy: 0.6111 - val_loss: 1.1831\n",
            "Epoch 280/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8511 - loss: 0.4567 - val_accuracy: 0.6319 - val_loss: 1.1658\n",
            "Epoch 281/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8652 - loss: 0.3806 - val_accuracy: 0.6146 - val_loss: 1.1729\n",
            "Epoch 282/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8432 - loss: 0.4549 - val_accuracy: 0.6181 - val_loss: 1.2107\n",
            "Epoch 283/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8389 - loss: 0.4290 - val_accuracy: 0.6424 - val_loss: 1.2100\n",
            "Epoch 284/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8400 - loss: 0.4917 - val_accuracy: 0.6458 - val_loss: 1.1365\n",
            "Epoch 285/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8383 - loss: 0.4643 - val_accuracy: 0.6389 - val_loss: 1.1529\n",
            "Epoch 286/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8537 - loss: 0.4200 - val_accuracy: 0.6319 - val_loss: 1.1407\n",
            "Epoch 287/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8487 - loss: 0.4313 - val_accuracy: 0.6319 - val_loss: 1.1480\n",
            "Epoch 288/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8139 - loss: 0.5180 - val_accuracy: 0.6146 - val_loss: 1.1585\n",
            "Epoch 289/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8358 - loss: 0.4336 - val_accuracy: 0.6181 - val_loss: 1.1783\n",
            "Epoch 290/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8520 - loss: 0.4299 - val_accuracy: 0.6215 - val_loss: 1.1826\n",
            "Epoch 291/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8407 - loss: 0.4105 - val_accuracy: 0.6111 - val_loss: 1.2066\n",
            "Epoch 292/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8754 - loss: 0.3941 - val_accuracy: 0.6146 - val_loss: 1.2306\n",
            "Epoch 293/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8262 - loss: 0.4853 - val_accuracy: 0.6424 - val_loss: 1.1705\n",
            "Epoch 294/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8601 - loss: 0.3989 - val_accuracy: 0.6493 - val_loss: 1.1580\n",
            "Epoch 295/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8685 - loss: 0.3883 - val_accuracy: 0.6458 - val_loss: 1.1471\n",
            "Epoch 296/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8224 - loss: 0.4695 - val_accuracy: 0.6319 - val_loss: 1.1555\n",
            "Epoch 297/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8226 - loss: 0.4862 - val_accuracy: 0.6319 - val_loss: 1.1640\n",
            "Epoch 298/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8447 - loss: 0.4293 - val_accuracy: 0.6493 - val_loss: 1.1301\n",
            "Epoch 299/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8505 - loss: 0.3925 - val_accuracy: 0.6285 - val_loss: 1.1350\n",
            "Epoch 300/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8279 - loss: 0.4981 - val_accuracy: 0.6319 - val_loss: 1.1749\n",
            "Epoch 301/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8595 - loss: 0.4100 - val_accuracy: 0.6493 - val_loss: 1.1281\n",
            "Epoch 302/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8651 - loss: 0.3928 - val_accuracy: 0.6354 - val_loss: 1.1195\n",
            "Epoch 303/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8490 - loss: 0.4440 - val_accuracy: 0.6250 - val_loss: 1.1405\n",
            "Epoch 304/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8422 - loss: 0.4052 - val_accuracy: 0.6319 - val_loss: 1.1313\n",
            "Epoch 305/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8645 - loss: 0.4074 - val_accuracy: 0.6181 - val_loss: 1.1533\n",
            "Epoch 306/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8562 - loss: 0.4114 - val_accuracy: 0.6424 - val_loss: 1.1477\n",
            "Epoch 307/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8274 - loss: 0.4699 - val_accuracy: 0.6389 - val_loss: 1.1198\n",
            "Epoch 308/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8624 - loss: 0.4216 - val_accuracy: 0.6424 - val_loss: 1.1099\n",
            "Epoch 309/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8631 - loss: 0.4372 - val_accuracy: 0.6528 - val_loss: 1.1162\n",
            "Epoch 310/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8271 - loss: 0.4913 - val_accuracy: 0.6528 - val_loss: 1.1652\n",
            "Epoch 311/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8720 - loss: 0.4006 - val_accuracy: 0.6354 - val_loss: 1.1626\n",
            "Epoch 312/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8578 - loss: 0.4533 - val_accuracy: 0.6215 - val_loss: 1.1693\n",
            "Epoch 313/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8717 - loss: 0.3419 - val_accuracy: 0.6285 - val_loss: 1.1722\n",
            "Epoch 314/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8769 - loss: 0.3543 - val_accuracy: 0.6354 - val_loss: 1.1662\n",
            "Epoch 315/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8275 - loss: 0.4765 - val_accuracy: 0.6458 - val_loss: 1.1749\n",
            "Epoch 316/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8372 - loss: 0.4429 - val_accuracy: 0.6528 - val_loss: 1.1323\n",
            "Epoch 317/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8662 - loss: 0.4012 - val_accuracy: 0.6701 - val_loss: 1.1060\n",
            "Epoch 318/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8637 - loss: 0.4035 - val_accuracy: 0.6389 - val_loss: 1.1602\n",
            "Epoch 319/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8543 - loss: 0.4652 - val_accuracy: 0.6528 - val_loss: 1.1445\n",
            "Epoch 320/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8428 - loss: 0.4652 - val_accuracy: 0.6458 - val_loss: 1.1784\n",
            "Epoch 321/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8677 - loss: 0.3748 - val_accuracy: 0.6389 - val_loss: 1.1950\n",
            "Epoch 322/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8830 - loss: 0.3541 - val_accuracy: 0.6458 - val_loss: 1.1626\n",
            "Epoch 323/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8398 - loss: 0.4574 - val_accuracy: 0.6319 - val_loss: 1.1994\n",
            "Epoch 324/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8267 - loss: 0.5062 - val_accuracy: 0.6458 - val_loss: 1.1883\n",
            "Epoch 325/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8675 - loss: 0.3806 - val_accuracy: 0.6424 - val_loss: 1.1951\n",
            "Epoch 326/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8285 - loss: 0.4614 - val_accuracy: 0.6424 - val_loss: 1.1704\n",
            "Epoch 327/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8631 - loss: 0.3792 - val_accuracy: 0.6319 - val_loss: 1.1905\n",
            "Epoch 328/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8469 - loss: 0.4386 - val_accuracy: 0.6215 - val_loss: 1.1767\n",
            "Epoch 329/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8356 - loss: 0.4212 - val_accuracy: 0.6528 - val_loss: 1.1543\n",
            "Epoch 330/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8726 - loss: 0.3629 - val_accuracy: 0.6632 - val_loss: 1.1418\n",
            "Epoch 331/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8614 - loss: 0.4157 - val_accuracy: 0.6597 - val_loss: 1.1642\n",
            "Epoch 332/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8547 - loss: 0.3981 - val_accuracy: 0.6424 - val_loss: 1.1726\n",
            "Epoch 333/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8604 - loss: 0.3846 - val_accuracy: 0.6354 - val_loss: 1.1671\n",
            "Epoch 334/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8665 - loss: 0.3769 - val_accuracy: 0.6424 - val_loss: 1.1675\n",
            "Epoch 335/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8553 - loss: 0.4250 - val_accuracy: 0.6389 - val_loss: 1.1892\n",
            "Epoch 336/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8315 - loss: 0.4616 - val_accuracy: 0.6354 - val_loss: 1.1887\n",
            "Epoch 337/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8705 - loss: 0.3911 - val_accuracy: 0.6215 - val_loss: 1.1877\n",
            "Epoch 338/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8265 - loss: 0.5272 - val_accuracy: 0.6250 - val_loss: 1.1668\n",
            "Epoch 339/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8429 - loss: 0.4071 - val_accuracy: 0.6354 - val_loss: 1.1803\n",
            "Epoch 340/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8458 - loss: 0.4193 - val_accuracy: 0.6181 - val_loss: 1.1888\n",
            "Epoch 341/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8466 - loss: 0.4690 - val_accuracy: 0.6181 - val_loss: 1.1726\n",
            "Epoch 342/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8567 - loss: 0.4010 - val_accuracy: 0.6389 - val_loss: 1.1509\n",
            "Epoch 343/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8635 - loss: 0.4030 - val_accuracy: 0.6354 - val_loss: 1.1486\n",
            "Epoch 344/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8454 - loss: 0.4158 - val_accuracy: 0.6389 - val_loss: 1.1381\n",
            "Epoch 345/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8458 - loss: 0.4012 - val_accuracy: 0.6424 - val_loss: 1.1161\n",
            "Epoch 346/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8390 - loss: 0.4428 - val_accuracy: 0.6424 - val_loss: 1.1767\n",
            "Epoch 347/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8382 - loss: 0.4176 - val_accuracy: 0.6354 - val_loss: 1.1899\n",
            "Epoch 348/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8608 - loss: 0.4050 - val_accuracy: 0.6493 - val_loss: 1.1911\n",
            "Epoch 349/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8498 - loss: 0.4307 - val_accuracy: 0.6319 - val_loss: 1.1689\n",
            "Epoch 350/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8310 - loss: 0.4685 - val_accuracy: 0.6493 - val_loss: 1.1285\n",
            "Epoch 351/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8390 - loss: 0.4225 - val_accuracy: 0.6597 - val_loss: 1.1582\n",
            "Epoch 352/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8559 - loss: 0.4177 - val_accuracy: 0.6597 - val_loss: 1.1573\n",
            "Epoch 353/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8695 - loss: 0.3398 - val_accuracy: 0.6319 - val_loss: 1.2298\n",
            "Epoch 354/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8408 - loss: 0.4570 - val_accuracy: 0.6285 - val_loss: 1.2470\n",
            "Epoch 355/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8862 - loss: 0.3382 - val_accuracy: 0.6319 - val_loss: 1.2193\n",
            "Epoch 356/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8543 - loss: 0.3665 - val_accuracy: 0.6285 - val_loss: 1.2050\n",
            "Epoch 357/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8355 - loss: 0.4224 - val_accuracy: 0.6250 - val_loss: 1.2387\n",
            "Epoch 358/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8576 - loss: 0.4312 - val_accuracy: 0.6424 - val_loss: 1.2309\n",
            "Epoch 359/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8588 - loss: 0.3869 - val_accuracy: 0.6528 - val_loss: 1.2196\n",
            "Epoch 360/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8573 - loss: 0.3976 - val_accuracy: 0.6319 - val_loss: 1.2139\n",
            "Epoch 361/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8611 - loss: 0.3992 - val_accuracy: 0.6354 - val_loss: 1.1928\n",
            "Epoch 362/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8624 - loss: 0.3975 - val_accuracy: 0.6736 - val_loss: 1.1772\n",
            "Epoch 363/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8591 - loss: 0.3893 - val_accuracy: 0.6389 - val_loss: 1.2046\n",
            "Epoch 364/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8583 - loss: 0.3992 - val_accuracy: 0.6458 - val_loss: 1.1577\n",
            "Epoch 365/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8741 - loss: 0.3538 - val_accuracy: 0.6424 - val_loss: 1.1736\n",
            "Epoch 366/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8727 - loss: 0.3712 - val_accuracy: 0.6562 - val_loss: 1.1687\n",
            "Epoch 367/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8703 - loss: 0.3627 - val_accuracy: 0.6528 - val_loss: 1.1747\n",
            "Epoch 368/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8757 - loss: 0.3685 - val_accuracy: 0.6389 - val_loss: 1.1893\n",
            "Epoch 369/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8830 - loss: 0.3629 - val_accuracy: 0.6493 - val_loss: 1.1830\n",
            "Epoch 370/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8606 - loss: 0.4191 - val_accuracy: 0.6424 - val_loss: 1.2065\n",
            "Epoch 371/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8656 - loss: 0.3893 - val_accuracy: 0.6389 - val_loss: 1.2313\n",
            "Epoch 372/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8725 - loss: 0.3666 - val_accuracy: 0.6389 - val_loss: 1.1991\n",
            "Epoch 373/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8749 - loss: 0.3431 - val_accuracy: 0.6354 - val_loss: 1.1993\n",
            "Epoch 374/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8754 - loss: 0.3780 - val_accuracy: 0.6424 - val_loss: 1.1879\n",
            "Epoch 375/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8572 - loss: 0.4228 - val_accuracy: 0.6215 - val_loss: 1.2209\n",
            "Epoch 376/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8576 - loss: 0.4436 - val_accuracy: 0.6319 - val_loss: 1.2079\n",
            "Epoch 377/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8658 - loss: 0.3793 - val_accuracy: 0.6354 - val_loss: 1.2252\n",
            "Epoch 378/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8267 - loss: 0.4517 - val_accuracy: 0.6389 - val_loss: 1.2239\n",
            "Epoch 379/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8272 - loss: 0.4673 - val_accuracy: 0.6528 - val_loss: 1.2455\n",
            "Epoch 380/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8521 - loss: 0.4049 - val_accuracy: 0.6319 - val_loss: 1.2075\n",
            "Epoch 381/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8601 - loss: 0.3860 - val_accuracy: 0.6389 - val_loss: 1.1490\n",
            "Epoch 382/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8412 - loss: 0.4309 - val_accuracy: 0.6146 - val_loss: 1.2159\n",
            "Epoch 383/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8731 - loss: 0.3730 - val_accuracy: 0.6424 - val_loss: 1.2028\n",
            "Epoch 384/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8585 - loss: 0.3636 - val_accuracy: 0.6319 - val_loss: 1.1918\n",
            "Epoch 385/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8720 - loss: 0.3917 - val_accuracy: 0.6632 - val_loss: 1.1748\n",
            "Epoch 386/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8723 - loss: 0.3646 - val_accuracy: 0.6562 - val_loss: 1.1723\n",
            "Epoch 387/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8340 - loss: 0.4559 - val_accuracy: 0.6424 - val_loss: 1.1947\n",
            "Epoch 388/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8604 - loss: 0.3890 - val_accuracy: 0.6528 - val_loss: 1.1611\n",
            "Epoch 389/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8520 - loss: 0.4067 - val_accuracy: 0.6458 - val_loss: 1.1612\n",
            "Epoch 390/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8820 - loss: 0.3705 - val_accuracy: 0.6597 - val_loss: 1.1694\n",
            "Epoch 391/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8672 - loss: 0.3969 - val_accuracy: 0.6597 - val_loss: 1.1690\n",
            "Epoch 392/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8744 - loss: 0.3409 - val_accuracy: 0.6562 - val_loss: 1.1686\n",
            "Epoch 393/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8701 - loss: 0.3707 - val_accuracy: 0.6528 - val_loss: 1.1694\n",
            "Epoch 394/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8604 - loss: 0.4001 - val_accuracy: 0.6701 - val_loss: 1.1587\n",
            "Epoch 395/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8505 - loss: 0.4445 - val_accuracy: 0.6528 - val_loss: 1.1744\n",
            "Epoch 396/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8418 - loss: 0.4398 - val_accuracy: 0.6493 - val_loss: 1.1722\n",
            "Epoch 397/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8812 - loss: 0.3594 - val_accuracy: 0.6597 - val_loss: 1.1960\n",
            "Epoch 398/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8685 - loss: 0.4355 - val_accuracy: 0.6493 - val_loss: 1.2000\n",
            "Epoch 399/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8661 - loss: 0.3833 - val_accuracy: 0.6597 - val_loss: 1.1564\n",
            "Epoch 400/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8708 - loss: 0.3833 - val_accuracy: 0.6458 - val_loss: 1.1620\n",
            "Epoch 401/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8622 - loss: 0.3964 - val_accuracy: 0.6667 - val_loss: 1.1446\n",
            "Epoch 402/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8834 - loss: 0.3500 - val_accuracy: 0.6493 - val_loss: 1.1418\n",
            "Epoch 403/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8674 - loss: 0.3590 - val_accuracy: 0.6285 - val_loss: 1.2065\n",
            "Epoch 404/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8663 - loss: 0.3886 - val_accuracy: 0.6285 - val_loss: 1.2208\n",
            "Epoch 405/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8726 - loss: 0.3966 - val_accuracy: 0.6562 - val_loss: 1.1877\n",
            "Epoch 406/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8765 - loss: 0.3607 - val_accuracy: 0.6632 - val_loss: 1.2020\n",
            "Epoch 407/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8760 - loss: 0.3654 - val_accuracy: 0.6319 - val_loss: 1.1820\n",
            "Epoch 408/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8358 - loss: 0.4438 - val_accuracy: 0.6285 - val_loss: 1.2055\n",
            "Epoch 409/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8822 - loss: 0.3469 - val_accuracy: 0.6319 - val_loss: 1.2248\n",
            "Epoch 410/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8668 - loss: 0.4092 - val_accuracy: 0.6319 - val_loss: 1.2174\n",
            "Epoch 411/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8807 - loss: 0.3426 - val_accuracy: 0.6285 - val_loss: 1.2262\n",
            "Epoch 412/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8724 - loss: 0.3592 - val_accuracy: 0.6250 - val_loss: 1.2326\n",
            "Epoch 413/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8624 - loss: 0.3956 - val_accuracy: 0.6354 - val_loss: 1.1870\n",
            "Epoch 414/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8579 - loss: 0.4020 - val_accuracy: 0.6562 - val_loss: 1.1756\n",
            "Epoch 415/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9019 - loss: 0.3084 - val_accuracy: 0.6562 - val_loss: 1.1929\n",
            "Epoch 416/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8618 - loss: 0.3994 - val_accuracy: 0.6493 - val_loss: 1.2091\n",
            "Epoch 417/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8667 - loss: 0.3820 - val_accuracy: 0.6389 - val_loss: 1.1801\n",
            "Epoch 418/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8763 - loss: 0.3406 - val_accuracy: 0.6458 - val_loss: 1.1861\n",
            "Epoch 419/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8610 - loss: 0.4255 - val_accuracy: 0.6319 - val_loss: 1.1902\n",
            "Epoch 420/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8806 - loss: 0.3721 - val_accuracy: 0.6354 - val_loss: 1.1991\n",
            "Epoch 421/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8490 - loss: 0.4095 - val_accuracy: 0.6285 - val_loss: 1.2002\n",
            "Epoch 422/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8523 - loss: 0.4174 - val_accuracy: 0.6146 - val_loss: 1.2215\n",
            "Epoch 423/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8634 - loss: 0.3939 - val_accuracy: 0.6285 - val_loss: 1.2196\n",
            "Epoch 424/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8425 - loss: 0.4354 - val_accuracy: 0.6146 - val_loss: 1.2077\n",
            "Epoch 425/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8569 - loss: 0.3941 - val_accuracy: 0.6215 - val_loss: 1.2162\n",
            "Epoch 426/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8338 - loss: 0.4186 - val_accuracy: 0.6458 - val_loss: 1.2044\n",
            "Epoch 427/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8508 - loss: 0.4038 - val_accuracy: 0.6319 - val_loss: 1.2206\n",
            "Epoch 428/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8588 - loss: 0.3948 - val_accuracy: 0.6424 - val_loss: 1.2111\n",
            "Epoch 429/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8676 - loss: 0.3719 - val_accuracy: 0.6354 - val_loss: 1.1999\n",
            "Epoch 430/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8559 - loss: 0.4233 - val_accuracy: 0.6319 - val_loss: 1.2580\n",
            "Epoch 431/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8748 - loss: 0.3588 - val_accuracy: 0.6632 - val_loss: 1.1882\n",
            "Epoch 432/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8629 - loss: 0.4070 - val_accuracy: 0.6424 - val_loss: 1.2093\n",
            "Epoch 433/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8888 - loss: 0.3351 - val_accuracy: 0.6424 - val_loss: 1.1590\n",
            "Epoch 434/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8544 - loss: 0.4008 - val_accuracy: 0.6285 - val_loss: 1.2333\n",
            "Epoch 435/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8654 - loss: 0.3602 - val_accuracy: 0.6319 - val_loss: 1.1944\n",
            "Epoch 436/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8787 - loss: 0.3423 - val_accuracy: 0.6597 - val_loss: 1.1497\n",
            "Epoch 437/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8600 - loss: 0.4235 - val_accuracy: 0.6597 - val_loss: 1.1771\n",
            "Epoch 438/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8547 - loss: 0.4300 - val_accuracy: 0.6528 - val_loss: 1.1290\n",
            "Epoch 439/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8626 - loss: 0.3943 - val_accuracy: 0.6389 - val_loss: 1.1347\n",
            "Epoch 440/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8360 - loss: 0.4140 - val_accuracy: 0.6528 - val_loss: 1.1263\n",
            "Epoch 441/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8777 - loss: 0.3681 - val_accuracy: 0.6597 - val_loss: 1.1508\n",
            "Epoch 442/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8585 - loss: 0.3788 - val_accuracy: 0.6389 - val_loss: 1.1176\n",
            "Epoch 443/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8830 - loss: 0.3949 - val_accuracy: 0.6875 - val_loss: 1.1054\n",
            "Epoch 444/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8548 - loss: 0.4003 - val_accuracy: 0.6667 - val_loss: 1.1227\n",
            "Epoch 445/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8610 - loss: 0.3672 - val_accuracy: 0.6771 - val_loss: 1.1324\n",
            "Epoch 446/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8721 - loss: 0.3665 - val_accuracy: 0.6528 - val_loss: 1.1665\n",
            "Epoch 447/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8765 - loss: 0.3541 - val_accuracy: 0.6667 - val_loss: 1.1694\n",
            "Epoch 448/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8679 - loss: 0.3874 - val_accuracy: 0.6389 - val_loss: 1.1943\n",
            "Epoch 449/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8698 - loss: 0.4008 - val_accuracy: 0.6389 - val_loss: 1.1934\n",
            "Epoch 450/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8753 - loss: 0.3837 - val_accuracy: 0.6493 - val_loss: 1.2248\n",
            "Epoch 451/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8758 - loss: 0.3447 - val_accuracy: 0.6493 - val_loss: 1.1932\n",
            "Epoch 452/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8783 - loss: 0.3402 - val_accuracy: 0.6493 - val_loss: 1.1729\n",
            "Epoch 453/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8747 - loss: 0.3575 - val_accuracy: 0.6458 - val_loss: 1.1485\n",
            "Epoch 454/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8793 - loss: 0.3563 - val_accuracy: 0.6493 - val_loss: 1.1612\n",
            "Epoch 455/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8970 - loss: 0.3328 - val_accuracy: 0.6632 - val_loss: 1.1330\n",
            "Epoch 456/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8670 - loss: 0.3469 - val_accuracy: 0.6736 - val_loss: 1.0810\n",
            "Epoch 457/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8823 - loss: 0.3586 - val_accuracy: 0.6771 - val_loss: 1.1434\n",
            "Epoch 458/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8844 - loss: 0.3449 - val_accuracy: 0.6424 - val_loss: 1.1355\n",
            "Epoch 459/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8758 - loss: 0.3290 - val_accuracy: 0.6562 - val_loss: 1.1225\n",
            "Epoch 460/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8700 - loss: 0.3271 - val_accuracy: 0.6701 - val_loss: 1.1527\n",
            "Epoch 461/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8530 - loss: 0.4270 - val_accuracy: 0.6667 - val_loss: 1.1635\n",
            "Epoch 462/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8818 - loss: 0.3478 - val_accuracy: 0.6632 - val_loss: 1.1557\n",
            "Epoch 463/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8325 - loss: 0.4428 - val_accuracy: 0.6493 - val_loss: 1.1686\n",
            "Epoch 464/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8653 - loss: 0.4115 - val_accuracy: 0.6701 - val_loss: 1.1701\n",
            "Epoch 465/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8680 - loss: 0.3952 - val_accuracy: 0.6493 - val_loss: 1.1685\n",
            "Epoch 466/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8812 - loss: 0.3442 - val_accuracy: 0.6424 - val_loss: 1.1201\n",
            "Epoch 467/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8935 - loss: 0.3301 - val_accuracy: 0.6319 - val_loss: 1.1618\n",
            "Epoch 468/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8546 - loss: 0.3943 - val_accuracy: 0.6389 - val_loss: 1.1938\n",
            "Epoch 469/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8570 - loss: 0.3944 - val_accuracy: 0.6389 - val_loss: 1.1657\n",
            "Epoch 470/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8940 - loss: 0.3233 - val_accuracy: 0.6389 - val_loss: 1.1771\n",
            "Epoch 471/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8458 - loss: 0.3926 - val_accuracy: 0.6424 - val_loss: 1.1765\n",
            "Epoch 472/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8788 - loss: 0.3830 - val_accuracy: 0.6458 - val_loss: 1.2114\n",
            "Epoch 473/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8814 - loss: 0.3442 - val_accuracy: 0.6493 - val_loss: 1.2221\n",
            "Epoch 474/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8930 - loss: 0.3107 - val_accuracy: 0.6354 - val_loss: 1.1833\n",
            "Epoch 475/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8806 - loss: 0.3812 - val_accuracy: 0.6389 - val_loss: 1.2059\n",
            "Epoch 476/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8783 - loss: 0.3218 - val_accuracy: 0.6424 - val_loss: 1.2314\n",
            "Epoch 477/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8936 - loss: 0.3379 - val_accuracy: 0.6458 - val_loss: 1.2510\n",
            "Epoch 478/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8794 - loss: 0.3470 - val_accuracy: 0.6111 - val_loss: 1.2794\n",
            "Epoch 479/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8572 - loss: 0.3608 - val_accuracy: 0.6042 - val_loss: 1.2277\n",
            "Epoch 480/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8667 - loss: 0.3930 - val_accuracy: 0.6181 - val_loss: 1.2074\n",
            "Epoch 481/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8748 - loss: 0.3494 - val_accuracy: 0.6111 - val_loss: 1.1779\n",
            "Epoch 482/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8731 - loss: 0.4076 - val_accuracy: 0.6354 - val_loss: 1.1482\n",
            "Epoch 483/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8752 - loss: 0.3696 - val_accuracy: 0.6562 - val_loss: 1.2071\n",
            "Epoch 484/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8565 - loss: 0.4104 - val_accuracy: 0.6597 - val_loss: 1.1618\n",
            "Epoch 485/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8742 - loss: 0.3584 - val_accuracy: 0.6389 - val_loss: 1.2437\n",
            "Epoch 486/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8930 - loss: 0.2930 - val_accuracy: 0.6354 - val_loss: 1.2484\n",
            "Epoch 487/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8639 - loss: 0.3522 - val_accuracy: 0.6389 - val_loss: 1.1913\n",
            "Epoch 488/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8882 - loss: 0.3277 - val_accuracy: 0.6354 - val_loss: 1.1780\n",
            "Epoch 489/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8696 - loss: 0.3607 - val_accuracy: 0.6354 - val_loss: 1.1948\n",
            "Epoch 490/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8742 - loss: 0.3287 - val_accuracy: 0.6424 - val_loss: 1.2001\n",
            "Epoch 491/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8659 - loss: 0.3407 - val_accuracy: 0.6389 - val_loss: 1.1875\n",
            "Epoch 492/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8913 - loss: 0.3443 - val_accuracy: 0.6528 - val_loss: 1.2044\n",
            "Epoch 493/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8611 - loss: 0.3653 - val_accuracy: 0.6493 - val_loss: 1.2139\n",
            "Epoch 494/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8704 - loss: 0.3803 - val_accuracy: 0.6667 - val_loss: 1.1723\n",
            "Epoch 495/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8543 - loss: 0.3560 - val_accuracy: 0.6389 - val_loss: 1.1732\n",
            "Epoch 496/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8756 - loss: 0.3464 - val_accuracy: 0.6458 - val_loss: 1.1820\n",
            "Epoch 497/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8666 - loss: 0.3880 - val_accuracy: 0.6354 - val_loss: 1.2321\n",
            "Epoch 498/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8744 - loss: 0.3698 - val_accuracy: 0.6250 - val_loss: 1.2441\n",
            "Epoch 499/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8801 - loss: 0.3561 - val_accuracy: 0.6354 - val_loss: 1.2185\n",
            "Epoch 500/500\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8752 - loss: 0.3581 - val_accuracy: 0.6319 - val_loss: 1.2143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=269,batch_size=64, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEP1WwYtCdt7",
        "outputId": "a8aad46e-16f9-4b9c-e8de-edd2d1f2cd08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.8950 - loss: 0.3256 - val_accuracy: 0.6493 - val_loss: 1.2091\n",
            "Epoch 2/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9309 - loss: 0.2099 - val_accuracy: 0.6319 - val_loss: 1.1901\n",
            "Epoch 3/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9312 - loss: 0.2075 - val_accuracy: 0.6424 - val_loss: 1.1836\n",
            "Epoch 4/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9162 - loss: 0.2569 - val_accuracy: 0.6458 - val_loss: 1.1596\n",
            "Epoch 5/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9328 - loss: 0.1954 - val_accuracy: 0.6493 - val_loss: 1.1629\n",
            "Epoch 6/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9211 - loss: 0.2385 - val_accuracy: 0.6424 - val_loss: 1.1660\n",
            "Epoch 7/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9242 - loss: 0.2396 - val_accuracy: 0.6424 - val_loss: 1.1666\n",
            "Epoch 8/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9492 - loss: 0.1791 - val_accuracy: 0.6458 - val_loss: 1.1607\n",
            "Epoch 9/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9243 - loss: 0.2305 - val_accuracy: 0.6493 - val_loss: 1.1552\n",
            "Epoch 10/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9428 - loss: 0.1702 - val_accuracy: 0.6493 - val_loss: 1.1501\n",
            "Epoch 11/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9263 - loss: 0.2012 - val_accuracy: 0.6597 - val_loss: 1.1781\n",
            "Epoch 12/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9423 - loss: 0.2122 - val_accuracy: 0.6597 - val_loss: 1.1994\n",
            "Epoch 13/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9461 - loss: 0.1581 - val_accuracy: 0.6458 - val_loss: 1.2006\n",
            "Epoch 14/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9279 - loss: 0.2086 - val_accuracy: 0.6562 - val_loss: 1.2017\n",
            "Epoch 15/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9422 - loss: 0.1896 - val_accuracy: 0.6597 - val_loss: 1.2057\n",
            "Epoch 16/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9328 - loss: 0.2227 - val_accuracy: 0.6632 - val_loss: 1.1964\n",
            "Epoch 17/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9376 - loss: 0.1559 - val_accuracy: 0.6632 - val_loss: 1.2062\n",
            "Epoch 18/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9363 - loss: 0.1799 - val_accuracy: 0.6667 - val_loss: 1.1969\n",
            "Epoch 19/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9273 - loss: 0.1841 - val_accuracy: 0.6597 - val_loss: 1.1995\n",
            "Epoch 20/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9277 - loss: 0.2087 - val_accuracy: 0.6528 - val_loss: 1.1958\n",
            "Epoch 21/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9320 - loss: 0.2050 - val_accuracy: 0.6493 - val_loss: 1.1858\n",
            "Epoch 22/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9313 - loss: 0.2078 - val_accuracy: 0.6667 - val_loss: 1.1913\n",
            "Epoch 23/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9359 - loss: 0.1797 - val_accuracy: 0.6597 - val_loss: 1.2080\n",
            "Epoch 24/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9283 - loss: 0.2216 - val_accuracy: 0.6667 - val_loss: 1.2083\n",
            "Epoch 25/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9512 - loss: 0.1728 - val_accuracy: 0.6597 - val_loss: 1.2108\n",
            "Epoch 26/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9383 - loss: 0.1849 - val_accuracy: 0.6424 - val_loss: 1.2303\n",
            "Epoch 27/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9311 - loss: 0.2368 - val_accuracy: 0.6562 - val_loss: 1.2234\n",
            "Epoch 28/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9520 - loss: 0.1460 - val_accuracy: 0.6562 - val_loss: 1.2349\n",
            "Epoch 29/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9461 - loss: 0.1639 - val_accuracy: 0.6701 - val_loss: 1.2330\n",
            "Epoch 30/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9339 - loss: 0.1965 - val_accuracy: 0.6667 - val_loss: 1.2343\n",
            "Epoch 31/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9283 - loss: 0.2131 - val_accuracy: 0.6667 - val_loss: 1.2312\n",
            "Epoch 32/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9446 - loss: 0.2014 - val_accuracy: 0.6597 - val_loss: 1.2051\n",
            "Epoch 33/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9359 - loss: 0.1843 - val_accuracy: 0.6771 - val_loss: 1.1796\n",
            "Epoch 34/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9282 - loss: 0.2103 - val_accuracy: 0.6736 - val_loss: 1.1929\n",
            "Epoch 35/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9384 - loss: 0.1830 - val_accuracy: 0.6597 - val_loss: 1.2049\n",
            "Epoch 36/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9443 - loss: 0.1627 - val_accuracy: 0.6562 - val_loss: 1.2132\n",
            "Epoch 37/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9386 - loss: 0.1736 - val_accuracy: 0.6632 - val_loss: 1.2376\n",
            "Epoch 38/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9498 - loss: 0.1559 - val_accuracy: 0.6701 - val_loss: 1.2481\n",
            "Epoch 39/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9423 - loss: 0.1777 - val_accuracy: 0.6806 - val_loss: 1.2459\n",
            "Epoch 40/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9469 - loss: 0.1690 - val_accuracy: 0.6771 - val_loss: 1.2067\n",
            "Epoch 41/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9323 - loss: 0.2000 - val_accuracy: 0.6701 - val_loss: 1.2200\n",
            "Epoch 42/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9348 - loss: 0.2035 - val_accuracy: 0.6597 - val_loss: 1.2577\n",
            "Epoch 43/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9451 - loss: 0.1794 - val_accuracy: 0.6562 - val_loss: 1.2738\n",
            "Epoch 44/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9379 - loss: 0.1695 - val_accuracy: 0.6528 - val_loss: 1.2830\n",
            "Epoch 45/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9167 - loss: 0.2246 - val_accuracy: 0.6458 - val_loss: 1.2719\n",
            "Epoch 46/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9421 - loss: 0.1826 - val_accuracy: 0.6250 - val_loss: 1.3012\n",
            "Epoch 47/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9443 - loss: 0.1793 - val_accuracy: 0.6319 - val_loss: 1.2990\n",
            "Epoch 48/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9467 - loss: 0.1844 - val_accuracy: 0.6285 - val_loss: 1.2754\n",
            "Epoch 49/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9217 - loss: 0.2068 - val_accuracy: 0.6528 - val_loss: 1.2854\n",
            "Epoch 50/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9573 - loss: 0.1439 - val_accuracy: 0.6562 - val_loss: 1.2993\n",
            "Epoch 51/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9423 - loss: 0.1905 - val_accuracy: 0.6562 - val_loss: 1.2949\n",
            "Epoch 52/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9307 - loss: 0.1827 - val_accuracy: 0.6389 - val_loss: 1.2997\n",
            "Epoch 53/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9503 - loss: 0.1777 - val_accuracy: 0.6493 - val_loss: 1.3057\n",
            "Epoch 54/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9531 - loss: 0.1711 - val_accuracy: 0.6528 - val_loss: 1.3263\n",
            "Epoch 55/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9246 - loss: 0.2076 - val_accuracy: 0.6493 - val_loss: 1.3247\n",
            "Epoch 56/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9425 - loss: 0.1863 - val_accuracy: 0.6424 - val_loss: 1.3724\n",
            "Epoch 57/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9354 - loss: 0.1737 - val_accuracy: 0.6424 - val_loss: 1.3689\n",
            "Epoch 58/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9252 - loss: 0.2024 - val_accuracy: 0.6493 - val_loss: 1.3758\n",
            "Epoch 59/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9489 - loss: 0.1719 - val_accuracy: 0.6424 - val_loss: 1.3614\n",
            "Epoch 60/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9224 - loss: 0.2169 - val_accuracy: 0.6667 - val_loss: 1.2990\n",
            "Epoch 61/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9394 - loss: 0.1604 - val_accuracy: 0.6562 - val_loss: 1.2899\n",
            "Epoch 62/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9483 - loss: 0.1412 - val_accuracy: 0.6528 - val_loss: 1.3026\n",
            "Epoch 63/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9321 - loss: 0.2161 - val_accuracy: 0.6562 - val_loss: 1.3365\n",
            "Epoch 64/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9287 - loss: 0.1802 - val_accuracy: 0.6528 - val_loss: 1.3483\n",
            "Epoch 65/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9448 - loss: 0.1613 - val_accuracy: 0.6562 - val_loss: 1.3442\n",
            "Epoch 66/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9413 - loss: 0.1504 - val_accuracy: 0.6493 - val_loss: 1.3539\n",
            "Epoch 67/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9166 - loss: 0.2121 - val_accuracy: 0.6458 - val_loss: 1.3275\n",
            "Epoch 68/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9283 - loss: 0.1927 - val_accuracy: 0.6562 - val_loss: 1.2951\n",
            "Epoch 69/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9262 - loss: 0.2061 - val_accuracy: 0.6632 - val_loss: 1.2671\n",
            "Epoch 70/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9407 - loss: 0.1690 - val_accuracy: 0.6562 - val_loss: 1.2680\n",
            "Epoch 71/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9317 - loss: 0.2022 - val_accuracy: 0.6493 - val_loss: 1.2852\n",
            "Epoch 72/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9351 - loss: 0.1826 - val_accuracy: 0.6632 - val_loss: 1.2683\n",
            "Epoch 73/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9399 - loss: 0.1796 - val_accuracy: 0.6562 - val_loss: 1.2798\n",
            "Epoch 74/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9394 - loss: 0.2012 - val_accuracy: 0.6528 - val_loss: 1.3204\n",
            "Epoch 75/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9434 - loss: 0.1737 - val_accuracy: 0.6458 - val_loss: 1.3400\n",
            "Epoch 76/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9370 - loss: 0.1724 - val_accuracy: 0.6389 - val_loss: 1.3622\n",
            "Epoch 77/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9307 - loss: 0.2192 - val_accuracy: 0.6285 - val_loss: 1.3391\n",
            "Epoch 78/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9295 - loss: 0.1927 - val_accuracy: 0.6181 - val_loss: 1.3454\n",
            "Epoch 79/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9288 - loss: 0.2045 - val_accuracy: 0.6424 - val_loss: 1.3508\n",
            "Epoch 80/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9318 - loss: 0.2164 - val_accuracy: 0.6528 - val_loss: 1.3133\n",
            "Epoch 81/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9255 - loss: 0.1884 - val_accuracy: 0.6493 - val_loss: 1.3508\n",
            "Epoch 82/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9332 - loss: 0.1857 - val_accuracy: 0.6493 - val_loss: 1.3421\n",
            "Epoch 83/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9534 - loss: 0.1492 - val_accuracy: 0.6667 - val_loss: 1.3407\n",
            "Epoch 84/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9348 - loss: 0.1883 - val_accuracy: 0.6632 - val_loss: 1.3380\n",
            "Epoch 85/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9403 - loss: 0.1632 - val_accuracy: 0.6701 - val_loss: 1.3514\n",
            "Epoch 86/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9358 - loss: 0.1891 - val_accuracy: 0.6701 - val_loss: 1.3373\n",
            "Epoch 87/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9426 - loss: 0.1771 - val_accuracy: 0.6632 - val_loss: 1.3126\n",
            "Epoch 88/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9325 - loss: 0.1804 - val_accuracy: 0.6528 - val_loss: 1.3010\n",
            "Epoch 89/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9331 - loss: 0.2002 - val_accuracy: 0.6632 - val_loss: 1.3017\n",
            "Epoch 90/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9531 - loss: 0.1664 - val_accuracy: 0.6493 - val_loss: 1.3027\n",
            "Epoch 91/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9407 - loss: 0.1745 - val_accuracy: 0.6562 - val_loss: 1.3024\n",
            "Epoch 92/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9519 - loss: 0.1616 - val_accuracy: 0.6632 - val_loss: 1.3054\n",
            "Epoch 93/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9416 - loss: 0.1741 - val_accuracy: 0.6354 - val_loss: 1.3511\n",
            "Epoch 94/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9480 - loss: 0.1784 - val_accuracy: 0.6285 - val_loss: 1.3737\n",
            "Epoch 95/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9335 - loss: 0.1946 - val_accuracy: 0.6424 - val_loss: 1.3644\n",
            "Epoch 96/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9298 - loss: 0.2033 - val_accuracy: 0.6493 - val_loss: 1.3699\n",
            "Epoch 97/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9247 - loss: 0.2169 - val_accuracy: 0.6632 - val_loss: 1.3589\n",
            "Epoch 98/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9542 - loss: 0.1710 - val_accuracy: 0.6528 - val_loss: 1.3314\n",
            "Epoch 99/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9405 - loss: 0.1666 - val_accuracy: 0.6458 - val_loss: 1.3461\n",
            "Epoch 100/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9416 - loss: 0.1663 - val_accuracy: 0.6493 - val_loss: 1.3821\n",
            "Epoch 101/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9111 - loss: 0.2613 - val_accuracy: 0.6458 - val_loss: 1.3551\n",
            "Epoch 102/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9089 - loss: 0.2512 - val_accuracy: 0.6667 - val_loss: 1.3522\n",
            "Epoch 103/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9405 - loss: 0.1784 - val_accuracy: 0.6528 - val_loss: 1.3656\n",
            "Epoch 104/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9151 - loss: 0.2165 - val_accuracy: 0.6528 - val_loss: 1.3743\n",
            "Epoch 105/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9293 - loss: 0.1882 - val_accuracy: 0.6597 - val_loss: 1.3975\n",
            "Epoch 106/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9357 - loss: 0.2206 - val_accuracy: 0.6562 - val_loss: 1.4094\n",
            "Epoch 107/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9299 - loss: 0.1779 - val_accuracy: 0.6424 - val_loss: 1.3946\n",
            "Epoch 108/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9429 - loss: 0.1619 - val_accuracy: 0.6424 - val_loss: 1.3662\n",
            "Epoch 109/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9482 - loss: 0.1552 - val_accuracy: 0.6597 - val_loss: 1.3896\n",
            "Epoch 110/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9438 - loss: 0.1956 - val_accuracy: 0.6562 - val_loss: 1.4020\n",
            "Epoch 111/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9294 - loss: 0.2005 - val_accuracy: 0.6597 - val_loss: 1.3889\n",
            "Epoch 112/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9482 - loss: 0.1759 - val_accuracy: 0.6319 - val_loss: 1.3926\n",
            "Epoch 113/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9339 - loss: 0.1964 - val_accuracy: 0.6319 - val_loss: 1.4295\n",
            "Epoch 114/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9403 - loss: 0.1911 - val_accuracy: 0.6146 - val_loss: 1.4328\n",
            "Epoch 115/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9424 - loss: 0.1733 - val_accuracy: 0.6215 - val_loss: 1.4196\n",
            "Epoch 116/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9269 - loss: 0.2064 - val_accuracy: 0.6215 - val_loss: 1.4089\n",
            "Epoch 117/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9351 - loss: 0.1971 - val_accuracy: 0.6181 - val_loss: 1.4156\n",
            "Epoch 118/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9309 - loss: 0.2177 - val_accuracy: 0.6389 - val_loss: 1.4016\n",
            "Epoch 119/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9425 - loss: 0.1660 - val_accuracy: 0.6319 - val_loss: 1.4042\n",
            "Epoch 120/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9121 - loss: 0.2346 - val_accuracy: 0.6076 - val_loss: 1.4405\n",
            "Epoch 121/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9454 - loss: 0.1775 - val_accuracy: 0.6250 - val_loss: 1.4229\n",
            "Epoch 122/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9367 - loss: 0.1990 - val_accuracy: 0.6354 - val_loss: 1.4071\n",
            "Epoch 123/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9404 - loss: 0.1620 - val_accuracy: 0.6458 - val_loss: 1.3897\n",
            "Epoch 124/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9424 - loss: 0.1681 - val_accuracy: 0.6354 - val_loss: 1.3961\n",
            "Epoch 125/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9172 - loss: 0.2216 - val_accuracy: 0.6424 - val_loss: 1.3880\n",
            "Epoch 126/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9505 - loss: 0.1588 - val_accuracy: 0.6493 - val_loss: 1.3823\n",
            "Epoch 127/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9449 - loss: 0.1833 - val_accuracy: 0.6458 - val_loss: 1.3674\n",
            "Epoch 128/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9449 - loss: 0.1558 - val_accuracy: 0.6389 - val_loss: 1.3782\n",
            "Epoch 129/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9349 - loss: 0.1786 - val_accuracy: 0.6458 - val_loss: 1.3731\n",
            "Epoch 130/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9364 - loss: 0.1828 - val_accuracy: 0.6597 - val_loss: 1.3758\n",
            "Epoch 131/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9414 - loss: 0.1667 - val_accuracy: 0.6701 - val_loss: 1.3429\n",
            "Epoch 132/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9481 - loss: 0.1757 - val_accuracy: 0.6701 - val_loss: 1.3267\n",
            "Epoch 133/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9450 - loss: 0.1690 - val_accuracy: 0.6597 - val_loss: 1.3391\n",
            "Epoch 134/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9211 - loss: 0.1931 - val_accuracy: 0.6424 - val_loss: 1.3951\n",
            "Epoch 135/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9298 - loss: 0.1823 - val_accuracy: 0.6389 - val_loss: 1.3918\n",
            "Epoch 136/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9486 - loss: 0.1438 - val_accuracy: 0.6528 - val_loss: 1.3970\n",
            "Epoch 137/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9476 - loss: 0.1870 - val_accuracy: 0.6493 - val_loss: 1.4229\n",
            "Epoch 138/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9341 - loss: 0.1927 - val_accuracy: 0.6285 - val_loss: 1.4466\n",
            "Epoch 139/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9273 - loss: 0.2046 - val_accuracy: 0.6424 - val_loss: 1.4006\n",
            "Epoch 140/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9425 - loss: 0.1698 - val_accuracy: 0.6389 - val_loss: 1.3577\n",
            "Epoch 141/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9390 - loss: 0.1820 - val_accuracy: 0.6424 - val_loss: 1.3752\n",
            "Epoch 142/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9271 - loss: 0.2049 - val_accuracy: 0.6458 - val_loss: 1.3840\n",
            "Epoch 143/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9422 - loss: 0.1839 - val_accuracy: 0.6458 - val_loss: 1.4075\n",
            "Epoch 144/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9164 - loss: 0.2211 - val_accuracy: 0.6285 - val_loss: 1.4310\n",
            "Epoch 145/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9362 - loss: 0.1681 - val_accuracy: 0.6250 - val_loss: 1.4004\n",
            "Epoch 146/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9376 - loss: 0.1734 - val_accuracy: 0.6458 - val_loss: 1.4060\n",
            "Epoch 147/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9403 - loss: 0.1775 - val_accuracy: 0.6389 - val_loss: 1.4107\n",
            "Epoch 148/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9363 - loss: 0.1956 - val_accuracy: 0.6389 - val_loss: 1.4221\n",
            "Epoch 149/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9392 - loss: 0.1665 - val_accuracy: 0.6285 - val_loss: 1.4155\n",
            "Epoch 150/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9235 - loss: 0.2138 - val_accuracy: 0.6250 - val_loss: 1.4442\n",
            "Epoch 151/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9530 - loss: 0.1270 - val_accuracy: 0.6319 - val_loss: 1.4351\n",
            "Epoch 152/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9509 - loss: 0.1548 - val_accuracy: 0.6215 - val_loss: 1.4116\n",
            "Epoch 153/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9300 - loss: 0.1908 - val_accuracy: 0.6354 - val_loss: 1.3988\n",
            "Epoch 154/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9363 - loss: 0.1874 - val_accuracy: 0.6319 - val_loss: 1.4198\n",
            "Epoch 155/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9243 - loss: 0.2053 - val_accuracy: 0.6111 - val_loss: 1.4418\n",
            "Epoch 156/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9422 - loss: 0.1714 - val_accuracy: 0.6250 - val_loss: 1.4292\n",
            "Epoch 157/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9465 - loss: 0.1658 - val_accuracy: 0.6389 - val_loss: 1.4400\n",
            "Epoch 158/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9445 - loss: 0.1704 - val_accuracy: 0.6181 - val_loss: 1.4724\n",
            "Epoch 159/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9520 - loss: 0.1556 - val_accuracy: 0.6285 - val_loss: 1.4520\n",
            "Epoch 160/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9570 - loss: 0.1395 - val_accuracy: 0.6215 - val_loss: 1.4722\n",
            "Epoch 161/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9193 - loss: 0.2253 - val_accuracy: 0.6424 - val_loss: 1.4564\n",
            "Epoch 162/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9235 - loss: 0.2241 - val_accuracy: 0.6389 - val_loss: 1.4555\n",
            "Epoch 163/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9543 - loss: 0.1563 - val_accuracy: 0.6354 - val_loss: 1.4224\n",
            "Epoch 164/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9418 - loss: 0.1681 - val_accuracy: 0.6319 - val_loss: 1.3976\n",
            "Epoch 165/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9406 - loss: 0.1751 - val_accuracy: 0.6389 - val_loss: 1.3896\n",
            "Epoch 166/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9289 - loss: 0.1826 - val_accuracy: 0.6319 - val_loss: 1.3948\n",
            "Epoch 167/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9328 - loss: 0.1863 - val_accuracy: 0.6215 - val_loss: 1.4343\n",
            "Epoch 168/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9420 - loss: 0.1616 - val_accuracy: 0.6146 - val_loss: 1.4677\n",
            "Epoch 169/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9252 - loss: 0.2087 - val_accuracy: 0.6250 - val_loss: 1.5095\n",
            "Epoch 170/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9467 - loss: 0.1661 - val_accuracy: 0.6111 - val_loss: 1.4978\n",
            "Epoch 171/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9290 - loss: 0.1882 - val_accuracy: 0.6250 - val_loss: 1.4785\n",
            "Epoch 172/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9478 - loss: 0.1950 - val_accuracy: 0.6181 - val_loss: 1.4697\n",
            "Epoch 173/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9376 - loss: 0.2181 - val_accuracy: 0.6285 - val_loss: 1.4445\n",
            "Epoch 174/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9393 - loss: 0.1729 - val_accuracy: 0.6285 - val_loss: 1.4367\n",
            "Epoch 175/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9394 - loss: 0.1869 - val_accuracy: 0.6354 - val_loss: 1.4340\n",
            "Epoch 176/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9214 - loss: 0.2055 - val_accuracy: 0.6458 - val_loss: 1.4393\n",
            "Epoch 177/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9589 - loss: 0.1345 - val_accuracy: 0.6354 - val_loss: 1.4718\n",
            "Epoch 178/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9384 - loss: 0.1884 - val_accuracy: 0.6528 - val_loss: 1.4573\n",
            "Epoch 179/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9340 - loss: 0.1741 - val_accuracy: 0.6528 - val_loss: 1.4069\n",
            "Epoch 180/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9417 - loss: 0.1751 - val_accuracy: 0.6493 - val_loss: 1.4156\n",
            "Epoch 181/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9361 - loss: 0.1923 - val_accuracy: 0.6319 - val_loss: 1.4261\n",
            "Epoch 182/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9341 - loss: 0.1668 - val_accuracy: 0.6528 - val_loss: 1.4047\n",
            "Epoch 183/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9386 - loss: 0.1748 - val_accuracy: 0.6597 - val_loss: 1.3571\n",
            "Epoch 184/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9459 - loss: 0.1754 - val_accuracy: 0.6528 - val_loss: 1.3613\n",
            "Epoch 185/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9458 - loss: 0.1514 - val_accuracy: 0.6424 - val_loss: 1.4041\n",
            "Epoch 186/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9261 - loss: 0.2027 - val_accuracy: 0.6424 - val_loss: 1.4066\n",
            "Epoch 187/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9231 - loss: 0.2023 - val_accuracy: 0.6458 - val_loss: 1.3974\n",
            "Epoch 188/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9397 - loss: 0.1680 - val_accuracy: 0.6458 - val_loss: 1.4328\n",
            "Epoch 189/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9144 - loss: 0.2459 - val_accuracy: 0.6250 - val_loss: 1.4776\n",
            "Epoch 190/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9414 - loss: 0.1858 - val_accuracy: 0.6458 - val_loss: 1.3786\n",
            "Epoch 191/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9259 - loss: 0.2367 - val_accuracy: 0.6458 - val_loss: 1.3428\n",
            "Epoch 192/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9480 - loss: 0.1473 - val_accuracy: 0.6424 - val_loss: 1.3383\n",
            "Epoch 193/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9442 - loss: 0.1543 - val_accuracy: 0.6424 - val_loss: 1.3537\n",
            "Epoch 194/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9496 - loss: 0.1564 - val_accuracy: 0.6319 - val_loss: 1.3760\n",
            "Epoch 195/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9459 - loss: 0.1488 - val_accuracy: 0.6354 - val_loss: 1.3833\n",
            "Epoch 196/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9479 - loss: 0.1610 - val_accuracy: 0.6250 - val_loss: 1.4241\n",
            "Epoch 197/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9407 - loss: 0.1645 - val_accuracy: 0.6181 - val_loss: 1.4445\n",
            "Epoch 198/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9391 - loss: 0.1762 - val_accuracy: 0.6215 - val_loss: 1.4379\n",
            "Epoch 199/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9419 - loss: 0.1670 - val_accuracy: 0.6181 - val_loss: 1.4202\n",
            "Epoch 200/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9331 - loss: 0.1896 - val_accuracy: 0.6250 - val_loss: 1.4353\n",
            "Epoch 201/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9366 - loss: 0.2014 - val_accuracy: 0.6319 - val_loss: 1.3996\n",
            "Epoch 202/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9485 - loss: 0.1723 - val_accuracy: 0.6458 - val_loss: 1.3982\n",
            "Epoch 203/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9239 - loss: 0.2245 - val_accuracy: 0.6493 - val_loss: 1.4020\n",
            "Epoch 204/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9355 - loss: 0.1742 - val_accuracy: 0.6493 - val_loss: 1.4061\n",
            "Epoch 205/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9389 - loss: 0.1806 - val_accuracy: 0.6389 - val_loss: 1.4221\n",
            "Epoch 206/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9315 - loss: 0.2058 - val_accuracy: 0.6424 - val_loss: 1.4054\n",
            "Epoch 207/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9240 - loss: 0.2103 - val_accuracy: 0.6424 - val_loss: 1.4224\n",
            "Epoch 208/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9247 - loss: 0.2470 - val_accuracy: 0.6424 - val_loss: 1.3850\n",
            "Epoch 209/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9336 - loss: 0.1788 - val_accuracy: 0.6632 - val_loss: 1.3358\n",
            "Epoch 210/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9394 - loss: 0.1538 - val_accuracy: 0.6597 - val_loss: 1.3544\n",
            "Epoch 211/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9466 - loss: 0.1592 - val_accuracy: 0.6528 - val_loss: 1.3663\n",
            "Epoch 212/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9538 - loss: 0.1288 - val_accuracy: 0.6458 - val_loss: 1.3842\n",
            "Epoch 213/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9533 - loss: 0.1579 - val_accuracy: 0.6528 - val_loss: 1.3563\n",
            "Epoch 214/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9452 - loss: 0.1587 - val_accuracy: 0.6667 - val_loss: 1.3634\n",
            "Epoch 215/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9344 - loss: 0.2124 - val_accuracy: 0.6562 - val_loss: 1.3621\n",
            "Epoch 216/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9285 - loss: 0.2137 - val_accuracy: 0.6389 - val_loss: 1.3946\n",
            "Epoch 217/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9401 - loss: 0.1813 - val_accuracy: 0.6424 - val_loss: 1.4369\n",
            "Epoch 218/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9477 - loss: 0.1357 - val_accuracy: 0.6354 - val_loss: 1.4306\n",
            "Epoch 219/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9426 - loss: 0.1624 - val_accuracy: 0.6250 - val_loss: 1.4370\n",
            "Epoch 220/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9593 - loss: 0.1401 - val_accuracy: 0.6215 - val_loss: 1.4630\n",
            "Epoch 221/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9417 - loss: 0.1920 - val_accuracy: 0.6493 - val_loss: 1.4174\n",
            "Epoch 222/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9492 - loss: 0.1557 - val_accuracy: 0.6424 - val_loss: 1.3962\n",
            "Epoch 223/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9581 - loss: 0.1367 - val_accuracy: 0.6458 - val_loss: 1.4105\n",
            "Epoch 224/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9367 - loss: 0.1906 - val_accuracy: 0.6597 - val_loss: 1.3943\n",
            "Epoch 225/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9397 - loss: 0.1878 - val_accuracy: 0.6493 - val_loss: 1.3609\n",
            "Epoch 226/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9464 - loss: 0.1641 - val_accuracy: 0.6424 - val_loss: 1.3803\n",
            "Epoch 227/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9287 - loss: 0.2346 - val_accuracy: 0.6562 - val_loss: 1.3897\n",
            "Epoch 228/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9452 - loss: 0.1559 - val_accuracy: 0.6285 - val_loss: 1.4327\n",
            "Epoch 229/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9369 - loss: 0.1778 - val_accuracy: 0.6285 - val_loss: 1.4313\n",
            "Epoch 230/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9548 - loss: 0.1599 - val_accuracy: 0.6354 - val_loss: 1.4151\n",
            "Epoch 231/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9387 - loss: 0.1940 - val_accuracy: 0.6389 - val_loss: 1.4142\n",
            "Epoch 232/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9565 - loss: 0.1533 - val_accuracy: 0.6597 - val_loss: 1.3816\n",
            "Epoch 233/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9422 - loss: 0.1757 - val_accuracy: 0.6424 - val_loss: 1.3924\n",
            "Epoch 234/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9527 - loss: 0.1458 - val_accuracy: 0.6493 - val_loss: 1.4131\n",
            "Epoch 235/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9406 - loss: 0.1649 - val_accuracy: 0.6424 - val_loss: 1.4213\n",
            "Epoch 236/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9236 - loss: 0.2003 - val_accuracy: 0.6667 - val_loss: 1.4175\n",
            "Epoch 237/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9252 - loss: 0.2181 - val_accuracy: 0.6528 - val_loss: 1.4559\n",
            "Epoch 238/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9331 - loss: 0.2029 - val_accuracy: 0.6562 - val_loss: 1.4139\n",
            "Epoch 239/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9563 - loss: 0.1628 - val_accuracy: 0.6424 - val_loss: 1.3992\n",
            "Epoch 240/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9218 - loss: 0.1902 - val_accuracy: 0.6632 - val_loss: 1.3730\n",
            "Epoch 241/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9528 - loss: 0.1561 - val_accuracy: 0.6736 - val_loss: 1.3646\n",
            "Epoch 242/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9463 - loss: 0.1604 - val_accuracy: 0.6528 - val_loss: 1.3972\n",
            "Epoch 243/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9458 - loss: 0.1589 - val_accuracy: 0.6319 - val_loss: 1.3809\n",
            "Epoch 244/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9328 - loss: 0.1668 - val_accuracy: 0.6354 - val_loss: 1.3743\n",
            "Epoch 245/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9444 - loss: 0.1604 - val_accuracy: 0.6285 - val_loss: 1.3885\n",
            "Epoch 246/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9365 - loss: 0.1753 - val_accuracy: 0.6354 - val_loss: 1.3984\n",
            "Epoch 247/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9320 - loss: 0.2177 - val_accuracy: 0.6354 - val_loss: 1.4278\n",
            "Epoch 248/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9335 - loss: 0.1833 - val_accuracy: 0.6354 - val_loss: 1.4328\n",
            "Epoch 249/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9409 - loss: 0.1906 - val_accuracy: 0.6389 - val_loss: 1.4418\n",
            "Epoch 250/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9337 - loss: 0.1912 - val_accuracy: 0.6354 - val_loss: 1.4395\n",
            "Epoch 251/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9446 - loss: 0.1588 - val_accuracy: 0.6319 - val_loss: 1.4495\n",
            "Epoch 252/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9348 - loss: 0.2069 - val_accuracy: 0.6285 - val_loss: 1.4360\n",
            "Epoch 253/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9634 - loss: 0.1351 - val_accuracy: 0.6424 - val_loss: 1.4344\n",
            "Epoch 254/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9468 - loss: 0.1725 - val_accuracy: 0.6424 - val_loss: 1.4407\n",
            "Epoch 255/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9548 - loss: 0.1460 - val_accuracy: 0.6458 - val_loss: 1.4594\n",
            "Epoch 256/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9242 - loss: 0.2376 - val_accuracy: 0.6389 - val_loss: 1.4798\n",
            "Epoch 257/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9407 - loss: 0.1773 - val_accuracy: 0.6181 - val_loss: 1.4523\n",
            "Epoch 258/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9344 - loss: 0.1921 - val_accuracy: 0.6285 - val_loss: 1.4229\n",
            "Epoch 259/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9299 - loss: 0.1897 - val_accuracy: 0.6389 - val_loss: 1.4388\n",
            "Epoch 260/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9339 - loss: 0.1764 - val_accuracy: 0.6250 - val_loss: 1.4724\n",
            "Epoch 261/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9267 - loss: 0.2412 - val_accuracy: 0.6389 - val_loss: 1.4914\n",
            "Epoch 262/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9444 - loss: 0.1793 - val_accuracy: 0.6389 - val_loss: 1.4654\n",
            "Epoch 263/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9292 - loss: 0.1950 - val_accuracy: 0.6493 - val_loss: 1.3971\n",
            "Epoch 264/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9485 - loss: 0.1613 - val_accuracy: 0.6424 - val_loss: 1.3919\n",
            "Epoch 265/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9534 - loss: 0.1645 - val_accuracy: 0.6458 - val_loss: 1.3547\n",
            "Epoch 266/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9253 - loss: 0.2432 - val_accuracy: 0.6493 - val_loss: 1.3760\n",
            "Epoch 267/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9451 - loss: 0.1707 - val_accuracy: 0.6424 - val_loss: 1.3929\n",
            "Epoch 268/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9474 - loss: 0.1695 - val_accuracy: 0.6389 - val_loss: 1.3924\n",
            "Epoch 269/269\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9353 - loss: 0.1864 - val_accuracy: 0.6424 - val_loss: 1.3832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Define the MLP model parameters\n",
        "num_layers = 5\n",
        "units = [65, 260, 364, 52, 52]\n",
        "activations = ['relu', 'relu', 'relu', 'relu', 'relu']\n",
        "dropouts = [0.1, 0.3, 0.4, 0.1, 0.1]\n",
        "learning_rate = 0.0010343990168720423\n",
        "loss_function = 'sparse_categorical_crossentropy'\n",
        "l2_lambda = 0.01  # L2 regularization parameter\n",
        "input_shape = (13,)  # Adjust based on your input feature size\n",
        "\n",
        "# Build the MLP model\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Input layer\n",
        "model.add(layers.Input(shape=input_shape))\n",
        "\n",
        "# Hidden layers with batch normalization, dropout, and L2 regularization\n",
        "for i in range(num_layers):\n",
        "    model.add(layers.Dense(units[i],\n",
        "                           kernel_regularizer=l2(l2_lambda)))  # L2 regularization added\n",
        "    model.add(layers.BatchNormalization())  # Batch normalization\n",
        "    model.add(layers.Activation(activations[i]))  # Activation\n",
        "    model.add(layers.Dropout(dropouts[i]))  # Dropout\n",
        "\n",
        "# Output layer (adjust the number of units to your number of classes)\n",
        "model.add(layers.Dense(8, activation='softmax'))  # Assuming 8 classes for output\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=loss_function,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        },
        "id": "qyZDfx4uCRRB",
        "outputId": "000dbfb0-8209-4eba-8155-d25329a6f2a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │             \u001b[38;5;34m910\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_17               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │             \u001b[38;5;34m260\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_10 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │          \u001b[38;5;34m17,160\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_18               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │           \u001b[38;5;34m1,040\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_11 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_23 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │          \u001b[38;5;34m95,004\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_19               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │           \u001b[38;5;34m1,456\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_12 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │          \u001b[38;5;34m18,980\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_20               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │             \u001b[38;5;34m208\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_13 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │           \u001b[38;5;34m2,756\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_21               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │             \u001b[38;5;34m208\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_14 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │             \u001b[38;5;34m424\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">910</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_17               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,160</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_18               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">95,004</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_19               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,456</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,980</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_20               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,756</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_21               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">424</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m138,406\u001b[0m (540.65 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">138,406</span> (540.65 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m136,820\u001b[0m (534.45 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">136,820</span> (534.45 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,586\u001b[0m (6.20 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,586</span> (6.20 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=200,batch_size=64, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNpE48ojCjVC",
        "outputId": "6694f147-fd11-41c5-d35b-a0a5ec53dc7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 115ms/step - accuracy: 0.1282 - loss: 7.9778 - val_accuracy: 0.2639 - val_loss: 7.3863\n",
            "Epoch 2/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.2038 - loss: 7.2871 - val_accuracy: 0.2951 - val_loss: 6.9897\n",
            "Epoch 3/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2790 - loss: 6.7785 - val_accuracy: 0.3021 - val_loss: 6.6031\n",
            "Epoch 4/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2934 - loss: 6.3301 - val_accuracy: 0.3403 - val_loss: 6.2198\n",
            "Epoch 5/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3187 - loss: 5.9154 - val_accuracy: 0.3194 - val_loss: 5.8772\n",
            "Epoch 6/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3751 - loss: 5.4824 - val_accuracy: 0.2535 - val_loss: 5.5477\n",
            "Epoch 7/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3738 - loss: 5.1793 - val_accuracy: 0.2569 - val_loss: 5.2429\n",
            "Epoch 8/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3890 - loss: 4.8092 - val_accuracy: 0.2500 - val_loss: 4.9673\n",
            "Epoch 9/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4348 - loss: 4.5055 - val_accuracy: 0.2431 - val_loss: 4.7127\n",
            "Epoch 10/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4115 - loss: 4.2678 - val_accuracy: 0.2326 - val_loss: 4.4757\n",
            "Epoch 11/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4610 - loss: 3.9538 - val_accuracy: 0.2639 - val_loss: 4.2576\n",
            "Epoch 12/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4553 - loss: 3.7451 - val_accuracy: 0.3021 - val_loss: 4.0438\n",
            "Epoch 13/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4667 - loss: 3.5981 - val_accuracy: 0.3090 - val_loss: 3.8520\n",
            "Epoch 14/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4774 - loss: 3.4287 - val_accuracy: 0.3333 - val_loss: 3.6843\n",
            "Epoch 15/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5124 - loss: 3.1774 - val_accuracy: 0.3438 - val_loss: 3.5074\n",
            "Epoch 16/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4784 - loss: 3.0843 - val_accuracy: 0.3681 - val_loss: 3.3594\n",
            "Epoch 17/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4654 - loss: 3.0042 - val_accuracy: 0.3785 - val_loss: 3.2171\n",
            "Epoch 18/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5114 - loss: 2.7871 - val_accuracy: 0.3924 - val_loss: 3.1034\n",
            "Epoch 19/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5400 - loss: 2.6808 - val_accuracy: 0.3854 - val_loss: 2.9988\n",
            "Epoch 20/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5492 - loss: 2.5537 - val_accuracy: 0.4410 - val_loss: 2.8805\n",
            "Epoch 21/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5425 - loss: 2.4603 - val_accuracy: 0.4062 - val_loss: 2.8254\n",
            "Epoch 22/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5761 - loss: 2.3666 - val_accuracy: 0.4201 - val_loss: 2.7349\n",
            "Epoch 23/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5253 - loss: 2.3795 - val_accuracy: 0.4167 - val_loss: 2.6843\n",
            "Epoch 24/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5631 - loss: 2.2559 - val_accuracy: 0.4757 - val_loss: 2.5248\n",
            "Epoch 25/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5755 - loss: 2.2204 - val_accuracy: 0.4688 - val_loss: 2.4653\n",
            "Epoch 26/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5677 - loss: 2.1500 - val_accuracy: 0.4583 - val_loss: 2.4299\n",
            "Epoch 27/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5912 - loss: 2.0571 - val_accuracy: 0.4965 - val_loss: 2.3492\n",
            "Epoch 28/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5744 - loss: 2.0795 - val_accuracy: 0.5347 - val_loss: 2.2462\n",
            "Epoch 29/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6077 - loss: 1.9902 - val_accuracy: 0.5382 - val_loss: 2.2024\n",
            "Epoch 30/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5796 - loss: 1.9535 - val_accuracy: 0.4826 - val_loss: 2.2331\n",
            "Epoch 31/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5850 - loss: 1.9183 - val_accuracy: 0.5208 - val_loss: 2.1309\n",
            "Epoch 32/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5804 - loss: 1.9265 - val_accuracy: 0.5347 - val_loss: 2.0858\n",
            "Epoch 33/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5865 - loss: 1.8837 - val_accuracy: 0.4410 - val_loss: 2.1749\n",
            "Epoch 34/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6248 - loss: 1.7977 - val_accuracy: 0.5417 - val_loss: 2.0804\n",
            "Epoch 35/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6204 - loss: 1.7455 - val_accuracy: 0.5347 - val_loss: 1.9785\n",
            "Epoch 36/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6079 - loss: 1.7803 - val_accuracy: 0.5521 - val_loss: 1.9966\n",
            "Epoch 37/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5974 - loss: 1.7988 - val_accuracy: 0.5347 - val_loss: 1.9686\n",
            "Epoch 38/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6290 - loss: 1.6807 - val_accuracy: 0.5625 - val_loss: 1.9385\n",
            "Epoch 39/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6304 - loss: 1.6892 - val_accuracy: 0.5764 - val_loss: 1.8972\n",
            "Epoch 40/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6435 - loss: 1.6624 - val_accuracy: 0.5556 - val_loss: 1.8997\n",
            "Epoch 41/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6416 - loss: 1.6651 - val_accuracy: 0.6111 - val_loss: 1.8287\n",
            "Epoch 42/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6364 - loss: 1.6755 - val_accuracy: 0.5556 - val_loss: 1.8199\n",
            "Epoch 43/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6212 - loss: 1.6392 - val_accuracy: 0.5938 - val_loss: 1.7890\n",
            "Epoch 44/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6572 - loss: 1.5754 - val_accuracy: 0.5590 - val_loss: 1.8445\n",
            "Epoch 45/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6256 - loss: 1.6333 - val_accuracy: 0.6111 - val_loss: 1.7688\n",
            "Epoch 46/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6264 - loss: 1.5966 - val_accuracy: 0.5694 - val_loss: 1.7963\n",
            "Epoch 47/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6679 - loss: 1.6030 - val_accuracy: 0.5833 - val_loss: 1.7969\n",
            "Epoch 48/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6382 - loss: 1.5460 - val_accuracy: 0.5556 - val_loss: 1.7799\n",
            "Epoch 49/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6623 - loss: 1.5208 - val_accuracy: 0.5590 - val_loss: 1.7833\n",
            "Epoch 50/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6599 - loss: 1.4792 - val_accuracy: 0.5694 - val_loss: 1.7797\n",
            "Epoch 51/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6732 - loss: 1.4824 - val_accuracy: 0.5590 - val_loss: 1.7883\n",
            "Epoch 52/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6861 - loss: 1.4581 - val_accuracy: 0.5694 - val_loss: 1.7433\n",
            "Epoch 53/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6705 - loss: 1.4692 - val_accuracy: 0.5417 - val_loss: 1.7354\n",
            "Epoch 54/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6834 - loss: 1.4811 - val_accuracy: 0.5590 - val_loss: 1.7198\n",
            "Epoch 55/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6681 - loss: 1.4556 - val_accuracy: 0.5799 - val_loss: 1.7054\n",
            "Epoch 56/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6618 - loss: 1.4915 - val_accuracy: 0.5660 - val_loss: 1.7404\n",
            "Epoch 57/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6784 - loss: 1.4462 - val_accuracy: 0.6250 - val_loss: 1.6728\n",
            "Epoch 58/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6552 - loss: 1.5122 - val_accuracy: 0.6007 - val_loss: 1.6920\n",
            "Epoch 59/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6656 - loss: 1.4621 - val_accuracy: 0.5938 - val_loss: 1.6497\n",
            "Epoch 60/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6470 - loss: 1.4876 - val_accuracy: 0.5972 - val_loss: 1.7023\n",
            "Epoch 61/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6871 - loss: 1.4188 - val_accuracy: 0.5938 - val_loss: 1.7176\n",
            "Epoch 62/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6633 - loss: 1.4714 - val_accuracy: 0.6007 - val_loss: 1.6615\n",
            "Epoch 63/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6949 - loss: 1.4165 - val_accuracy: 0.6111 - val_loss: 1.6365\n",
            "Epoch 64/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6884 - loss: 1.4013 - val_accuracy: 0.5868 - val_loss: 1.7104\n",
            "Epoch 65/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6724 - loss: 1.4123 - val_accuracy: 0.5972 - val_loss: 1.6653\n",
            "Epoch 66/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6748 - loss: 1.4111 - val_accuracy: 0.6042 - val_loss: 1.6584\n",
            "Epoch 67/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6822 - loss: 1.4373 - val_accuracy: 0.6215 - val_loss: 1.6563\n",
            "Epoch 68/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6920 - loss: 1.3716 - val_accuracy: 0.6389 - val_loss: 1.6019\n",
            "Epoch 69/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7049 - loss: 1.3577 - val_accuracy: 0.5799 - val_loss: 1.6540\n",
            "Epoch 70/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6583 - loss: 1.3990 - val_accuracy: 0.6007 - val_loss: 1.6196\n",
            "Epoch 71/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7046 - loss: 1.3621 - val_accuracy: 0.6146 - val_loss: 1.5891\n",
            "Epoch 72/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7373 - loss: 1.2907 - val_accuracy: 0.6354 - val_loss: 1.6306\n",
            "Epoch 73/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6707 - loss: 1.3965 - val_accuracy: 0.6007 - val_loss: 1.6598\n",
            "Epoch 74/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6825 - loss: 1.3921 - val_accuracy: 0.5972 - val_loss: 1.6282\n",
            "Epoch 75/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6694 - loss: 1.4610 - val_accuracy: 0.6111 - val_loss: 1.6271\n",
            "Epoch 76/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6878 - loss: 1.3804 - val_accuracy: 0.6042 - val_loss: 1.6172\n",
            "Epoch 77/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7298 - loss: 1.2777 - val_accuracy: 0.5972 - val_loss: 1.6133\n",
            "Epoch 78/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6939 - loss: 1.3337 - val_accuracy: 0.5868 - val_loss: 1.6780\n",
            "Epoch 79/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6824 - loss: 1.4114 - val_accuracy: 0.6111 - val_loss: 1.6479\n",
            "Epoch 80/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7237 - loss: 1.3421 - val_accuracy: 0.6181 - val_loss: 1.6133\n",
            "Epoch 81/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7135 - loss: 1.3239 - val_accuracy: 0.6424 - val_loss: 1.5711\n",
            "Epoch 82/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6874 - loss: 1.3222 - val_accuracy: 0.5764 - val_loss: 1.6547\n",
            "Epoch 83/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6977 - loss: 1.3623 - val_accuracy: 0.6319 - val_loss: 1.5737\n",
            "Epoch 84/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7426 - loss: 1.2611 - val_accuracy: 0.5556 - val_loss: 1.6603\n",
            "Epoch 85/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7091 - loss: 1.3132 - val_accuracy: 0.6007 - val_loss: 1.6253\n",
            "Epoch 86/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7027 - loss: 1.3543 - val_accuracy: 0.6076 - val_loss: 1.5674\n",
            "Epoch 87/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7146 - loss: 1.2938 - val_accuracy: 0.5938 - val_loss: 1.6239\n",
            "Epoch 88/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7074 - loss: 1.3146 - val_accuracy: 0.6181 - val_loss: 1.6277\n",
            "Epoch 89/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7173 - loss: 1.3027 - val_accuracy: 0.6285 - val_loss: 1.5847\n",
            "Epoch 90/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6979 - loss: 1.3285 - val_accuracy: 0.5972 - val_loss: 1.6702\n",
            "Epoch 91/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7070 - loss: 1.2991 - val_accuracy: 0.6215 - val_loss: 1.6468\n",
            "Epoch 92/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7365 - loss: 1.2772 - val_accuracy: 0.5903 - val_loss: 1.6240\n",
            "Epoch 93/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7393 - loss: 1.2392 - val_accuracy: 0.6319 - val_loss: 1.5720\n",
            "Epoch 94/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7321 - loss: 1.2555 - val_accuracy: 0.6285 - val_loss: 1.6214\n",
            "Epoch 95/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7453 - loss: 1.2622 - val_accuracy: 0.6007 - val_loss: 1.6610\n",
            "Epoch 96/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7267 - loss: 1.2752 - val_accuracy: 0.5972 - val_loss: 1.6061\n",
            "Epoch 97/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7174 - loss: 1.2687 - val_accuracy: 0.6111 - val_loss: 1.5693\n",
            "Epoch 98/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7205 - loss: 1.2588 - val_accuracy: 0.5903 - val_loss: 1.6093\n",
            "Epoch 99/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7653 - loss: 1.2307 - val_accuracy: 0.6285 - val_loss: 1.5482\n",
            "Epoch 100/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7213 - loss: 1.2857 - val_accuracy: 0.6146 - val_loss: 1.5581\n",
            "Epoch 101/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6938 - loss: 1.3688 - val_accuracy: 0.6146 - val_loss: 1.5417\n",
            "Epoch 102/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7258 - loss: 1.2821 - val_accuracy: 0.6146 - val_loss: 1.5471\n",
            "Epoch 103/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7340 - loss: 1.2604 - val_accuracy: 0.6007 - val_loss: 1.5736\n",
            "Epoch 104/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7325 - loss: 1.2879 - val_accuracy: 0.6146 - val_loss: 1.5833\n",
            "Epoch 105/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7130 - loss: 1.2892 - val_accuracy: 0.6181 - val_loss: 1.5183\n",
            "Epoch 106/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7513 - loss: 1.2317 - val_accuracy: 0.6319 - val_loss: 1.5860\n",
            "Epoch 107/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7347 - loss: 1.2245 - val_accuracy: 0.6528 - val_loss: 1.5254\n",
            "Epoch 108/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7395 - loss: 1.2044 - val_accuracy: 0.6250 - val_loss: 1.5374\n",
            "Epoch 109/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7504 - loss: 1.2173 - val_accuracy: 0.6250 - val_loss: 1.5129\n",
            "Epoch 110/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7625 - loss: 1.2025 - val_accuracy: 0.6181 - val_loss: 1.5289\n",
            "Epoch 111/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7172 - loss: 1.2249 - val_accuracy: 0.6250 - val_loss: 1.4965\n",
            "Epoch 112/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7032 - loss: 1.3013 - val_accuracy: 0.6215 - val_loss: 1.5294\n",
            "Epoch 113/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7529 - loss: 1.2001 - val_accuracy: 0.6493 - val_loss: 1.5046\n",
            "Epoch 114/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7203 - loss: 1.2198 - val_accuracy: 0.6250 - val_loss: 1.5504\n",
            "Epoch 115/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7438 - loss: 1.2607 - val_accuracy: 0.6424 - val_loss: 1.4927\n",
            "Epoch 116/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7217 - loss: 1.2236 - val_accuracy: 0.6458 - val_loss: 1.5786\n",
            "Epoch 117/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7267 - loss: 1.2352 - val_accuracy: 0.6181 - val_loss: 1.5349\n",
            "Epoch 118/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7536 - loss: 1.1926 - val_accuracy: 0.6424 - val_loss: 1.4865\n",
            "Epoch 119/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7372 - loss: 1.2013 - val_accuracy: 0.6319 - val_loss: 1.5263\n",
            "Epoch 120/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7392 - loss: 1.2149 - val_accuracy: 0.6597 - val_loss: 1.5428\n",
            "Epoch 121/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7350 - loss: 1.2476 - val_accuracy: 0.6597 - val_loss: 1.4991\n",
            "Epoch 122/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7582 - loss: 1.1600 - val_accuracy: 0.6667 - val_loss: 1.5566\n",
            "Epoch 123/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7417 - loss: 1.2308 - val_accuracy: 0.6319 - val_loss: 1.6353\n",
            "Epoch 124/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7386 - loss: 1.2581 - val_accuracy: 0.6597 - val_loss: 1.5575\n",
            "Epoch 125/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7477 - loss: 1.2051 - val_accuracy: 0.6076 - val_loss: 1.5926\n",
            "Epoch 126/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7608 - loss: 1.1961 - val_accuracy: 0.6215 - val_loss: 1.5828\n",
            "Epoch 127/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7427 - loss: 1.1876 - val_accuracy: 0.6215 - val_loss: 1.6136\n",
            "Epoch 128/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7368 - loss: 1.2463 - val_accuracy: 0.6042 - val_loss: 1.6746\n",
            "Epoch 129/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7428 - loss: 1.2426 - val_accuracy: 0.6111 - val_loss: 1.5949\n",
            "Epoch 130/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7581 - loss: 1.2322 - val_accuracy: 0.6007 - val_loss: 1.5738\n",
            "Epoch 131/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7654 - loss: 1.1753 - val_accuracy: 0.6250 - val_loss: 1.5518\n",
            "Epoch 132/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7426 - loss: 1.2561 - val_accuracy: 0.6285 - val_loss: 1.5945\n",
            "Epoch 133/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7382 - loss: 1.2214 - val_accuracy: 0.6042 - val_loss: 1.5951\n",
            "Epoch 134/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7493 - loss: 1.1636 - val_accuracy: 0.6493 - val_loss: 1.5007\n",
            "Epoch 135/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7361 - loss: 1.2119 - val_accuracy: 0.6215 - val_loss: 1.5557\n",
            "Epoch 136/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7393 - loss: 1.1637 - val_accuracy: 0.6389 - val_loss: 1.5621\n",
            "Epoch 137/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7221 - loss: 1.2048 - val_accuracy: 0.6146 - val_loss: 1.5860\n",
            "Epoch 138/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7252 - loss: 1.2410 - val_accuracy: 0.6146 - val_loss: 1.5651\n",
            "Epoch 139/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7681 - loss: 1.1456 - val_accuracy: 0.6285 - val_loss: 1.5119\n",
            "Epoch 140/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7450 - loss: 1.1928 - val_accuracy: 0.6181 - val_loss: 1.5237\n",
            "Epoch 141/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7507 - loss: 1.1633 - val_accuracy: 0.6389 - val_loss: 1.5812\n",
            "Epoch 142/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7437 - loss: 1.1876 - val_accuracy: 0.6319 - val_loss: 1.5905\n",
            "Epoch 143/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7592 - loss: 1.1964 - val_accuracy: 0.6319 - val_loss: 1.5356\n",
            "Epoch 144/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7727 - loss: 1.1615 - val_accuracy: 0.5938 - val_loss: 1.6829\n",
            "Epoch 145/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7614 - loss: 1.1585 - val_accuracy: 0.6528 - val_loss: 1.5212\n",
            "Epoch 146/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7578 - loss: 1.1821 - val_accuracy: 0.6215 - val_loss: 1.6109\n",
            "Epoch 147/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7487 - loss: 1.1863 - val_accuracy: 0.6215 - val_loss: 1.5626\n",
            "Epoch 148/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7736 - loss: 1.1871 - val_accuracy: 0.6215 - val_loss: 1.5683\n",
            "Epoch 149/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7524 - loss: 1.1796 - val_accuracy: 0.6389 - val_loss: 1.5652\n",
            "Epoch 150/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7504 - loss: 1.2044 - val_accuracy: 0.6562 - val_loss: 1.5557\n",
            "Epoch 151/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7463 - loss: 1.1813 - val_accuracy: 0.6042 - val_loss: 1.5677\n",
            "Epoch 152/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7418 - loss: 1.2206 - val_accuracy: 0.6250 - val_loss: 1.6188\n",
            "Epoch 153/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7604 - loss: 1.1792 - val_accuracy: 0.6250 - val_loss: 1.5527\n",
            "Epoch 154/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8028 - loss: 1.0924 - val_accuracy: 0.6493 - val_loss: 1.5367\n",
            "Epoch 155/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7576 - loss: 1.1513 - val_accuracy: 0.6285 - val_loss: 1.5549\n",
            "Epoch 156/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7815 - loss: 1.1610 - val_accuracy: 0.6146 - val_loss: 1.6168\n",
            "Epoch 157/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7487 - loss: 1.2212 - val_accuracy: 0.6250 - val_loss: 1.5279\n",
            "Epoch 158/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7683 - loss: 1.1539 - val_accuracy: 0.6424 - val_loss: 1.5466\n",
            "Epoch 159/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7518 - loss: 1.1638 - val_accuracy: 0.6319 - val_loss: 1.5167\n",
            "Epoch 160/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7435 - loss: 1.1895 - val_accuracy: 0.6458 - val_loss: 1.4745\n",
            "Epoch 161/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7687 - loss: 1.1663 - val_accuracy: 0.6458 - val_loss: 1.5182\n",
            "Epoch 162/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7518 - loss: 1.1729 - val_accuracy: 0.6389 - val_loss: 1.5179\n",
            "Epoch 163/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7418 - loss: 1.1817 - val_accuracy: 0.6181 - val_loss: 1.5612\n",
            "Epoch 164/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7776 - loss: 1.1287 - val_accuracy: 0.6319 - val_loss: 1.5683\n",
            "Epoch 165/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7663 - loss: 1.1680 - val_accuracy: 0.6181 - val_loss: 1.5564\n",
            "Epoch 166/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7701 - loss: 1.1451 - val_accuracy: 0.6215 - val_loss: 1.5372\n",
            "Epoch 167/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7817 - loss: 1.0865 - val_accuracy: 0.6736 - val_loss: 1.4796\n",
            "Epoch 168/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7597 - loss: 1.1872 - val_accuracy: 0.6111 - val_loss: 1.6464\n",
            "Epoch 169/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7923 - loss: 1.1051 - val_accuracy: 0.6285 - val_loss: 1.5420\n",
            "Epoch 170/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7681 - loss: 1.1590 - val_accuracy: 0.6146 - val_loss: 1.6071\n",
            "Epoch 171/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7690 - loss: 1.1320 - val_accuracy: 0.6215 - val_loss: 1.5377\n",
            "Epoch 172/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7917 - loss: 1.0872 - val_accuracy: 0.6285 - val_loss: 1.5711\n",
            "Epoch 173/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7685 - loss: 1.1311 - val_accuracy: 0.6111 - val_loss: 1.5909\n",
            "Epoch 174/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7602 - loss: 1.1659 - val_accuracy: 0.6354 - val_loss: 1.5108\n",
            "Epoch 175/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7690 - loss: 1.1332 - val_accuracy: 0.5799 - val_loss: 1.6396\n",
            "Epoch 176/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7827 - loss: 1.1567 - val_accuracy: 0.6458 - val_loss: 1.6076\n",
            "Epoch 177/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7636 - loss: 1.1805 - val_accuracy: 0.6285 - val_loss: 1.5649\n",
            "Epoch 178/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7613 - loss: 1.1575 - val_accuracy: 0.6285 - val_loss: 1.5932\n",
            "Epoch 179/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7567 - loss: 1.1517 - val_accuracy: 0.6458 - val_loss: 1.5774\n",
            "Epoch 180/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7779 - loss: 1.1772 - val_accuracy: 0.6215 - val_loss: 1.6081\n",
            "Epoch 181/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7672 - loss: 1.1676 - val_accuracy: 0.6250 - val_loss: 1.6433\n",
            "Epoch 182/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7516 - loss: 1.2184 - val_accuracy: 0.6493 - val_loss: 1.5741\n",
            "Epoch 183/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8083 - loss: 1.1082 - val_accuracy: 0.6250 - val_loss: 1.5796\n",
            "Epoch 184/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7611 - loss: 1.1782 - val_accuracy: 0.6215 - val_loss: 1.5930\n",
            "Epoch 185/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7865 - loss: 1.1069 - val_accuracy: 0.6319 - val_loss: 1.5571\n",
            "Epoch 186/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7885 - loss: 1.1061 - val_accuracy: 0.6007 - val_loss: 1.6515\n",
            "Epoch 187/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7752 - loss: 1.0991 - val_accuracy: 0.6285 - val_loss: 1.5877\n",
            "Epoch 188/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7720 - loss: 1.1260 - val_accuracy: 0.6146 - val_loss: 1.5728\n",
            "Epoch 189/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7488 - loss: 1.1900 - val_accuracy: 0.6250 - val_loss: 1.6121\n",
            "Epoch 190/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7496 - loss: 1.1681 - val_accuracy: 0.6562 - val_loss: 1.5758\n",
            "Epoch 191/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7775 - loss: 1.1495 - val_accuracy: 0.6562 - val_loss: 1.5609\n",
            "Epoch 192/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7647 - loss: 1.1609 - val_accuracy: 0.6562 - val_loss: 1.6388\n",
            "Epoch 193/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7854 - loss: 1.1058 - val_accuracy: 0.6250 - val_loss: 1.5848\n",
            "Epoch 194/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7937 - loss: 1.0677 - val_accuracy: 0.6042 - val_loss: 1.6811\n",
            "Epoch 195/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7905 - loss: 1.1253 - val_accuracy: 0.6285 - val_loss: 1.5936\n",
            "Epoch 196/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7384 - loss: 1.1950 - val_accuracy: 0.6424 - val_loss: 1.5812\n",
            "Epoch 197/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7877 - loss: 1.0851 - val_accuracy: 0.6285 - val_loss: 1.5647\n",
            "Epoch 198/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7569 - loss: 1.1574 - val_accuracy: 0.6111 - val_loss: 1.5331\n",
            "Epoch 199/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7620 - loss: 1.1609 - val_accuracy: 0.6042 - val_loss: 1.6052\n",
            "Epoch 200/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8111 - loss: 1.0798 - val_accuracy: 0.6389 - val_loss: 1.5572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4th"
      ],
      "metadata": {
        "id": "roE90N3PKR9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the HyperModel class:1\n",
        "class MLPHyperModel7(HyperModel):\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "\n",
        "        # Number of hidden layers\n",
        "        for i in range(hp.Int('num_layers', 1, 12)):\n",
        "            model.add(Dense(\n",
        "                units=hp.Int(f'units_{i}', min_value=130, max_value=390, step=13),\n",
        "                activation=hp.Choice(f'activation_{i}', ['relu', 'elu','tanh','softmax']),\n",
        "                input_dim=13 if i == 0 else None\n",
        "            ))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dropout(rate=hp.Float(f'dropout_{i}', 0.1, 0.5, step=0.1)))\n",
        "\n",
        "        # Output layer\n",
        "        model.add(Dense(8, activation='softmax'))\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-1, sampling='log')),\n",
        "            loss=hp.Choice('loss', ['sparse_categorical_crossentropy', 'categorical_crossentropy']),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model\n"
      ],
      "metadata": {
        "id": "dOxmdeYl3S6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tuner\n",
        "tuner = RandomSearch(\n",
        "    MLPHyperModel7(),\n",
        "    objective='val_accuracy',\n",
        "    max_trials=3,\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='mlp_tuning'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RzCIDgK3ceJ",
        "outputId": "08508116-e4b2-4c91-e3f6-763703eb9a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading Tuner from my_dir/mlp_tuning/tuner0.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(x_train, y_train_sparse, epochs=100,batch_size=64, validation_data=(x_test, y_test_sparse))\n",
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ],
      "metadata": {
        "id": "3XuG6H0N3l-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the best hyperparameters\n",
        "best_hyperparameters = tuner.get_best_hyperparameters()[0]"
      ],
      "metadata": {
        "id": "ELxZ4Ok33tiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "for hp_name in best_hyperparameters.values.keys():\n",
        "    print(f\"{hp_name}: {best_hyperparameters.get(hp_name)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQipHSgI5iGg",
        "outputId": "d096e49e-1b07-4989-8ed4-0c118e2d4686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "num_layers: 3\n",
            "units_0: 364\n",
            "activation_0: relu\n",
            "dropout_0: 0.4\n",
            "learning_rate: 0.006019793254027166\n",
            "loss: sparse_categorical_crossentropy\n",
            "units_1: 377\n",
            "activation_1: elu\n",
            "dropout_1: 0.1\n",
            "units_2: 91\n",
            "activation_2: relu\n",
            "dropout_2: 0.1\n",
            "units_3: 221\n",
            "activation_3: relu\n",
            "dropout_3: 0.5\n",
            "units_4: 286\n",
            "activation_4: tanh\n",
            "dropout_4: 0.5\n",
            "units_5: 286\n",
            "activation_5: softmax\n",
            "dropout_5: 0.1\n",
            "units_6: 156\n",
            "activation_6: elu\n",
            "dropout_6: 0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5th"
      ],
      "metadata": {
        "id": "vBOKiKWVKV_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the HyperModel class:1\n",
        "class MLPHyperModel11(HyperModel):\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "\n",
        "        # Number of hidden layers\n",
        "        for i in range(hp.Int('num_layers', 1, 7)):\n",
        "            model.add(Dense(\n",
        "                units=hp.Int(f'units_{i}', min_value=130, max_value=780, step=13),\n",
        "                activation=hp.Choice(f'activation_{i}', ['relu', 'elu','tanh','softmax']),\n",
        "                input_dim=13 if i == 0 else None\n",
        "            ))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dropout(rate=hp.Float(f'dropout_{i}', 0.1, 0.5, step=0.1)))\n",
        "\n",
        "        # Output layer\n",
        "        model.add(Dense(8, activation='softmax'))\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-1, sampling='log')),\n",
        "            loss=hp.Choice('loss', ['sparse_categorical_crossentropy', 'categorical_crossentropy']),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model\n"
      ],
      "metadata": {
        "id": "qshls2-65v3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tuner\n",
        "tuner = RandomSearch(\n",
        "    MLPHyperModel11(),\n",
        "    objective='val_accuracy',\n",
        "    max_trials=3,\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='mlp_tuning'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bDOVT5N6WZ2",
        "outputId": "ed58ec8b-b7b6-4fb2-cb81-41efbf555309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading Tuner from my_dir/mlp_tuning/tuner0.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(x_train, y_train_sparse, epochs=100,batch_size=64, validation_data=(x_test, y_test_sparse))\n",
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ],
      "metadata": {
        "id": "lTAY7Q0B6dHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the best hyperparameters\n",
        "best_hyperparameters = tuner.get_best_hyperparameters()[0]"
      ],
      "metadata": {
        "id": "WaIXOEkK6nOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "for hp_name in best_hyperparameters.values.keys():\n",
        "    print(f\"{hp_name}: {best_hyperparameters.get(hp_name)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8fCmjSV6r-2",
        "outputId": "03c1fdb7-60de-4ba9-e5f6-95d96d810f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "num_layers: 3\n",
            "units_0: 364\n",
            "activation_0: relu\n",
            "dropout_0: 0.4\n",
            "learning_rate: 0.006019793254027166\n",
            "loss: sparse_categorical_crossentropy\n",
            "units_1: 377\n",
            "activation_1: elu\n",
            "dropout_1: 0.1\n",
            "units_2: 91\n",
            "activation_2: relu\n",
            "dropout_2: 0.1\n",
            "units_3: 221\n",
            "activation_3: relu\n",
            "dropout_3: 0.5\n",
            "units_4: 286\n",
            "activation_4: tanh\n",
            "dropout_4: 0.5\n",
            "units_5: 286\n",
            "activation_5: softmax\n",
            "dropout_5: 0.1\n",
            "units_6: 156\n",
            "activation_6: elu\n",
            "dropout_6: 0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6th"
      ],
      "metadata": {
        "id": "w7oNMAtEKa3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the HyperModel class:1\n",
        "class MLPHyperModel12(HyperModel):\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "\n",
        "        # Number of hidden layers\n",
        "        for i in range(hp.Int('num_layers', 1, 7)):\n",
        "            model.add(Dense(\n",
        "                units=hp.Int(f'units_{i}', min_value=130, max_value=1170, step=13),\n",
        "                activation=hp.Choice(f'activation_{i}', ['relu', 'elu','tanh','softmax']),\n",
        "                input_dim=13 if i == 0 else None\n",
        "            ))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dropout(rate=hp.Float(f'dropout_{i}', 0.1, 0.5, step=0.1)))\n",
        "\n",
        "        # Output layer\n",
        "        model.add(Dense(8, activation='softmax'))\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-1, sampling='log')),\n",
        "            loss=hp.Choice('loss', ['sparse_categorical_crossentropy', 'categorical_crossentropy']),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model"
      ],
      "metadata": {
        "id": "kxwEY_KJ7JN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tuner\n",
        "tuner = RandomSearch(\n",
        "    MLPHyperModel12(),\n",
        "    objective='val_accuracy',\n",
        "    max_trials=3,\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='mlp_tuning'\n",
        ")"
      ],
      "metadata": {
        "id": "07I0L4mI7Ar3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(x_train, y_train_sparse, epochs=100,batch_size=64, validation_data=(x_test, y_test_sparse))\n",
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTf7ualC63-0",
        "outputId": "26f9fbbc-527e-4e89-ea92-4f473a34f6f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3 Complete [00h 00m 20s]\n",
            "val_accuracy: 0.6076388955116272\n",
            "\n",
            "Best val_accuracy So Far: 0.7013888955116272\n",
            "Total elapsed time: 00h 01m 40s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the best hyperparameters\n",
        "best_hyperparameters = tuner.get_best_hyperparameters()[0]"
      ],
      "metadata": {
        "id": "GoovF5Rh605_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "for hp_name in best_hyperparameters.values.keys():\n",
        "    print(f\"{hp_name}: {best_hyperparameters.get(hp_name)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r78LgPmr6r7V",
        "outputId": "0f4e2fc7-9ac1-4246-b5b8-4c60952e596e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "num_layers: 6\n",
            "units_0: 819\n",
            "activation_0: elu\n",
            "dropout_0: 0.4\n",
            "learning_rate: 0.0018936190094949422\n",
            "loss: sparse_categorical_crossentropy\n",
            "units_1: 130\n",
            "activation_1: relu\n",
            "dropout_1: 0.1\n",
            "units_2: 130\n",
            "activation_2: relu\n",
            "dropout_2: 0.1\n",
            "units_3: 130\n",
            "activation_3: relu\n",
            "dropout_3: 0.1\n",
            "units_4: 130\n",
            "activation_4: relu\n",
            "dropout_4: 0.1\n",
            "units_5: 130\n",
            "activation_5: relu\n",
            "dropout_5: 0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters\n",
        "num_layers = 6\n",
        "units_0 = 819\n",
        "activation_0 = 'elu'\n",
        "dropout_0 = 0.4\n",
        "learning_rate = 0.0018936190094949422\n",
        "loss_function = 'sparse_categorical_crossentropy'\n",
        "\n",
        "# Units and activations for subsequent layers\n",
        "layer_config = [\n",
        "    {'units': 130, 'activation': 'relu', 'dropout': 0.1},\n",
        "    {'units': 130, 'activation': 'relu', 'dropout': 0.1},\n",
        "    {'units': 130, 'activation': 'relu', 'dropout': 0.1},\n",
        "    {'units': 130, 'activation': 'relu', 'dropout': 0.1},\n",
        "    {'units': 130, 'activation': 'relu', 'dropout': 0.1}\n",
        "]\n",
        "\n",
        "# Initialize the Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer with Batch Normalization\n",
        "model.add(Dense(units_0, activation=activation_0, input_shape=(13,)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout_0))\n",
        "\n",
        "# Adding subsequent layers\n",
        "for layer in layer_config:\n",
        "    model.add(Dense(layer['units'], activation=layer['activation']))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(layer['dropout']))\n",
        "\n",
        "# Output layer (adjust number of output units to your problem)\n",
        "model.add(Dense(8, activation='softmax'))  # 8 classes as an example for multiclass classification\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "N3T5x9mX6r46",
        "outputId": "1b541ac7-48dd-4fe1-9657-ff1a644317c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m819\u001b[0m)                 │          \u001b[38;5;34m11,466\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m819\u001b[0m)                 │           \u001b[38;5;34m3,276\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m819\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m)                 │         \u001b[38;5;34m106,600\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m)                 │             \u001b[38;5;34m520\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m)                 │          \u001b[38;5;34m17,030\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_8                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m)                 │             \u001b[38;5;34m520\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m)                 │          \u001b[38;5;34m17,030\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_9                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m)                 │             \u001b[38;5;34m520\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m)                 │          \u001b[38;5;34m17,030\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_10               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m)                 │             \u001b[38;5;34m520\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m)                 │          \u001b[38;5;34m17,030\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_11               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m)                 │             \u001b[38;5;34m520\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │           \u001b[38;5;34m1,048\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">819</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">11,466</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">819</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,276</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">819</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">106,600</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,030</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_8                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,030</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_9                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,030</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_10               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,030</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_11               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,048</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m193,110\u001b[0m (754.34 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">193,110</span> (754.34 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m190,172\u001b[0m (742.86 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">190,172</span> (742.86 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,938\u001b[0m (11.48 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,938</span> (11.48 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=200,batch_size=64, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA_Jq5x9_Xrn",
        "outputId": "b7ca68bf-6b6f-4023-8d36-9713a61f3e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.1500 - loss: 2.6148 - val_accuracy: 0.2674 - val_loss: 1.9960\n",
            "Epoch 2/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.2965 - loss: 1.9323 - val_accuracy: 0.2951 - val_loss: 1.9552\n",
            "Epoch 3/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3795 - loss: 1.7152 - val_accuracy: 0.2882 - val_loss: 1.8817\n",
            "Epoch 4/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4007 - loss: 1.6320 - val_accuracy: 0.3264 - val_loss: 1.8368\n",
            "Epoch 5/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4590 - loss: 1.5052 - val_accuracy: 0.3507 - val_loss: 1.8048\n",
            "Epoch 6/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4822 - loss: 1.3899 - val_accuracy: 0.3368 - val_loss: 1.7606\n",
            "Epoch 7/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4989 - loss: 1.3523 - val_accuracy: 0.3472 - val_loss: 1.7352\n",
            "Epoch 8/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5088 - loss: 1.3076 - val_accuracy: 0.3056 - val_loss: 1.7367\n",
            "Epoch 9/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5602 - loss: 1.2207 - val_accuracy: 0.3299 - val_loss: 1.6607\n",
            "Epoch 10/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5708 - loss: 1.1703 - val_accuracy: 0.3819 - val_loss: 1.6204\n",
            "Epoch 11/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5702 - loss: 1.1467 - val_accuracy: 0.4167 - val_loss: 1.5604\n",
            "Epoch 12/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5843 - loss: 1.1025 - val_accuracy: 0.4201 - val_loss: 1.5349\n",
            "Epoch 13/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6082 - loss: 1.0866 - val_accuracy: 0.4271 - val_loss: 1.4718\n",
            "Epoch 14/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6175 - loss: 1.0374 - val_accuracy: 0.4688 - val_loss: 1.4424\n",
            "Epoch 15/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6266 - loss: 0.9862 - val_accuracy: 0.4792 - val_loss: 1.4741\n",
            "Epoch 16/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6113 - loss: 1.0353 - val_accuracy: 0.4965 - val_loss: 1.3671\n",
            "Epoch 17/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6408 - loss: 0.9669 - val_accuracy: 0.5069 - val_loss: 1.3377\n",
            "Epoch 18/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6332 - loss: 0.9883 - val_accuracy: 0.5417 - val_loss: 1.2184\n",
            "Epoch 19/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6269 - loss: 0.9683 - val_accuracy: 0.5625 - val_loss: 1.1779\n",
            "Epoch 20/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6558 - loss: 0.8887 - val_accuracy: 0.5764 - val_loss: 1.1555\n",
            "Epoch 21/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7000 - loss: 0.8398 - val_accuracy: 0.5556 - val_loss: 1.1733\n",
            "Epoch 22/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7019 - loss: 0.8521 - val_accuracy: 0.5833 - val_loss: 1.1362\n",
            "Epoch 23/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6733 - loss: 0.8772 - val_accuracy: 0.6076 - val_loss: 1.1350\n",
            "Epoch 24/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6900 - loss: 0.8430 - val_accuracy: 0.6111 - val_loss: 1.1373\n",
            "Epoch 25/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7023 - loss: 0.7890 - val_accuracy: 0.5799 - val_loss: 1.1371\n",
            "Epoch 26/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6955 - loss: 0.7949 - val_accuracy: 0.5833 - val_loss: 1.1171\n",
            "Epoch 27/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7361 - loss: 0.7381 - val_accuracy: 0.6007 - val_loss: 1.0777\n",
            "Epoch 28/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7138 - loss: 0.7455 - val_accuracy: 0.6007 - val_loss: 1.0828\n",
            "Epoch 29/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7629 - loss: 0.6737 - val_accuracy: 0.6111 - val_loss: 1.1387\n",
            "Epoch 30/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.7278 - loss: 0.7390 - val_accuracy: 0.6042 - val_loss: 1.1282\n",
            "Epoch 31/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7291 - loss: 0.6800 - val_accuracy: 0.6146 - val_loss: 1.0934\n",
            "Epoch 32/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7477 - loss: 0.6922 - val_accuracy: 0.6181 - val_loss: 1.1014\n",
            "Epoch 33/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7741 - loss: 0.6286 - val_accuracy: 0.6181 - val_loss: 1.1306\n",
            "Epoch 34/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7504 - loss: 0.6511 - val_accuracy: 0.6076 - val_loss: 1.1167\n",
            "Epoch 35/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7604 - loss: 0.6672 - val_accuracy: 0.6389 - val_loss: 1.0560\n",
            "Epoch 36/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7860 - loss: 0.5821 - val_accuracy: 0.6493 - val_loss: 1.0858\n",
            "Epoch 37/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7529 - loss: 0.6689 - val_accuracy: 0.6181 - val_loss: 1.1663\n",
            "Epoch 38/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7776 - loss: 0.6286 - val_accuracy: 0.6146 - val_loss: 1.1169\n",
            "Epoch 39/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7670 - loss: 0.5831 - val_accuracy: 0.6319 - val_loss: 1.0746\n",
            "Epoch 40/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8012 - loss: 0.5726 - val_accuracy: 0.6285 - val_loss: 1.1475\n",
            "Epoch 41/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7754 - loss: 0.6060 - val_accuracy: 0.6389 - val_loss: 1.1320\n",
            "Epoch 42/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7936 - loss: 0.6113 - val_accuracy: 0.6042 - val_loss: 1.2652\n",
            "Epoch 43/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7761 - loss: 0.6233 - val_accuracy: 0.6076 - val_loss: 1.1659\n",
            "Epoch 44/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7827 - loss: 0.6020 - val_accuracy: 0.6146 - val_loss: 1.2612\n",
            "Epoch 45/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7983 - loss: 0.5670 - val_accuracy: 0.6042 - val_loss: 1.2903\n",
            "Epoch 46/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7941 - loss: 0.5788 - val_accuracy: 0.6146 - val_loss: 1.2408\n",
            "Epoch 47/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8468 - loss: 0.4538 - val_accuracy: 0.6250 - val_loss: 1.1765\n",
            "Epoch 48/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8395 - loss: 0.4704 - val_accuracy: 0.6528 - val_loss: 1.1082\n",
            "Epoch 49/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8011 - loss: 0.5262 - val_accuracy: 0.6667 - val_loss: 1.1497\n",
            "Epoch 50/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8335 - loss: 0.4897 - val_accuracy: 0.6701 - val_loss: 1.1606\n",
            "Epoch 51/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8069 - loss: 0.5371 - val_accuracy: 0.6458 - val_loss: 1.2181\n",
            "Epoch 52/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8434 - loss: 0.4676 - val_accuracy: 0.6597 - val_loss: 1.0844\n",
            "Epoch 53/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8308 - loss: 0.4723 - val_accuracy: 0.6042 - val_loss: 1.2170\n",
            "Epoch 54/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8309 - loss: 0.4376 - val_accuracy: 0.5903 - val_loss: 1.3397\n",
            "Epoch 55/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8056 - loss: 0.5093 - val_accuracy: 0.6389 - val_loss: 1.2450\n",
            "Epoch 56/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8151 - loss: 0.5067 - val_accuracy: 0.6458 - val_loss: 1.2606\n",
            "Epoch 57/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8010 - loss: 0.5144 - val_accuracy: 0.6076 - val_loss: 1.2125\n",
            "Epoch 58/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8361 - loss: 0.4539 - val_accuracy: 0.6111 - val_loss: 1.1795\n",
            "Epoch 59/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8252 - loss: 0.4549 - val_accuracy: 0.6389 - val_loss: 1.1727\n",
            "Epoch 60/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8641 - loss: 0.3894 - val_accuracy: 0.6562 - val_loss: 1.2596\n",
            "Epoch 61/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8266 - loss: 0.4427 - val_accuracy: 0.6215 - val_loss: 1.2528\n",
            "Epoch 62/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8551 - loss: 0.4204 - val_accuracy: 0.6215 - val_loss: 1.2180\n",
            "Epoch 63/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8524 - loss: 0.4223 - val_accuracy: 0.6076 - val_loss: 1.3223\n",
            "Epoch 64/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8581 - loss: 0.4019 - val_accuracy: 0.6354 - val_loss: 1.2519\n",
            "Epoch 65/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8529 - loss: 0.4114 - val_accuracy: 0.6389 - val_loss: 1.2533\n",
            "Epoch 66/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8380 - loss: 0.4220 - val_accuracy: 0.6215 - val_loss: 1.2618\n",
            "Epoch 67/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8196 - loss: 0.4240 - val_accuracy: 0.6319 - val_loss: 1.3461\n",
            "Epoch 68/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8546 - loss: 0.4142 - val_accuracy: 0.6285 - val_loss: 1.2898\n",
            "Epoch 69/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8495 - loss: 0.4155 - val_accuracy: 0.6111 - val_loss: 1.3469\n",
            "Epoch 70/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8389 - loss: 0.4485 - val_accuracy: 0.6632 - val_loss: 1.2308\n",
            "Epoch 71/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8845 - loss: 0.3404 - val_accuracy: 0.6493 - val_loss: 1.2364\n",
            "Epoch 72/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8863 - loss: 0.3077 - val_accuracy: 0.6632 - val_loss: 1.2848\n",
            "Epoch 73/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8633 - loss: 0.3755 - val_accuracy: 0.6458 - val_loss: 1.3006\n",
            "Epoch 74/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8514 - loss: 0.3801 - val_accuracy: 0.6181 - val_loss: 1.2877\n",
            "Epoch 75/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8648 - loss: 0.3603 - val_accuracy: 0.6458 - val_loss: 1.2234\n",
            "Epoch 76/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8824 - loss: 0.3298 - val_accuracy: 0.6493 - val_loss: 1.1788\n",
            "Epoch 77/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8836 - loss: 0.3275 - val_accuracy: 0.6562 - val_loss: 1.1829\n",
            "Epoch 78/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8762 - loss: 0.3377 - val_accuracy: 0.6562 - val_loss: 1.2095\n",
            "Epoch 79/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8848 - loss: 0.3358 - val_accuracy: 0.6632 - val_loss: 1.1529\n",
            "Epoch 80/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8567 - loss: 0.3794 - val_accuracy: 0.6597 - val_loss: 1.1346\n",
            "Epoch 81/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9053 - loss: 0.2785 - val_accuracy: 0.6597 - val_loss: 1.2562\n",
            "Epoch 82/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8697 - loss: 0.3512 - val_accuracy: 0.6632 - val_loss: 1.2799\n",
            "Epoch 83/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8892 - loss: 0.3216 - val_accuracy: 0.6562 - val_loss: 1.2343\n",
            "Epoch 84/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8639 - loss: 0.3576 - val_accuracy: 0.6354 - val_loss: 1.2580\n",
            "Epoch 85/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8740 - loss: 0.3788 - val_accuracy: 0.6667 - val_loss: 1.2478\n",
            "Epoch 86/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8654 - loss: 0.3588 - val_accuracy: 0.6597 - val_loss: 1.3389\n",
            "Epoch 87/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8804 - loss: 0.3559 - val_accuracy: 0.6181 - val_loss: 1.3466\n",
            "Epoch 88/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8869 - loss: 0.3233 - val_accuracy: 0.6354 - val_loss: 1.2631\n",
            "Epoch 89/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8819 - loss: 0.2965 - val_accuracy: 0.6250 - val_loss: 1.3178\n",
            "Epoch 90/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8855 - loss: 0.3134 - val_accuracy: 0.6319 - val_loss: 1.4285\n",
            "Epoch 91/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8949 - loss: 0.3010 - val_accuracy: 0.6528 - val_loss: 1.3296\n",
            "Epoch 92/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8820 - loss: 0.3207 - val_accuracy: 0.6215 - val_loss: 1.3624\n",
            "Epoch 93/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8922 - loss: 0.3280 - val_accuracy: 0.6458 - val_loss: 1.4301\n",
            "Epoch 94/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9072 - loss: 0.2891 - val_accuracy: 0.6354 - val_loss: 1.3053\n",
            "Epoch 95/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8807 - loss: 0.3379 - val_accuracy: 0.6319 - val_loss: 1.3412\n",
            "Epoch 96/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8700 - loss: 0.3763 - val_accuracy: 0.6354 - val_loss: 1.3082\n",
            "Epoch 97/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8788 - loss: 0.3074 - val_accuracy: 0.6181 - val_loss: 1.2899\n",
            "Epoch 98/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8768 - loss: 0.3313 - val_accuracy: 0.6562 - val_loss: 1.2994\n",
            "Epoch 99/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8885 - loss: 0.3286 - val_accuracy: 0.6424 - val_loss: 1.3092\n",
            "Epoch 100/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8988 - loss: 0.3031 - val_accuracy: 0.6389 - val_loss: 1.3943\n",
            "Epoch 101/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8917 - loss: 0.2890 - val_accuracy: 0.6701 - val_loss: 1.3396\n",
            "Epoch 102/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9053 - loss: 0.2770 - val_accuracy: 0.6632 - val_loss: 1.3437\n",
            "Epoch 103/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8746 - loss: 0.3470 - val_accuracy: 0.6285 - val_loss: 1.4504\n",
            "Epoch 104/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8948 - loss: 0.2923 - val_accuracy: 0.6319 - val_loss: 1.3568\n",
            "Epoch 105/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8962 - loss: 0.2951 - val_accuracy: 0.6389 - val_loss: 1.3668\n",
            "Epoch 106/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9063 - loss: 0.2766 - val_accuracy: 0.6528 - val_loss: 1.2595\n",
            "Epoch 107/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8949 - loss: 0.2881 - val_accuracy: 0.6424 - val_loss: 1.3125\n",
            "Epoch 108/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8936 - loss: 0.2942 - val_accuracy: 0.6632 - val_loss: 1.2829\n",
            "Epoch 109/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9173 - loss: 0.2691 - val_accuracy: 0.6667 - val_loss: 1.3212\n",
            "Epoch 110/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9082 - loss: 0.2933 - val_accuracy: 0.6424 - val_loss: 1.3440\n",
            "Epoch 111/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8862 - loss: 0.2936 - val_accuracy: 0.6493 - val_loss: 1.3092\n",
            "Epoch 112/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8847 - loss: 0.3441 - val_accuracy: 0.6771 - val_loss: 1.2216\n",
            "Epoch 113/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8960 - loss: 0.3057 - val_accuracy: 0.7118 - val_loss: 1.1309\n",
            "Epoch 114/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8957 - loss: 0.2845 - val_accuracy: 0.6250 - val_loss: 1.3772\n",
            "Epoch 115/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9160 - loss: 0.2465 - val_accuracy: 0.6493 - val_loss: 1.3458\n",
            "Epoch 116/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8798 - loss: 0.3319 - val_accuracy: 0.6493 - val_loss: 1.3291\n",
            "Epoch 117/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8986 - loss: 0.2529 - val_accuracy: 0.6562 - val_loss: 1.3582\n",
            "Epoch 118/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9026 - loss: 0.2719 - val_accuracy: 0.6528 - val_loss: 1.3556\n",
            "Epoch 119/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8950 - loss: 0.2973 - val_accuracy: 0.6528 - val_loss: 1.3225\n",
            "Epoch 120/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9062 - loss: 0.2663 - val_accuracy: 0.6840 - val_loss: 1.3535\n",
            "Epoch 121/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9281 - loss: 0.2215 - val_accuracy: 0.6389 - val_loss: 1.4508\n",
            "Epoch 122/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9146 - loss: 0.2501 - val_accuracy: 0.6319 - val_loss: 1.3721\n",
            "Epoch 123/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9101 - loss: 0.2531 - val_accuracy: 0.6285 - val_loss: 1.4694\n",
            "Epoch 124/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8901 - loss: 0.3029 - val_accuracy: 0.6215 - val_loss: 1.3169\n",
            "Epoch 125/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9157 - loss: 0.2142 - val_accuracy: 0.6354 - val_loss: 1.4194\n",
            "Epoch 126/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9143 - loss: 0.2349 - val_accuracy: 0.6562 - val_loss: 1.3763\n",
            "Epoch 127/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9094 - loss: 0.2588 - val_accuracy: 0.6250 - val_loss: 1.3668\n",
            "Epoch 128/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8988 - loss: 0.2947 - val_accuracy: 0.6493 - val_loss: 1.3982\n",
            "Epoch 129/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9112 - loss: 0.2634 - val_accuracy: 0.6493 - val_loss: 1.3672\n",
            "Epoch 130/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9069 - loss: 0.2572 - val_accuracy: 0.6250 - val_loss: 1.4201\n",
            "Epoch 131/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8956 - loss: 0.2850 - val_accuracy: 0.6597 - val_loss: 1.4156\n",
            "Epoch 132/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8918 - loss: 0.3027 - val_accuracy: 0.6667 - val_loss: 1.5082\n",
            "Epoch 133/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9179 - loss: 0.2544 - val_accuracy: 0.6285 - val_loss: 1.5681\n",
            "Epoch 134/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8954 - loss: 0.2920 - val_accuracy: 0.6562 - val_loss: 1.3801\n",
            "Epoch 135/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9104 - loss: 0.2549 - val_accuracy: 0.6354 - val_loss: 1.4998\n",
            "Epoch 136/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9121 - loss: 0.2407 - val_accuracy: 0.6424 - val_loss: 1.3956\n",
            "Epoch 137/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8943 - loss: 0.2743 - val_accuracy: 0.6458 - val_loss: 1.4137\n",
            "Epoch 138/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9070 - loss: 0.2526 - val_accuracy: 0.6632 - val_loss: 1.3658\n",
            "Epoch 139/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9072 - loss: 0.2544 - val_accuracy: 0.6354 - val_loss: 1.4626\n",
            "Epoch 140/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9241 - loss: 0.2132 - val_accuracy: 0.6215 - val_loss: 1.4736\n",
            "Epoch 141/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9084 - loss: 0.2562 - val_accuracy: 0.6215 - val_loss: 1.3669\n",
            "Epoch 142/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9085 - loss: 0.2602 - val_accuracy: 0.6562 - val_loss: 1.3157\n",
            "Epoch 143/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9075 - loss: 0.2471 - val_accuracy: 0.6528 - val_loss: 1.4026\n",
            "Epoch 144/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9177 - loss: 0.2335 - val_accuracy: 0.6354 - val_loss: 1.3570\n",
            "Epoch 145/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9194 - loss: 0.2334 - val_accuracy: 0.6424 - val_loss: 1.4081\n",
            "Epoch 146/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9374 - loss: 0.2034 - val_accuracy: 0.6632 - val_loss: 1.3360\n",
            "Epoch 147/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9032 - loss: 0.2706 - val_accuracy: 0.6528 - val_loss: 1.3504\n",
            "Epoch 148/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9120 - loss: 0.2233 - val_accuracy: 0.6528 - val_loss: 1.3190\n",
            "Epoch 149/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9222 - loss: 0.2122 - val_accuracy: 0.6667 - val_loss: 1.3433\n",
            "Epoch 150/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9362 - loss: 0.1735 - val_accuracy: 0.6562 - val_loss: 1.3283\n",
            "Epoch 151/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8912 - loss: 0.2755 - val_accuracy: 0.6597 - val_loss: 1.3792\n",
            "Epoch 152/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9149 - loss: 0.2346 - val_accuracy: 0.6493 - val_loss: 1.3240\n",
            "Epoch 153/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9153 - loss: 0.2647 - val_accuracy: 0.6319 - val_loss: 1.3224\n",
            "Epoch 154/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.9428 - loss: 0.1711 - val_accuracy: 0.6493 - val_loss: 1.4062\n",
            "Epoch 155/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9183 - loss: 0.2107 - val_accuracy: 0.6528 - val_loss: 1.3557\n",
            "Epoch 156/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9340 - loss: 0.2141 - val_accuracy: 0.6632 - val_loss: 1.3320\n",
            "Epoch 157/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9430 - loss: 0.1885 - val_accuracy: 0.6354 - val_loss: 1.4243\n",
            "Epoch 158/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9278 - loss: 0.2040 - val_accuracy: 0.6562 - val_loss: 1.4013\n",
            "Epoch 159/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9255 - loss: 0.2031 - val_accuracy: 0.6701 - val_loss: 1.3356\n",
            "Epoch 160/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9419 - loss: 0.2068 - val_accuracy: 0.6389 - val_loss: 1.3242\n",
            "Epoch 161/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9199 - loss: 0.2195 - val_accuracy: 0.6528 - val_loss: 1.3589\n",
            "Epoch 162/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9123 - loss: 0.2596 - val_accuracy: 0.6389 - val_loss: 1.5806\n",
            "Epoch 163/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9279 - loss: 0.2194 - val_accuracy: 0.6597 - val_loss: 1.2897\n",
            "Epoch 164/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8993 - loss: 0.2452 - val_accuracy: 0.6389 - val_loss: 1.4275\n",
            "Epoch 165/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9005 - loss: 0.3071 - val_accuracy: 0.6458 - val_loss: 1.4321\n",
            "Epoch 166/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9347 - loss: 0.1811 - val_accuracy: 0.6771 - val_loss: 1.4738\n",
            "Epoch 167/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9365 - loss: 0.1738 - val_accuracy: 0.6806 - val_loss: 1.5357\n",
            "Epoch 168/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9165 - loss: 0.2459 - val_accuracy: 0.6458 - val_loss: 1.4941\n",
            "Epoch 169/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9239 - loss: 0.2129 - val_accuracy: 0.6597 - val_loss: 1.4738\n",
            "Epoch 170/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9243 - loss: 0.2398 - val_accuracy: 0.6458 - val_loss: 1.4572\n",
            "Epoch 171/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9346 - loss: 0.2041 - val_accuracy: 0.6771 - val_loss: 1.4622\n",
            "Epoch 172/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9477 - loss: 0.1573 - val_accuracy: 0.6701 - val_loss: 1.4665\n",
            "Epoch 173/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9467 - loss: 0.1632 - val_accuracy: 0.6875 - val_loss: 1.3375\n",
            "Epoch 174/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9292 - loss: 0.1881 - val_accuracy: 0.6771 - val_loss: 1.3355\n",
            "Epoch 175/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9359 - loss: 0.2175 - val_accuracy: 0.6562 - val_loss: 1.3757\n",
            "Epoch 176/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9129 - loss: 0.2427 - val_accuracy: 0.6771 - val_loss: 1.4090\n",
            "Epoch 177/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9299 - loss: 0.1988 - val_accuracy: 0.6632 - val_loss: 1.3985\n",
            "Epoch 178/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9265 - loss: 0.2018 - val_accuracy: 0.6528 - val_loss: 1.4895\n",
            "Epoch 179/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9374 - loss: 0.1675 - val_accuracy: 0.6632 - val_loss: 1.4334\n",
            "Epoch 180/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9298 - loss: 0.2007 - val_accuracy: 0.6389 - val_loss: 1.4776\n",
            "Epoch 181/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9447 - loss: 0.1707 - val_accuracy: 0.6736 - val_loss: 1.4373\n",
            "Epoch 182/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9426 - loss: 0.1650 - val_accuracy: 0.6701 - val_loss: 1.3781\n",
            "Epoch 183/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9239 - loss: 0.2010 - val_accuracy: 0.6389 - val_loss: 1.4602\n",
            "Epoch 184/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9458 - loss: 0.1484 - val_accuracy: 0.6597 - val_loss: 1.4155\n",
            "Epoch 185/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9270 - loss: 0.1989 - val_accuracy: 0.6736 - val_loss: 1.5243\n",
            "Epoch 186/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9335 - loss: 0.1689 - val_accuracy: 0.6597 - val_loss: 1.5368\n",
            "Epoch 187/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9362 - loss: 0.1724 - val_accuracy: 0.6458 - val_loss: 1.5082\n",
            "Epoch 188/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9400 - loss: 0.1828 - val_accuracy: 0.6493 - val_loss: 1.4654\n",
            "Epoch 189/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9286 - loss: 0.2076 - val_accuracy: 0.6562 - val_loss: 1.4583\n",
            "Epoch 190/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9213 - loss: 0.2137 - val_accuracy: 0.6736 - val_loss: 1.5149\n",
            "Epoch 191/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9188 - loss: 0.2111 - val_accuracy: 0.6667 - val_loss: 1.4228\n",
            "Epoch 192/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9256 - loss: 0.2192 - val_accuracy: 0.6285 - val_loss: 1.4870\n",
            "Epoch 193/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9282 - loss: 0.1994 - val_accuracy: 0.6389 - val_loss: 1.5085\n",
            "Epoch 194/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9237 - loss: 0.2435 - val_accuracy: 0.6250 - val_loss: 1.6290\n",
            "Epoch 195/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9262 - loss: 0.1774 - val_accuracy: 0.6215 - val_loss: 1.5456\n",
            "Epoch 196/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9221 - loss: 0.1912 - val_accuracy: 0.6528 - val_loss: 1.4943\n",
            "Epoch 197/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9508 - loss: 0.1584 - val_accuracy: 0.6458 - val_loss: 1.5153\n",
            "Epoch 198/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9400 - loss: 0.1711 - val_accuracy: 0.6424 - val_loss: 1.5895\n",
            "Epoch 199/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9434 - loss: 0.2003 - val_accuracy: 0.6285 - val_loss: 1.6225\n",
            "Epoch 200/200\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9366 - loss: 0.1734 - val_accuracy: 0.6389 - val_loss: 1.5228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=200,batch_size=100, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WU48f5B3_5EW",
        "outputId": "df42c00b-49c8-4f05-8aa1-5a8d6c6ddcac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.9649 - loss: 0.1011 - val_accuracy: 0.6562 - val_loss: 1.5279\n",
            "Epoch 2/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9463 - loss: 0.1323 - val_accuracy: 0.6701 - val_loss: 1.4633\n",
            "Epoch 3/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9636 - loss: 0.1164 - val_accuracy: 0.6562 - val_loss: 1.4530\n",
            "Epoch 4/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9718 - loss: 0.0905 - val_accuracy: 0.6389 - val_loss: 1.5155\n",
            "Epoch 5/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9859 - loss: 0.0807 - val_accuracy: 0.6354 - val_loss: 1.5244\n",
            "Epoch 6/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9673 - loss: 0.0899 - val_accuracy: 0.6562 - val_loss: 1.4932\n",
            "Epoch 7/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9725 - loss: 0.0990 - val_accuracy: 0.6562 - val_loss: 1.5080\n",
            "Epoch 8/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9718 - loss: 0.0842 - val_accuracy: 0.6597 - val_loss: 1.5177\n",
            "Epoch 9/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9632 - loss: 0.0994 - val_accuracy: 0.6389 - val_loss: 1.5939\n",
            "Epoch 10/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9565 - loss: 0.0952 - val_accuracy: 0.6354 - val_loss: 1.6878\n",
            "Epoch 11/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9658 - loss: 0.0922 - val_accuracy: 0.6493 - val_loss: 1.6095\n",
            "Epoch 12/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9638 - loss: 0.1093 - val_accuracy: 0.6528 - val_loss: 1.5147\n",
            "Epoch 13/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9706 - loss: 0.0953 - val_accuracy: 0.6458 - val_loss: 1.4803\n",
            "Epoch 14/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9538 - loss: 0.1295 - val_accuracy: 0.6667 - val_loss: 1.4780\n",
            "Epoch 15/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9680 - loss: 0.0995 - val_accuracy: 0.6667 - val_loss: 1.5200\n",
            "Epoch 16/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9639 - loss: 0.0947 - val_accuracy: 0.6528 - val_loss: 1.4766\n",
            "Epoch 17/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9695 - loss: 0.0807 - val_accuracy: 0.6701 - val_loss: 1.4211\n",
            "Epoch 18/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9583 - loss: 0.1375 - val_accuracy: 0.6493 - val_loss: 1.4908\n",
            "Epoch 19/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9681 - loss: 0.1231 - val_accuracy: 0.6354 - val_loss: 1.4839\n",
            "Epoch 20/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9735 - loss: 0.0826 - val_accuracy: 0.6493 - val_loss: 1.5330\n",
            "Epoch 21/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9757 - loss: 0.0793 - val_accuracy: 0.6389 - val_loss: 1.5623\n",
            "Epoch 22/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9680 - loss: 0.0954 - val_accuracy: 0.6215 - val_loss: 1.5995\n",
            "Epoch 23/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9635 - loss: 0.0959 - val_accuracy: 0.6667 - val_loss: 1.5580\n",
            "Epoch 24/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9755 - loss: 0.0832 - val_accuracy: 0.6493 - val_loss: 1.5256\n",
            "Epoch 25/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9644 - loss: 0.0945 - val_accuracy: 0.6354 - val_loss: 1.6491\n",
            "Epoch 26/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9769 - loss: 0.0680 - val_accuracy: 0.6424 - val_loss: 1.6285\n",
            "Epoch 27/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9591 - loss: 0.1014 - val_accuracy: 0.6597 - val_loss: 1.5834\n",
            "Epoch 28/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9658 - loss: 0.0934 - val_accuracy: 0.6424 - val_loss: 1.6171\n",
            "Epoch 29/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9791 - loss: 0.0778 - val_accuracy: 0.6458 - val_loss: 1.6184\n",
            "Epoch 30/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9699 - loss: 0.0963 - val_accuracy: 0.6389 - val_loss: 1.5933\n",
            "Epoch 31/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9594 - loss: 0.1108 - val_accuracy: 0.6528 - val_loss: 1.6338\n",
            "Epoch 32/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9733 - loss: 0.0669 - val_accuracy: 0.6493 - val_loss: 1.6868\n",
            "Epoch 33/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9631 - loss: 0.0949 - val_accuracy: 0.6424 - val_loss: 1.6639\n",
            "Epoch 34/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9721 - loss: 0.0981 - val_accuracy: 0.6458 - val_loss: 1.6832\n",
            "Epoch 35/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9660 - loss: 0.1138 - val_accuracy: 0.6493 - val_loss: 1.6773\n",
            "Epoch 36/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9677 - loss: 0.0829 - val_accuracy: 0.6562 - val_loss: 1.6103\n",
            "Epoch 37/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9709 - loss: 0.0868 - val_accuracy: 0.6424 - val_loss: 1.6415\n",
            "Epoch 38/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9735 - loss: 0.0821 - val_accuracy: 0.6389 - val_loss: 1.6849\n",
            "Epoch 39/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9762 - loss: 0.0846 - val_accuracy: 0.6562 - val_loss: 1.7008\n",
            "Epoch 40/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9745 - loss: 0.0832 - val_accuracy: 0.6528 - val_loss: 1.7077\n",
            "Epoch 41/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9809 - loss: 0.0594 - val_accuracy: 0.6562 - val_loss: 1.7309\n",
            "Epoch 42/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9732 - loss: 0.0850 - val_accuracy: 0.6597 - val_loss: 1.6873\n",
            "Epoch 43/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9740 - loss: 0.0899 - val_accuracy: 0.6528 - val_loss: 1.6810\n",
            "Epoch 44/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9735 - loss: 0.0866 - val_accuracy: 0.6632 - val_loss: 1.6877\n",
            "Epoch 45/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9548 - loss: 0.1232 - val_accuracy: 0.6701 - val_loss: 1.5998\n",
            "Epoch 46/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9728 - loss: 0.0861 - val_accuracy: 0.6597 - val_loss: 1.5296\n",
            "Epoch 47/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9784 - loss: 0.0673 - val_accuracy: 0.6632 - val_loss: 1.4862\n",
            "Epoch 48/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9759 - loss: 0.0745 - val_accuracy: 0.6528 - val_loss: 1.5353\n",
            "Epoch 49/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9694 - loss: 0.0808 - val_accuracy: 0.6597 - val_loss: 1.6197\n",
            "Epoch 50/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9608 - loss: 0.1156 - val_accuracy: 0.6597 - val_loss: 1.6386\n",
            "Epoch 51/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9704 - loss: 0.0916 - val_accuracy: 0.6528 - val_loss: 1.5917\n",
            "Epoch 52/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9707 - loss: 0.0916 - val_accuracy: 0.6493 - val_loss: 1.5929\n",
            "Epoch 53/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9761 - loss: 0.0843 - val_accuracy: 0.6667 - val_loss: 1.6165\n",
            "Epoch 54/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9738 - loss: 0.0805 - val_accuracy: 0.6562 - val_loss: 1.6274\n",
            "Epoch 55/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9711 - loss: 0.0845 - val_accuracy: 0.6562 - val_loss: 1.6317\n",
            "Epoch 56/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9628 - loss: 0.0989 - val_accuracy: 0.6285 - val_loss: 1.6756\n",
            "Epoch 57/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9633 - loss: 0.0982 - val_accuracy: 0.6250 - val_loss: 1.6824\n",
            "Epoch 58/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9656 - loss: 0.0982 - val_accuracy: 0.6458 - val_loss: 1.6513\n",
            "Epoch 59/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9586 - loss: 0.1233 - val_accuracy: 0.6562 - val_loss: 1.6076\n",
            "Epoch 60/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9661 - loss: 0.1118 - val_accuracy: 0.6701 - val_loss: 1.6136\n",
            "Epoch 61/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9589 - loss: 0.1401 - val_accuracy: 0.6597 - val_loss: 1.6532\n",
            "Epoch 62/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9797 - loss: 0.0704 - val_accuracy: 0.6597 - val_loss: 1.6798\n",
            "Epoch 63/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9622 - loss: 0.1114 - val_accuracy: 0.6562 - val_loss: 1.6632\n",
            "Epoch 64/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9659 - loss: 0.1117 - val_accuracy: 0.6424 - val_loss: 1.6591\n",
            "Epoch 65/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9672 - loss: 0.1013 - val_accuracy: 0.6597 - val_loss: 1.6147\n",
            "Epoch 66/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9609 - loss: 0.1323 - val_accuracy: 0.6389 - val_loss: 1.6704\n",
            "Epoch 67/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9691 - loss: 0.1050 - val_accuracy: 0.6597 - val_loss: 1.7314\n",
            "Epoch 68/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9549 - loss: 0.1223 - val_accuracy: 0.6250 - val_loss: 1.8561\n",
            "Epoch 69/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9667 - loss: 0.0933 - val_accuracy: 0.6667 - val_loss: 1.7163\n",
            "Epoch 70/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9565 - loss: 0.1211 - val_accuracy: 0.6910 - val_loss: 1.7044\n",
            "Epoch 71/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9568 - loss: 0.1270 - val_accuracy: 0.6736 - val_loss: 1.6589\n",
            "Epoch 72/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9671 - loss: 0.1178 - val_accuracy: 0.6562 - val_loss: 1.7781\n",
            "Epoch 73/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9615 - loss: 0.1076 - val_accuracy: 0.6424 - val_loss: 1.7543\n",
            "Epoch 74/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9626 - loss: 0.1047 - val_accuracy: 0.6319 - val_loss: 1.6907\n",
            "Epoch 75/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9614 - loss: 0.1157 - val_accuracy: 0.6667 - val_loss: 1.6062\n",
            "Epoch 76/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9652 - loss: 0.0889 - val_accuracy: 0.6597 - val_loss: 1.6954\n",
            "Epoch 77/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9614 - loss: 0.1099 - val_accuracy: 0.6562 - val_loss: 1.6858\n",
            "Epoch 78/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9619 - loss: 0.0991 - val_accuracy: 0.6701 - val_loss: 1.6606\n",
            "Epoch 79/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9588 - loss: 0.1305 - val_accuracy: 0.6632 - val_loss: 1.7482\n",
            "Epoch 80/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9536 - loss: 0.1180 - val_accuracy: 0.6597 - val_loss: 1.6708\n",
            "Epoch 81/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9728 - loss: 0.0922 - val_accuracy: 0.6562 - val_loss: 1.6781\n",
            "Epoch 82/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9636 - loss: 0.0884 - val_accuracy: 0.6493 - val_loss: 1.6982\n",
            "Epoch 83/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9769 - loss: 0.0662 - val_accuracy: 0.6354 - val_loss: 1.6894\n",
            "Epoch 84/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9681 - loss: 0.1018 - val_accuracy: 0.6458 - val_loss: 1.6489\n",
            "Epoch 85/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9520 - loss: 0.1458 - val_accuracy: 0.6354 - val_loss: 1.6725\n",
            "Epoch 86/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9685 - loss: 0.0946 - val_accuracy: 0.6424 - val_loss: 1.7055\n",
            "Epoch 87/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9635 - loss: 0.0988 - val_accuracy: 0.6597 - val_loss: 1.6380\n",
            "Epoch 88/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9707 - loss: 0.0899 - val_accuracy: 0.6632 - val_loss: 1.5742\n",
            "Epoch 89/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9559 - loss: 0.1099 - val_accuracy: 0.6632 - val_loss: 1.6184\n",
            "Epoch 90/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9653 - loss: 0.0998 - val_accuracy: 0.6632 - val_loss: 1.6780\n",
            "Epoch 91/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9614 - loss: 0.1175 - val_accuracy: 0.6597 - val_loss: 1.6284\n",
            "Epoch 92/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9658 - loss: 0.0988 - val_accuracy: 0.6528 - val_loss: 1.6341\n",
            "Epoch 93/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9725 - loss: 0.0971 - val_accuracy: 0.6319 - val_loss: 1.7001\n",
            "Epoch 94/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9637 - loss: 0.1040 - val_accuracy: 0.6806 - val_loss: 1.6710\n",
            "Epoch 95/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9610 - loss: 0.1052 - val_accuracy: 0.6667 - val_loss: 1.6933\n",
            "Epoch 96/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9560 - loss: 0.1237 - val_accuracy: 0.6736 - val_loss: 1.6905\n",
            "Epoch 97/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9625 - loss: 0.1009 - val_accuracy: 0.6562 - val_loss: 1.7068\n",
            "Epoch 98/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9815 - loss: 0.0628 - val_accuracy: 0.6493 - val_loss: 1.7455\n",
            "Epoch 99/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9682 - loss: 0.0981 - val_accuracy: 0.6493 - val_loss: 1.6951\n",
            "Epoch 100/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9664 - loss: 0.1409 - val_accuracy: 0.6389 - val_loss: 1.6357\n",
            "Epoch 101/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9683 - loss: 0.0923 - val_accuracy: 0.6424 - val_loss: 1.5873\n",
            "Epoch 102/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9679 - loss: 0.0896 - val_accuracy: 0.6701 - val_loss: 1.6339\n",
            "Epoch 103/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9617 - loss: 0.1108 - val_accuracy: 0.6562 - val_loss: 1.6830\n",
            "Epoch 104/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9598 - loss: 0.1110 - val_accuracy: 0.6389 - val_loss: 1.6295\n",
            "Epoch 105/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9540 - loss: 0.1258 - val_accuracy: 0.6250 - val_loss: 1.6440\n",
            "Epoch 106/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9770 - loss: 0.0846 - val_accuracy: 0.6285 - val_loss: 1.7256\n",
            "Epoch 107/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9781 - loss: 0.0756 - val_accuracy: 0.6458 - val_loss: 1.7142\n",
            "Epoch 108/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9624 - loss: 0.1065 - val_accuracy: 0.6424 - val_loss: 1.7339\n",
            "Epoch 109/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9734 - loss: 0.0834 - val_accuracy: 0.6458 - val_loss: 1.7116\n",
            "Epoch 110/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9697 - loss: 0.0873 - val_accuracy: 0.6493 - val_loss: 1.7337\n",
            "Epoch 111/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9634 - loss: 0.0950 - val_accuracy: 0.6424 - val_loss: 1.7351\n",
            "Epoch 112/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9593 - loss: 0.1133 - val_accuracy: 0.6458 - val_loss: 1.8129\n",
            "Epoch 113/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9708 - loss: 0.0887 - val_accuracy: 0.6493 - val_loss: 1.8069\n",
            "Epoch 114/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9593 - loss: 0.1344 - val_accuracy: 0.6528 - val_loss: 1.8021\n",
            "Epoch 115/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9751 - loss: 0.0734 - val_accuracy: 0.6701 - val_loss: 1.7139\n",
            "Epoch 116/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9580 - loss: 0.1125 - val_accuracy: 0.6667 - val_loss: 1.7885\n",
            "Epoch 117/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9663 - loss: 0.1042 - val_accuracy: 0.6701 - val_loss: 1.7893\n",
            "Epoch 118/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9574 - loss: 0.1033 - val_accuracy: 0.6597 - val_loss: 1.7268\n",
            "Epoch 119/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9594 - loss: 0.1139 - val_accuracy: 0.6597 - val_loss: 1.6912\n",
            "Epoch 120/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9739 - loss: 0.0838 - val_accuracy: 0.6667 - val_loss: 1.7217\n",
            "Epoch 121/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9805 - loss: 0.0672 - val_accuracy: 0.6806 - val_loss: 1.7180\n",
            "Epoch 122/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9686 - loss: 0.0803 - val_accuracy: 0.6806 - val_loss: 1.7169\n",
            "Epoch 123/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9508 - loss: 0.1280 - val_accuracy: 0.6979 - val_loss: 1.7132\n",
            "Epoch 124/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9698 - loss: 0.0879 - val_accuracy: 0.6806 - val_loss: 1.7654\n",
            "Epoch 125/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9701 - loss: 0.0867 - val_accuracy: 0.6528 - val_loss: 1.7674\n",
            "Epoch 126/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9675 - loss: 0.0812 - val_accuracy: 0.6493 - val_loss: 1.8297\n",
            "Epoch 127/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9680 - loss: 0.1016 - val_accuracy: 0.6424 - val_loss: 1.8704\n",
            "Epoch 128/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9628 - loss: 0.1182 - val_accuracy: 0.6528 - val_loss: 1.7522\n",
            "Epoch 129/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9559 - loss: 0.1220 - val_accuracy: 0.6562 - val_loss: 1.7369\n",
            "Epoch 130/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9621 - loss: 0.0980 - val_accuracy: 0.6771 - val_loss: 1.6886\n",
            "Epoch 131/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9649 - loss: 0.1034 - val_accuracy: 0.6597 - val_loss: 1.6333\n",
            "Epoch 132/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9723 - loss: 0.0950 - val_accuracy: 0.6424 - val_loss: 1.6767\n",
            "Epoch 133/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9576 - loss: 0.1114 - val_accuracy: 0.6528 - val_loss: 1.6592\n",
            "Epoch 134/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9741 - loss: 0.0890 - val_accuracy: 0.6667 - val_loss: 1.6255\n",
            "Epoch 135/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9659 - loss: 0.0996 - val_accuracy: 0.6562 - val_loss: 1.6928\n",
            "Epoch 136/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9567 - loss: 0.1381 - val_accuracy: 0.6389 - val_loss: 1.7821\n",
            "Epoch 137/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9705 - loss: 0.1054 - val_accuracy: 0.6424 - val_loss: 1.7333\n",
            "Epoch 138/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9706 - loss: 0.0837 - val_accuracy: 0.6667 - val_loss: 1.7142\n",
            "Epoch 139/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9543 - loss: 0.1283 - val_accuracy: 0.6667 - val_loss: 1.6885\n",
            "Epoch 140/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9775 - loss: 0.0723 - val_accuracy: 0.6597 - val_loss: 1.7083\n",
            "Epoch 141/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9697 - loss: 0.1008 - val_accuracy: 0.6562 - val_loss: 1.7248\n",
            "Epoch 142/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9679 - loss: 0.0884 - val_accuracy: 0.6389 - val_loss: 1.7128\n",
            "Epoch 143/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9719 - loss: 0.0736 - val_accuracy: 0.6424 - val_loss: 1.6486\n",
            "Epoch 144/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9696 - loss: 0.0750 - val_accuracy: 0.6528 - val_loss: 1.7218\n",
            "Epoch 145/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9662 - loss: 0.0851 - val_accuracy: 0.6424 - val_loss: 1.7384\n",
            "Epoch 146/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9611 - loss: 0.1037 - val_accuracy: 0.6424 - val_loss: 1.7705\n",
            "Epoch 147/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9619 - loss: 0.1152 - val_accuracy: 0.6562 - val_loss: 1.6830\n",
            "Epoch 148/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9724 - loss: 0.0627 - val_accuracy: 0.6597 - val_loss: 1.6009\n",
            "Epoch 149/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9469 - loss: 0.1275 - val_accuracy: 0.6701 - val_loss: 1.6271\n",
            "Epoch 150/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9709 - loss: 0.0884 - val_accuracy: 0.6944 - val_loss: 1.6170\n",
            "Epoch 151/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9730 - loss: 0.0798 - val_accuracy: 0.6806 - val_loss: 1.7210\n",
            "Epoch 152/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9604 - loss: 0.1111 - val_accuracy: 0.6701 - val_loss: 1.7635\n",
            "Epoch 153/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9780 - loss: 0.0741 - val_accuracy: 0.6389 - val_loss: 1.7995\n",
            "Epoch 154/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9804 - loss: 0.0734 - val_accuracy: 0.6354 - val_loss: 1.8391\n",
            "Epoch 155/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9641 - loss: 0.0919 - val_accuracy: 0.6701 - val_loss: 1.6697\n",
            "Epoch 156/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9689 - loss: 0.1018 - val_accuracy: 0.6319 - val_loss: 1.7702\n",
            "Epoch 157/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9724 - loss: 0.0940 - val_accuracy: 0.6354 - val_loss: 1.8372\n",
            "Epoch 158/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9652 - loss: 0.0968 - val_accuracy: 0.6701 - val_loss: 1.8537\n",
            "Epoch 159/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9581 - loss: 0.0959 - val_accuracy: 0.6319 - val_loss: 1.7887\n",
            "Epoch 160/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9684 - loss: 0.0859 - val_accuracy: 0.6458 - val_loss: 1.7284\n",
            "Epoch 161/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9720 - loss: 0.0939 - val_accuracy: 0.6458 - val_loss: 1.8177\n",
            "Epoch 162/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9745 - loss: 0.0718 - val_accuracy: 0.6354 - val_loss: 1.8102\n",
            "Epoch 163/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9738 - loss: 0.0775 - val_accuracy: 0.6458 - val_loss: 1.7911\n",
            "Epoch 164/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9842 - loss: 0.0459 - val_accuracy: 0.6632 - val_loss: 1.7980\n",
            "Epoch 165/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9721 - loss: 0.0774 - val_accuracy: 0.6458 - val_loss: 1.7593\n",
            "Epoch 166/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9666 - loss: 0.0792 - val_accuracy: 0.6424 - val_loss: 1.6996\n",
            "Epoch 167/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9781 - loss: 0.0749 - val_accuracy: 0.6493 - val_loss: 1.6596\n",
            "Epoch 168/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9554 - loss: 0.1172 - val_accuracy: 0.6667 - val_loss: 1.6332\n",
            "Epoch 169/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9689 - loss: 0.0993 - val_accuracy: 0.6458 - val_loss: 1.6923\n",
            "Epoch 170/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9656 - loss: 0.0924 - val_accuracy: 0.6424 - val_loss: 1.7715\n",
            "Epoch 171/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9734 - loss: 0.0870 - val_accuracy: 0.6632 - val_loss: 1.7648\n",
            "Epoch 172/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9616 - loss: 0.1101 - val_accuracy: 0.6528 - val_loss: 1.7695\n",
            "Epoch 173/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9634 - loss: 0.0953 - val_accuracy: 0.6597 - val_loss: 1.7968\n",
            "Epoch 174/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9554 - loss: 0.1120 - val_accuracy: 0.6319 - val_loss: 1.8451\n",
            "Epoch 175/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9624 - loss: 0.0908 - val_accuracy: 0.6285 - val_loss: 1.8547\n",
            "Epoch 176/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9772 - loss: 0.0642 - val_accuracy: 0.6493 - val_loss: 1.8264\n",
            "Epoch 177/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9742 - loss: 0.0852 - val_accuracy: 0.6597 - val_loss: 1.7032\n",
            "Epoch 178/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9522 - loss: 0.1082 - val_accuracy: 0.6458 - val_loss: 1.6392\n",
            "Epoch 179/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9593 - loss: 0.0935 - val_accuracy: 0.6632 - val_loss: 1.6839\n",
            "Epoch 180/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9606 - loss: 0.1193 - val_accuracy: 0.6667 - val_loss: 1.7609\n",
            "Epoch 181/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9715 - loss: 0.0855 - val_accuracy: 0.6736 - val_loss: 1.8292\n",
            "Epoch 182/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9653 - loss: 0.1010 - val_accuracy: 0.6042 - val_loss: 1.8610\n",
            "Epoch 183/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9677 - loss: 0.0947 - val_accuracy: 0.6424 - val_loss: 1.8115\n",
            "Epoch 184/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9684 - loss: 0.0864 - val_accuracy: 0.6528 - val_loss: 1.7838\n",
            "Epoch 185/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9655 - loss: 0.1015 - val_accuracy: 0.6285 - val_loss: 1.8125\n",
            "Epoch 186/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9529 - loss: 0.1344 - val_accuracy: 0.6354 - val_loss: 1.9400\n",
            "Epoch 187/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9722 - loss: 0.0787 - val_accuracy: 0.6250 - val_loss: 1.9343\n",
            "Epoch 188/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9757 - loss: 0.0707 - val_accuracy: 0.6632 - val_loss: 1.9037\n",
            "Epoch 189/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9656 - loss: 0.0831 - val_accuracy: 0.6562 - val_loss: 1.8478\n",
            "Epoch 190/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9709 - loss: 0.0804 - val_accuracy: 0.6493 - val_loss: 1.7999\n",
            "Epoch 191/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9651 - loss: 0.0978 - val_accuracy: 0.6389 - val_loss: 1.8423\n",
            "Epoch 192/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9651 - loss: 0.1004 - val_accuracy: 0.6424 - val_loss: 1.8142\n",
            "Epoch 193/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9830 - loss: 0.0631 - val_accuracy: 0.6597 - val_loss: 1.8584\n",
            "Epoch 194/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9710 - loss: 0.1027 - val_accuracy: 0.6562 - val_loss: 1.8946\n",
            "Epoch 195/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9635 - loss: 0.1236 - val_accuracy: 0.6319 - val_loss: 1.8910\n",
            "Epoch 196/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9608 - loss: 0.1155 - val_accuracy: 0.6215 - val_loss: 2.0384\n",
            "Epoch 197/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9596 - loss: 0.1227 - val_accuracy: 0.6215 - val_loss: 2.0876\n",
            "Epoch 198/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9573 - loss: 0.1131 - val_accuracy: 0.6562 - val_loss: 1.8069\n",
            "Epoch 199/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9663 - loss: 0.1042 - val_accuracy: 0.6493 - val_loss: 1.7141\n",
            "Epoch 200/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9689 - loss: 0.0986 - val_accuracy: 0.6319 - val_loss: 1.7343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hostory=model.fit(x_train, y_train_sparse, epochs=700,batch_size=16, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMWKaCEbAMdh",
        "outputId": "cb964f9e-fa4d-4495-989d-1708accf3cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.6630 - loss: 1.5160 - val_accuracy: 0.3715 - val_loss: 2.7161\n",
            "Epoch 2/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4290 - loss: 1.6942 - val_accuracy: 0.4549 - val_loss: 1.6262\n",
            "Epoch 3/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5170 - loss: 1.3621 - val_accuracy: 0.5069 - val_loss: 1.3467\n",
            "Epoch 4/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5728 - loss: 1.1575 - val_accuracy: 0.5347 - val_loss: 1.2159\n",
            "Epoch 5/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5940 - loss: 1.1170 - val_accuracy: 0.5833 - val_loss: 1.2490\n",
            "Epoch 6/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6776 - loss: 0.9586 - val_accuracy: 0.5799 - val_loss: 1.2494\n",
            "Epoch 7/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7048 - loss: 0.8819 - val_accuracy: 0.5799 - val_loss: 1.2134\n",
            "Epoch 8/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6559 - loss: 0.9288 - val_accuracy: 0.5729 - val_loss: 1.2609\n",
            "Epoch 9/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6728 - loss: 0.8691 - val_accuracy: 0.6111 - val_loss: 1.1406\n",
            "Epoch 10/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7317 - loss: 0.7891 - val_accuracy: 0.6042 - val_loss: 1.1826\n",
            "Epoch 11/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7047 - loss: 0.8512 - val_accuracy: 0.5729 - val_loss: 1.1731\n",
            "Epoch 12/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6925 - loss: 0.8476 - val_accuracy: 0.5903 - val_loss: 1.1709\n",
            "Epoch 13/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6923 - loss: 0.8483 - val_accuracy: 0.5799 - val_loss: 1.1937\n",
            "Epoch 14/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7122 - loss: 0.8180 - val_accuracy: 0.5729 - val_loss: 1.1563\n",
            "Epoch 15/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7115 - loss: 0.8440 - val_accuracy: 0.6076 - val_loss: 1.1760\n",
            "Epoch 16/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7422 - loss: 0.7396 - val_accuracy: 0.5590 - val_loss: 1.2900\n",
            "Epoch 17/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7402 - loss: 0.7861 - val_accuracy: 0.6076 - val_loss: 1.1730\n",
            "Epoch 18/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7448 - loss: 0.6983 - val_accuracy: 0.5938 - val_loss: 1.2917\n",
            "Epoch 19/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6989 - loss: 0.8579 - val_accuracy: 0.5972 - val_loss: 1.2185\n",
            "Epoch 20/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7524 - loss: 0.6986 - val_accuracy: 0.5764 - val_loss: 1.2345\n",
            "Epoch 21/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7297 - loss: 0.7803 - val_accuracy: 0.6076 - val_loss: 1.1445\n",
            "Epoch 22/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7449 - loss: 0.7344 - val_accuracy: 0.5938 - val_loss: 1.1639\n",
            "Epoch 23/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7543 - loss: 0.6832 - val_accuracy: 0.6146 - val_loss: 1.1419\n",
            "Epoch 24/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7417 - loss: 0.7602 - val_accuracy: 0.6111 - val_loss: 1.2116\n",
            "Epoch 25/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7543 - loss: 0.6998 - val_accuracy: 0.6319 - val_loss: 1.0804\n",
            "Epoch 26/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7446 - loss: 0.7510 - val_accuracy: 0.6493 - val_loss: 1.1433\n",
            "Epoch 27/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7459 - loss: 0.7509 - val_accuracy: 0.6597 - val_loss: 1.0982\n",
            "Epoch 28/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7513 - loss: 0.7326 - val_accuracy: 0.5833 - val_loss: 1.2234\n",
            "Epoch 29/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7830 - loss: 0.7046 - val_accuracy: 0.6250 - val_loss: 1.1303\n",
            "Epoch 30/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7596 - loss: 0.7189 - val_accuracy: 0.6146 - val_loss: 1.1726\n",
            "Epoch 31/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7795 - loss: 0.6343 - val_accuracy: 0.6007 - val_loss: 1.1445\n",
            "Epoch 32/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7802 - loss: 0.6392 - val_accuracy: 0.6250 - val_loss: 1.1851\n",
            "Epoch 33/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7665 - loss: 0.6424 - val_accuracy: 0.6111 - val_loss: 1.1403\n",
            "Epoch 34/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7619 - loss: 0.6451 - val_accuracy: 0.5833 - val_loss: 1.2333\n",
            "Epoch 35/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7571 - loss: 0.7084 - val_accuracy: 0.5938 - val_loss: 1.1663\n",
            "Epoch 36/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8033 - loss: 0.5809 - val_accuracy: 0.5938 - val_loss: 1.1636\n",
            "Epoch 37/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7511 - loss: 0.7155 - val_accuracy: 0.6181 - val_loss: 1.1426\n",
            "Epoch 38/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7762 - loss: 0.5988 - val_accuracy: 0.6354 - val_loss: 1.1401\n",
            "Epoch 39/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7933 - loss: 0.6054 - val_accuracy: 0.6146 - val_loss: 1.1087\n",
            "Epoch 40/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7916 - loss: 0.6218 - val_accuracy: 0.6215 - val_loss: 1.1532\n",
            "Epoch 41/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7963 - loss: 0.5799 - val_accuracy: 0.6111 - val_loss: 1.1876\n",
            "Epoch 42/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7789 - loss: 0.6563 - val_accuracy: 0.6007 - val_loss: 1.1740\n",
            "Epoch 43/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7741 - loss: 0.6236 - val_accuracy: 0.6458 - val_loss: 1.1315\n",
            "Epoch 44/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8057 - loss: 0.5687 - val_accuracy: 0.6215 - val_loss: 1.2503\n",
            "Epoch 45/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7873 - loss: 0.5973 - val_accuracy: 0.6007 - val_loss: 1.1704\n",
            "Epoch 46/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7769 - loss: 0.6534 - val_accuracy: 0.6146 - val_loss: 1.2389\n",
            "Epoch 47/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8098 - loss: 0.5905 - val_accuracy: 0.6215 - val_loss: 1.1971\n",
            "Epoch 48/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7797 - loss: 0.6361 - val_accuracy: 0.6354 - val_loss: 1.1464\n",
            "Epoch 49/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7975 - loss: 0.5722 - val_accuracy: 0.5938 - val_loss: 1.3060\n",
            "Epoch 50/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7885 - loss: 0.5717 - val_accuracy: 0.6354 - val_loss: 1.2117\n",
            "Epoch 51/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7695 - loss: 0.6429 - val_accuracy: 0.6250 - val_loss: 1.1928\n",
            "Epoch 52/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7500 - loss: 0.7077 - val_accuracy: 0.6285 - val_loss: 1.1171\n",
            "Epoch 53/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7787 - loss: 0.6041 - val_accuracy: 0.6250 - val_loss: 1.1896\n",
            "Epoch 54/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7802 - loss: 0.6129 - val_accuracy: 0.6076 - val_loss: 1.1219\n",
            "Epoch 55/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8077 - loss: 0.5237 - val_accuracy: 0.6215 - val_loss: 1.2206\n",
            "Epoch 56/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7756 - loss: 0.6119 - val_accuracy: 0.6354 - val_loss: 1.0808\n",
            "Epoch 57/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7877 - loss: 0.6091 - val_accuracy: 0.6528 - val_loss: 1.0871\n",
            "Epoch 58/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7944 - loss: 0.5794 - val_accuracy: 0.6493 - val_loss: 1.0643\n",
            "Epoch 59/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7517 - loss: 0.6971 - val_accuracy: 0.6250 - val_loss: 1.1168\n",
            "Epoch 60/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7941 - loss: 0.6484 - val_accuracy: 0.6042 - val_loss: 1.1965\n",
            "Epoch 61/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7961 - loss: 0.5668 - val_accuracy: 0.6701 - val_loss: 1.0539\n",
            "Epoch 62/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7727 - loss: 0.6218 - val_accuracy: 0.6319 - val_loss: 1.0151\n",
            "Epoch 63/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8142 - loss: 0.5399 - val_accuracy: 0.6215 - val_loss: 1.1616\n",
            "Epoch 64/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7827 - loss: 0.6111 - val_accuracy: 0.6597 - val_loss: 1.0937\n",
            "Epoch 65/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7912 - loss: 0.5738 - val_accuracy: 0.6389 - val_loss: 1.1207\n",
            "Epoch 66/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8062 - loss: 0.5555 - val_accuracy: 0.6493 - val_loss: 1.1144\n",
            "Epoch 67/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7931 - loss: 0.5990 - val_accuracy: 0.6181 - val_loss: 1.1186\n",
            "Epoch 68/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8231 - loss: 0.5377 - val_accuracy: 0.6042 - val_loss: 1.3303\n",
            "Epoch 69/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8071 - loss: 0.5607 - val_accuracy: 0.5799 - val_loss: 1.1850\n",
            "Epoch 70/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7777 - loss: 0.5966 - val_accuracy: 0.6389 - val_loss: 1.0939\n",
            "Epoch 71/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7861 - loss: 0.5709 - val_accuracy: 0.6250 - val_loss: 1.1339\n",
            "Epoch 72/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8102 - loss: 0.5907 - val_accuracy: 0.6285 - val_loss: 1.1470\n",
            "Epoch 73/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8103 - loss: 0.5755 - val_accuracy: 0.6285 - val_loss: 1.1541\n",
            "Epoch 74/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7867 - loss: 0.6176 - val_accuracy: 0.6146 - val_loss: 1.2135\n",
            "Epoch 75/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8393 - loss: 0.4814 - val_accuracy: 0.6076 - val_loss: 1.2822\n",
            "Epoch 76/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8075 - loss: 0.5667 - val_accuracy: 0.6250 - val_loss: 1.2464\n",
            "Epoch 77/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8137 - loss: 0.5373 - val_accuracy: 0.6319 - val_loss: 1.1604\n",
            "Epoch 78/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8256 - loss: 0.4972 - val_accuracy: 0.6285 - val_loss: 1.1451\n",
            "Epoch 79/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8248 - loss: 0.4779 - val_accuracy: 0.6181 - val_loss: 1.1938\n",
            "Epoch 80/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8062 - loss: 0.5544 - val_accuracy: 0.6250 - val_loss: 1.1544\n",
            "Epoch 81/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8175 - loss: 0.5570 - val_accuracy: 0.6597 - val_loss: 1.1002\n",
            "Epoch 82/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7910 - loss: 0.6085 - val_accuracy: 0.6215 - val_loss: 1.1878\n",
            "Epoch 83/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8055 - loss: 0.5673 - val_accuracy: 0.5972 - val_loss: 1.1662\n",
            "Epoch 84/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8193 - loss: 0.5267 - val_accuracy: 0.6354 - val_loss: 1.0876\n",
            "Epoch 85/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8224 - loss: 0.5037 - val_accuracy: 0.6493 - val_loss: 1.0999\n",
            "Epoch 86/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7999 - loss: 0.6435 - val_accuracy: 0.6424 - val_loss: 1.2051\n",
            "Epoch 87/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7903 - loss: 0.5920 - val_accuracy: 0.6181 - val_loss: 1.1647\n",
            "Epoch 88/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8230 - loss: 0.5449 - val_accuracy: 0.6215 - val_loss: 1.1624\n",
            "Epoch 89/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7918 - loss: 0.5608 - val_accuracy: 0.6215 - val_loss: 1.1197\n",
            "Epoch 90/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8093 - loss: 0.5475 - val_accuracy: 0.6111 - val_loss: 1.2004\n",
            "Epoch 91/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8100 - loss: 0.5445 - val_accuracy: 0.6528 - val_loss: 1.1021\n",
            "Epoch 92/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8078 - loss: 0.5876 - val_accuracy: 0.6007 - val_loss: 1.2363\n",
            "Epoch 93/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8103 - loss: 0.5478 - val_accuracy: 0.5833 - val_loss: 1.2910\n",
            "Epoch 94/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7975 - loss: 0.5962 - val_accuracy: 0.5903 - val_loss: 1.3142\n",
            "Epoch 95/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7863 - loss: 0.6409 - val_accuracy: 0.6319 - val_loss: 1.1052\n",
            "Epoch 96/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8338 - loss: 0.5134 - val_accuracy: 0.6632 - val_loss: 1.1052\n",
            "Epoch 97/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8345 - loss: 0.4549 - val_accuracy: 0.6389 - val_loss: 1.1015\n",
            "Epoch 98/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8185 - loss: 0.5101 - val_accuracy: 0.6285 - val_loss: 1.1610\n",
            "Epoch 99/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8037 - loss: 0.5271 - val_accuracy: 0.6285 - val_loss: 1.2029\n",
            "Epoch 100/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7801 - loss: 0.6144 - val_accuracy: 0.6354 - val_loss: 1.1555\n",
            "Epoch 101/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7977 - loss: 0.6412 - val_accuracy: 0.6354 - val_loss: 1.1977\n",
            "Epoch 102/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8181 - loss: 0.5515 - val_accuracy: 0.6285 - val_loss: 1.0978\n",
            "Epoch 103/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8413 - loss: 0.5385 - val_accuracy: 0.6076 - val_loss: 1.2504\n",
            "Epoch 104/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8100 - loss: 0.5484 - val_accuracy: 0.6319 - val_loss: 1.1043\n",
            "Epoch 105/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7974 - loss: 0.5431 - val_accuracy: 0.6424 - val_loss: 1.1574\n",
            "Epoch 106/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8175 - loss: 0.5026 - val_accuracy: 0.6458 - val_loss: 1.1418\n",
            "Epoch 107/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8332 - loss: 0.4649 - val_accuracy: 0.6458 - val_loss: 1.1521\n",
            "Epoch 108/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7983 - loss: 0.5760 - val_accuracy: 0.6215 - val_loss: 1.1318\n",
            "Epoch 109/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.8023 - loss: 0.5180 - val_accuracy: 0.6146 - val_loss: 1.1989\n",
            "Epoch 110/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8132 - loss: 0.5327 - val_accuracy: 0.6319 - val_loss: 1.2698\n",
            "Epoch 111/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8133 - loss: 0.5563 - val_accuracy: 0.6146 - val_loss: 1.1497\n",
            "Epoch 112/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8073 - loss: 0.5312 - val_accuracy: 0.6458 - val_loss: 1.2367\n",
            "Epoch 113/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8074 - loss: 0.5968 - val_accuracy: 0.6319 - val_loss: 1.2005\n",
            "Epoch 114/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8346 - loss: 0.5025 - val_accuracy: 0.6285 - val_loss: 1.1743\n",
            "Epoch 115/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7987 - loss: 0.5810 - val_accuracy: 0.6319 - val_loss: 1.1372\n",
            "Epoch 116/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8025 - loss: 0.5533 - val_accuracy: 0.6319 - val_loss: 1.0883\n",
            "Epoch 117/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8167 - loss: 0.4875 - val_accuracy: 0.6389 - val_loss: 1.1139\n",
            "Epoch 118/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8281 - loss: 0.4903 - val_accuracy: 0.6285 - val_loss: 1.2548\n",
            "Epoch 119/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7957 - loss: 0.5531 - val_accuracy: 0.6285 - val_loss: 1.1766\n",
            "Epoch 120/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8251 - loss: 0.5238 - val_accuracy: 0.6319 - val_loss: 1.1972\n",
            "Epoch 121/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8240 - loss: 0.5092 - val_accuracy: 0.6597 - val_loss: 1.1940\n",
            "Epoch 122/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8407 - loss: 0.4782 - val_accuracy: 0.6389 - val_loss: 1.0986\n",
            "Epoch 123/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8012 - loss: 0.5564 - val_accuracy: 0.5972 - val_loss: 1.3025\n",
            "Epoch 124/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8230 - loss: 0.5549 - val_accuracy: 0.6285 - val_loss: 1.2064\n",
            "Epoch 125/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8253 - loss: 0.4632 - val_accuracy: 0.6562 - val_loss: 1.1438\n",
            "Epoch 126/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8075 - loss: 0.5770 - val_accuracy: 0.6528 - val_loss: 1.0943\n",
            "Epoch 127/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8043 - loss: 0.5845 - val_accuracy: 0.6632 - val_loss: 1.1075\n",
            "Epoch 128/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8391 - loss: 0.4448 - val_accuracy: 0.6493 - val_loss: 1.1653\n",
            "Epoch 129/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8380 - loss: 0.4985 - val_accuracy: 0.6354 - val_loss: 1.1935\n",
            "Epoch 130/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8267 - loss: 0.4838 - val_accuracy: 0.6319 - val_loss: 1.1688\n",
            "Epoch 131/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8357 - loss: 0.4802 - val_accuracy: 0.6562 - val_loss: 1.2009\n",
            "Epoch 132/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7950 - loss: 0.5759 - val_accuracy: 0.6424 - val_loss: 1.2326\n",
            "Epoch 133/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8136 - loss: 0.5223 - val_accuracy: 0.6424 - val_loss: 1.2215\n",
            "Epoch 134/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8122 - loss: 0.5314 - val_accuracy: 0.6285 - val_loss: 1.2616\n",
            "Epoch 135/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8302 - loss: 0.5082 - val_accuracy: 0.6250 - val_loss: 1.2354\n",
            "Epoch 136/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8383 - loss: 0.5083 - val_accuracy: 0.6250 - val_loss: 1.3082\n",
            "Epoch 137/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8458 - loss: 0.4741 - val_accuracy: 0.6146 - val_loss: 1.2920\n",
            "Epoch 138/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8146 - loss: 0.5451 - val_accuracy: 0.6146 - val_loss: 1.2184\n",
            "Epoch 139/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8162 - loss: 0.4812 - val_accuracy: 0.6493 - val_loss: 1.2070\n",
            "Epoch 140/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8575 - loss: 0.4466 - val_accuracy: 0.6146 - val_loss: 1.2347\n",
            "Epoch 141/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8063 - loss: 0.5551 - val_accuracy: 0.6319 - val_loss: 1.1455\n",
            "Epoch 142/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8222 - loss: 0.5209 - val_accuracy: 0.6111 - val_loss: 1.1907\n",
            "Epoch 143/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8383 - loss: 0.4533 - val_accuracy: 0.6458 - val_loss: 1.2047\n",
            "Epoch 144/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8096 - loss: 0.5387 - val_accuracy: 0.6250 - val_loss: 1.2369\n",
            "Epoch 145/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8235 - loss: 0.5441 - val_accuracy: 0.6215 - val_loss: 1.2752\n",
            "Epoch 146/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8349 - loss: 0.4504 - val_accuracy: 0.6528 - val_loss: 1.2363\n",
            "Epoch 147/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8576 - loss: 0.4232 - val_accuracy: 0.6250 - val_loss: 1.1470\n",
            "Epoch 148/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8357 - loss: 0.5006 - val_accuracy: 0.6215 - val_loss: 1.2258\n",
            "Epoch 149/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8493 - loss: 0.4591 - val_accuracy: 0.6146 - val_loss: 1.2982\n",
            "Epoch 150/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8434 - loss: 0.4723 - val_accuracy: 0.6285 - val_loss: 1.2149\n",
            "Epoch 151/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8453 - loss: 0.4692 - val_accuracy: 0.6285 - val_loss: 1.2096\n",
            "Epoch 152/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8415 - loss: 0.4702 - val_accuracy: 0.6319 - val_loss: 1.0804\n",
            "Epoch 153/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8673 - loss: 0.3898 - val_accuracy: 0.6424 - val_loss: 1.1760\n",
            "Epoch 154/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8184 - loss: 0.4802 - val_accuracy: 0.6597 - val_loss: 1.1937\n",
            "Epoch 155/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8436 - loss: 0.4281 - val_accuracy: 0.6458 - val_loss: 1.2826\n",
            "Epoch 156/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8104 - loss: 0.5500 - val_accuracy: 0.6424 - val_loss: 1.2275\n",
            "Epoch 157/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8387 - loss: 0.4625 - val_accuracy: 0.6250 - val_loss: 1.3179\n",
            "Epoch 158/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8278 - loss: 0.5339 - val_accuracy: 0.6667 - val_loss: 1.2124\n",
            "Epoch 159/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8254 - loss: 0.4769 - val_accuracy: 0.6597 - val_loss: 1.2148\n",
            "Epoch 160/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8581 - loss: 0.4227 - val_accuracy: 0.6736 - val_loss: 1.1975\n",
            "Epoch 161/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8184 - loss: 0.5387 - val_accuracy: 0.6319 - val_loss: 1.2896\n",
            "Epoch 162/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8689 - loss: 0.3984 - val_accuracy: 0.6458 - val_loss: 1.1996\n",
            "Epoch 163/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8393 - loss: 0.4675 - val_accuracy: 0.6493 - val_loss: 1.2813\n",
            "Epoch 164/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8327 - loss: 0.4919 - val_accuracy: 0.6215 - val_loss: 1.2892\n",
            "Epoch 165/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8537 - loss: 0.4532 - val_accuracy: 0.6597 - val_loss: 1.2096\n",
            "Epoch 166/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8012 - loss: 0.5455 - val_accuracy: 0.6042 - val_loss: 1.2640\n",
            "Epoch 167/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8449 - loss: 0.4543 - val_accuracy: 0.6319 - val_loss: 1.2594\n",
            "Epoch 168/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8516 - loss: 0.4473 - val_accuracy: 0.6250 - val_loss: 1.2311\n",
            "Epoch 169/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8430 - loss: 0.4651 - val_accuracy: 0.6354 - val_loss: 1.2673\n",
            "Epoch 170/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8146 - loss: 0.5661 - val_accuracy: 0.6319 - val_loss: 1.1826\n",
            "Epoch 171/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8413 - loss: 0.4468 - val_accuracy: 0.6146 - val_loss: 1.2834\n",
            "Epoch 172/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8275 - loss: 0.4975 - val_accuracy: 0.6424 - val_loss: 1.2635\n",
            "Epoch 173/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8271 - loss: 0.4715 - val_accuracy: 0.6562 - val_loss: 1.2459\n",
            "Epoch 174/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8532 - loss: 0.4558 - val_accuracy: 0.6319 - val_loss: 1.2763\n",
            "Epoch 175/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8442 - loss: 0.4653 - val_accuracy: 0.6424 - val_loss: 1.1749\n",
            "Epoch 176/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8455 - loss: 0.4631 - val_accuracy: 0.6146 - val_loss: 1.1869\n",
            "Epoch 177/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8404 - loss: 0.4749 - val_accuracy: 0.6181 - val_loss: 1.2360\n",
            "Epoch 178/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8565 - loss: 0.4817 - val_accuracy: 0.6146 - val_loss: 1.3087\n",
            "Epoch 179/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8558 - loss: 0.4325 - val_accuracy: 0.6528 - val_loss: 1.2426\n",
            "Epoch 180/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8259 - loss: 0.4895 - val_accuracy: 0.6354 - val_loss: 1.1974\n",
            "Epoch 181/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8118 - loss: 0.5639 - val_accuracy: 0.6250 - val_loss: 1.1812\n",
            "Epoch 182/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8590 - loss: 0.4261 - val_accuracy: 0.6493 - val_loss: 1.1562\n",
            "Epoch 183/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8449 - loss: 0.4734 - val_accuracy: 0.6389 - val_loss: 1.1922\n",
            "Epoch 184/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8496 - loss: 0.4352 - val_accuracy: 0.6389 - val_loss: 1.1462\n",
            "Epoch 185/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8489 - loss: 0.4307 - val_accuracy: 0.6493 - val_loss: 1.1577\n",
            "Epoch 186/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8614 - loss: 0.4193 - val_accuracy: 0.6528 - val_loss: 1.1205\n",
            "Epoch 187/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8764 - loss: 0.4089 - val_accuracy: 0.6389 - val_loss: 1.1685\n",
            "Epoch 188/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8430 - loss: 0.4546 - val_accuracy: 0.6319 - val_loss: 1.1577\n",
            "Epoch 189/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8095 - loss: 0.5182 - val_accuracy: 0.6632 - val_loss: 1.1484\n",
            "Epoch 190/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8524 - loss: 0.4171 - val_accuracy: 0.6319 - val_loss: 1.1685\n",
            "Epoch 191/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8413 - loss: 0.4737 - val_accuracy: 0.6215 - val_loss: 1.2049\n",
            "Epoch 192/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8458 - loss: 0.4563 - val_accuracy: 0.6181 - val_loss: 1.1792\n",
            "Epoch 193/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8300 - loss: 0.4822 - val_accuracy: 0.6458 - val_loss: 1.1997\n",
            "Epoch 194/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8583 - loss: 0.3942 - val_accuracy: 0.6319 - val_loss: 1.1700\n",
            "Epoch 195/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8561 - loss: 0.4236 - val_accuracy: 0.6181 - val_loss: 1.1887\n",
            "Epoch 196/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8711 - loss: 0.3960 - val_accuracy: 0.6597 - val_loss: 1.1473\n",
            "Epoch 197/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8365 - loss: 0.5026 - val_accuracy: 0.6632 - val_loss: 1.1125\n",
            "Epoch 198/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8437 - loss: 0.5113 - val_accuracy: 0.6701 - val_loss: 1.1561\n",
            "Epoch 199/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8727 - loss: 0.3583 - val_accuracy: 0.6458 - val_loss: 1.1716\n",
            "Epoch 200/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.8530 - loss: 0.4487 - val_accuracy: 0.6181 - val_loss: 1.2641\n",
            "Epoch 201/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8446 - loss: 0.4471 - val_accuracy: 0.6562 - val_loss: 1.1704\n",
            "Epoch 202/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8470 - loss: 0.4586 - val_accuracy: 0.6701 - val_loss: 1.1151\n",
            "Epoch 203/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8270 - loss: 0.4654 - val_accuracy: 0.6319 - val_loss: 1.1814\n",
            "Epoch 204/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8439 - loss: 0.4747 - val_accuracy: 0.6285 - val_loss: 1.1531\n",
            "Epoch 205/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8318 - loss: 0.4461 - val_accuracy: 0.6319 - val_loss: 1.1997\n",
            "Epoch 206/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8383 - loss: 0.4714 - val_accuracy: 0.6493 - val_loss: 1.1944\n",
            "Epoch 207/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8433 - loss: 0.4446 - val_accuracy: 0.6354 - val_loss: 1.2164\n",
            "Epoch 208/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8128 - loss: 0.5097 - val_accuracy: 0.6389 - val_loss: 1.1540\n",
            "Epoch 209/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8677 - loss: 0.3972 - val_accuracy: 0.6632 - val_loss: 1.1511\n",
            "Epoch 210/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8314 - loss: 0.4754 - val_accuracy: 0.6181 - val_loss: 1.2391\n",
            "Epoch 211/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8506 - loss: 0.4541 - val_accuracy: 0.6667 - val_loss: 1.1904\n",
            "Epoch 212/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8478 - loss: 0.4668 - val_accuracy: 0.6215 - val_loss: 1.1856\n",
            "Epoch 213/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8401 - loss: 0.4951 - val_accuracy: 0.6215 - val_loss: 1.2523\n",
            "Epoch 214/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8451 - loss: 0.4544 - val_accuracy: 0.6528 - val_loss: 1.2051\n",
            "Epoch 215/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8335 - loss: 0.5041 - val_accuracy: 0.6111 - val_loss: 1.2365\n",
            "Epoch 216/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8367 - loss: 0.4908 - val_accuracy: 0.6181 - val_loss: 1.1781\n",
            "Epoch 217/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8756 - loss: 0.3908 - val_accuracy: 0.6424 - val_loss: 1.2512\n",
            "Epoch 218/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8658 - loss: 0.3837 - val_accuracy: 0.6493 - val_loss: 1.2198\n",
            "Epoch 219/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8413 - loss: 0.4666 - val_accuracy: 0.6493 - val_loss: 1.1759\n",
            "Epoch 220/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8834 - loss: 0.3685 - val_accuracy: 0.6389 - val_loss: 1.1988\n",
            "Epoch 221/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8775 - loss: 0.3707 - val_accuracy: 0.6458 - val_loss: 1.2604\n",
            "Epoch 222/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8500 - loss: 0.4439 - val_accuracy: 0.6632 - val_loss: 1.2226\n",
            "Epoch 223/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8460 - loss: 0.5031 - val_accuracy: 0.6528 - val_loss: 1.1683\n",
            "Epoch 224/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8768 - loss: 0.3785 - val_accuracy: 0.6354 - val_loss: 1.1948\n",
            "Epoch 225/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8554 - loss: 0.4321 - val_accuracy: 0.6354 - val_loss: 1.1857\n",
            "Epoch 226/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8547 - loss: 0.4123 - val_accuracy: 0.6285 - val_loss: 1.1790\n",
            "Epoch 227/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8462 - loss: 0.4276 - val_accuracy: 0.6667 - val_loss: 1.1711\n",
            "Epoch 228/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8517 - loss: 0.4462 - val_accuracy: 0.6597 - val_loss: 1.1513\n",
            "Epoch 229/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8785 - loss: 0.3729 - val_accuracy: 0.6389 - val_loss: 1.2006\n",
            "Epoch 230/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8594 - loss: 0.4254 - val_accuracy: 0.6319 - val_loss: 1.2118\n",
            "Epoch 231/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8614 - loss: 0.4143 - val_accuracy: 0.6354 - val_loss: 1.2788\n",
            "Epoch 232/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8369 - loss: 0.4640 - val_accuracy: 0.6319 - val_loss: 1.2556\n",
            "Epoch 233/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8334 - loss: 0.4950 - val_accuracy: 0.6250 - val_loss: 1.2334\n",
            "Epoch 234/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8344 - loss: 0.4852 - val_accuracy: 0.6354 - val_loss: 1.2142\n",
            "Epoch 235/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8356 - loss: 0.4789 - val_accuracy: 0.6458 - val_loss: 1.1539\n",
            "Epoch 236/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8416 - loss: 0.4632 - val_accuracy: 0.6667 - val_loss: 1.1774\n",
            "Epoch 237/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8497 - loss: 0.4846 - val_accuracy: 0.6424 - val_loss: 1.2510\n",
            "Epoch 238/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8685 - loss: 0.4116 - val_accuracy: 0.6493 - val_loss: 1.2550\n",
            "Epoch 239/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8785 - loss: 0.3956 - val_accuracy: 0.6319 - val_loss: 1.2695\n",
            "Epoch 240/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8813 - loss: 0.3599 - val_accuracy: 0.6319 - val_loss: 1.2545\n",
            "Epoch 241/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8325 - loss: 0.5144 - val_accuracy: 0.6354 - val_loss: 1.2660\n",
            "Epoch 242/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8609 - loss: 0.4151 - val_accuracy: 0.6319 - val_loss: 1.2934\n",
            "Epoch 243/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8500 - loss: 0.4044 - val_accuracy: 0.6146 - val_loss: 1.2621\n",
            "Epoch 244/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8207 - loss: 0.5276 - val_accuracy: 0.6389 - val_loss: 1.2316\n",
            "Epoch 245/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8660 - loss: 0.3990 - val_accuracy: 0.6389 - val_loss: 1.1780\n",
            "Epoch 246/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8542 - loss: 0.4486 - val_accuracy: 0.6562 - val_loss: 1.0962\n",
            "Epoch 247/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8627 - loss: 0.4213 - val_accuracy: 0.6285 - val_loss: 1.1429\n",
            "Epoch 248/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8501 - loss: 0.4473 - val_accuracy: 0.6736 - val_loss: 1.1604\n",
            "Epoch 249/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8329 - loss: 0.4852 - val_accuracy: 0.6389 - val_loss: 1.1596\n",
            "Epoch 250/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8665 - loss: 0.4308 - val_accuracy: 0.6354 - val_loss: 1.1706\n",
            "Epoch 251/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8525 - loss: 0.4450 - val_accuracy: 0.6319 - val_loss: 1.2192\n",
            "Epoch 252/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8853 - loss: 0.3537 - val_accuracy: 0.6458 - val_loss: 1.3662\n",
            "Epoch 253/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8374 - loss: 0.4327 - val_accuracy: 0.6389 - val_loss: 1.2734\n",
            "Epoch 254/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8377 - loss: 0.4589 - val_accuracy: 0.6424 - val_loss: 1.2291\n",
            "Epoch 255/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8498 - loss: 0.4255 - val_accuracy: 0.6424 - val_loss: 1.2286\n",
            "Epoch 256/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8570 - loss: 0.4434 - val_accuracy: 0.6597 - val_loss: 1.2479\n",
            "Epoch 257/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8447 - loss: 0.4123 - val_accuracy: 0.6389 - val_loss: 1.2383\n",
            "Epoch 258/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8546 - loss: 0.4596 - val_accuracy: 0.6285 - val_loss: 1.2075\n",
            "Epoch 259/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8573 - loss: 0.4032 - val_accuracy: 0.6424 - val_loss: 1.1955\n",
            "Epoch 260/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8701 - loss: 0.3789 - val_accuracy: 0.6458 - val_loss: 1.1764\n",
            "Epoch 261/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8592 - loss: 0.4545 - val_accuracy: 0.6250 - val_loss: 1.2118\n",
            "Epoch 262/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8406 - loss: 0.4560 - val_accuracy: 0.6424 - val_loss: 1.2071\n",
            "Epoch 263/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8767 - loss: 0.3664 - val_accuracy: 0.6424 - val_loss: 1.1575\n",
            "Epoch 264/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8542 - loss: 0.4080 - val_accuracy: 0.6424 - val_loss: 1.1824\n",
            "Epoch 265/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8435 - loss: 0.4480 - val_accuracy: 0.6354 - val_loss: 1.2325\n",
            "Epoch 266/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8564 - loss: 0.4142 - val_accuracy: 0.6215 - val_loss: 1.2276\n",
            "Epoch 267/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8190 - loss: 0.5057 - val_accuracy: 0.6458 - val_loss: 1.1871\n",
            "Epoch 268/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8604 - loss: 0.3899 - val_accuracy: 0.6458 - val_loss: 1.1792\n",
            "Epoch 269/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8650 - loss: 0.3999 - val_accuracy: 0.6319 - val_loss: 1.2122\n",
            "Epoch 270/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8474 - loss: 0.4500 - val_accuracy: 0.6250 - val_loss: 1.2405\n",
            "Epoch 271/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8700 - loss: 0.4125 - val_accuracy: 0.6458 - val_loss: 1.2337\n",
            "Epoch 272/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8365 - loss: 0.4702 - val_accuracy: 0.6354 - val_loss: 1.2717\n",
            "Epoch 273/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8819 - loss: 0.3759 - val_accuracy: 0.6458 - val_loss: 1.2105\n",
            "Epoch 274/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8603 - loss: 0.4101 - val_accuracy: 0.6493 - val_loss: 1.2525\n",
            "Epoch 275/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8719 - loss: 0.3998 - val_accuracy: 0.6250 - val_loss: 1.2981\n",
            "Epoch 276/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8172 - loss: 0.5610 - val_accuracy: 0.6389 - val_loss: 1.2204\n",
            "Epoch 277/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8528 - loss: 0.4353 - val_accuracy: 0.6528 - val_loss: 1.2003\n",
            "Epoch 278/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8668 - loss: 0.4151 - val_accuracy: 0.6528 - val_loss: 1.1507\n",
            "Epoch 279/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8691 - loss: 0.3836 - val_accuracy: 0.6701 - val_loss: 1.1194\n",
            "Epoch 280/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8591 - loss: 0.4093 - val_accuracy: 0.6771 - val_loss: 1.1431\n",
            "Epoch 281/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9051 - loss: 0.3312 - val_accuracy: 0.6667 - val_loss: 1.1976\n",
            "Epoch 282/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8509 - loss: 0.4422 - val_accuracy: 0.6771 - val_loss: 1.1956\n",
            "Epoch 283/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8801 - loss: 0.3594 - val_accuracy: 0.6528 - val_loss: 1.2461\n",
            "Epoch 284/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8537 - loss: 0.4029 - val_accuracy: 0.6632 - val_loss: 1.1761\n",
            "Epoch 285/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8517 - loss: 0.3950 - val_accuracy: 0.6667 - val_loss: 1.2580\n",
            "Epoch 286/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8824 - loss: 0.3887 - val_accuracy: 0.6667 - val_loss: 1.1912\n",
            "Epoch 287/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8400 - loss: 0.4830 - val_accuracy: 0.6667 - val_loss: 1.1646\n",
            "Epoch 288/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8615 - loss: 0.3793 - val_accuracy: 0.6910 - val_loss: 1.1437\n",
            "Epoch 289/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8557 - loss: 0.4301 - val_accuracy: 0.6354 - val_loss: 1.2603\n",
            "Epoch 290/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8621 - loss: 0.4406 - val_accuracy: 0.6736 - val_loss: 1.2192\n",
            "Epoch 291/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8429 - loss: 0.4627 - val_accuracy: 0.6806 - val_loss: 1.1628\n",
            "Epoch 292/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8487 - loss: 0.4031 - val_accuracy: 0.6701 - val_loss: 1.1791\n",
            "Epoch 293/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8475 - loss: 0.4833 - val_accuracy: 0.6354 - val_loss: 1.2184\n",
            "Epoch 294/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8429 - loss: 0.4586 - val_accuracy: 0.6528 - val_loss: 1.2351\n",
            "Epoch 295/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8537 - loss: 0.4190 - val_accuracy: 0.6354 - val_loss: 1.2841\n",
            "Epoch 296/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8534 - loss: 0.4042 - val_accuracy: 0.6458 - val_loss: 1.2918\n",
            "Epoch 297/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8608 - loss: 0.4203 - val_accuracy: 0.6458 - val_loss: 1.2825\n",
            "Epoch 298/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8530 - loss: 0.4345 - val_accuracy: 0.6389 - val_loss: 1.2298\n",
            "Epoch 299/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8708 - loss: 0.3854 - val_accuracy: 0.6562 - val_loss: 1.2348\n",
            "Epoch 300/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8545 - loss: 0.4148 - val_accuracy: 0.6493 - val_loss: 1.2561\n",
            "Epoch 301/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8507 - loss: 0.4780 - val_accuracy: 0.6667 - val_loss: 1.1699\n",
            "Epoch 302/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8671 - loss: 0.3513 - val_accuracy: 0.6493 - val_loss: 1.1863\n",
            "Epoch 303/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8505 - loss: 0.4398 - val_accuracy: 0.6389 - val_loss: 1.1974\n",
            "Epoch 304/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8532 - loss: 0.3838 - val_accuracy: 0.6458 - val_loss: 1.2347\n",
            "Epoch 305/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8652 - loss: 0.4162 - val_accuracy: 0.6597 - val_loss: 1.2251\n",
            "Epoch 306/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8613 - loss: 0.3677 - val_accuracy: 0.6701 - val_loss: 1.2421\n",
            "Epoch 307/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8895 - loss: 0.3429 - val_accuracy: 0.6250 - val_loss: 1.2606\n",
            "Epoch 308/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8810 - loss: 0.3452 - val_accuracy: 0.6597 - val_loss: 1.2689\n",
            "Epoch 309/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8858 - loss: 0.3391 - val_accuracy: 0.6597 - val_loss: 1.2066\n",
            "Epoch 310/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8613 - loss: 0.4188 - val_accuracy: 0.6458 - val_loss: 1.2112\n",
            "Epoch 311/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8517 - loss: 0.3740 - val_accuracy: 0.6597 - val_loss: 1.1941\n",
            "Epoch 312/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8366 - loss: 0.4797 - val_accuracy: 0.6458 - val_loss: 1.2366\n",
            "Epoch 313/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8576 - loss: 0.4065 - val_accuracy: 0.6667 - val_loss: 1.1150\n",
            "Epoch 314/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8645 - loss: 0.3696 - val_accuracy: 0.6458 - val_loss: 1.1784\n",
            "Epoch 315/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8488 - loss: 0.4416 - val_accuracy: 0.6562 - val_loss: 1.2015\n",
            "Epoch 316/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8708 - loss: 0.3790 - val_accuracy: 0.6736 - val_loss: 1.2115\n",
            "Epoch 317/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8666 - loss: 0.3691 - val_accuracy: 0.6667 - val_loss: 1.2013\n",
            "Epoch 318/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8717 - loss: 0.3695 - val_accuracy: 0.6424 - val_loss: 1.3119\n",
            "Epoch 319/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8560 - loss: 0.4144 - val_accuracy: 0.6632 - val_loss: 1.1746\n",
            "Epoch 320/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8510 - loss: 0.4190 - val_accuracy: 0.6701 - val_loss: 1.1657\n",
            "Epoch 321/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8649 - loss: 0.3737 - val_accuracy: 0.6562 - val_loss: 1.1960\n",
            "Epoch 322/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8919 - loss: 0.3259 - val_accuracy: 0.6562 - val_loss: 1.2352\n",
            "Epoch 323/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8737 - loss: 0.3551 - val_accuracy: 0.6701 - val_loss: 1.2062\n",
            "Epoch 324/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8756 - loss: 0.3598 - val_accuracy: 0.6771 - val_loss: 1.2191\n",
            "Epoch 325/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8678 - loss: 0.4034 - val_accuracy: 0.6215 - val_loss: 1.2421\n",
            "Epoch 326/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8401 - loss: 0.4697 - val_accuracy: 0.6632 - val_loss: 1.1847\n",
            "Epoch 327/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.8604 - loss: 0.4191 - val_accuracy: 0.6493 - val_loss: 1.1431\n",
            "Epoch 328/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8442 - loss: 0.4439 - val_accuracy: 0.6597 - val_loss: 1.1486\n",
            "Epoch 329/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8631 - loss: 0.4117 - val_accuracy: 0.6667 - val_loss: 1.1340\n",
            "Epoch 330/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8548 - loss: 0.4605 - val_accuracy: 0.6771 - val_loss: 1.1671\n",
            "Epoch 331/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9038 - loss: 0.3369 - val_accuracy: 0.6736 - val_loss: 1.1317\n",
            "Epoch 332/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.8821 - loss: 0.3527 - val_accuracy: 0.6667 - val_loss: 1.1694\n",
            "Epoch 333/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.8861 - loss: 0.3461 - val_accuracy: 0.6528 - val_loss: 1.1999\n",
            "Epoch 334/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8908 - loss: 0.3183 - val_accuracy: 0.6562 - val_loss: 1.1536\n",
            "Epoch 335/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8706 - loss: 0.3762 - val_accuracy: 0.6458 - val_loss: 1.1953\n",
            "Epoch 336/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9000 - loss: 0.3317 - val_accuracy: 0.6667 - val_loss: 1.2327\n",
            "Epoch 337/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8675 - loss: 0.3850 - val_accuracy: 0.6493 - val_loss: 1.2969\n",
            "Epoch 338/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8717 - loss: 0.3981 - val_accuracy: 0.6493 - val_loss: 1.2802\n",
            "Epoch 339/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8740 - loss: 0.3668 - val_accuracy: 0.6562 - val_loss: 1.1955\n",
            "Epoch 340/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8488 - loss: 0.4330 - val_accuracy: 0.6354 - val_loss: 1.2125\n",
            "Epoch 341/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8937 - loss: 0.3157 - val_accuracy: 0.6389 - val_loss: 1.1968\n",
            "Epoch 342/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8601 - loss: 0.3715 - val_accuracy: 0.6632 - val_loss: 1.1807\n",
            "Epoch 343/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8642 - loss: 0.3822 - val_accuracy: 0.6458 - val_loss: 1.3047\n",
            "Epoch 344/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8672 - loss: 0.3834 - val_accuracy: 0.6528 - val_loss: 1.2533\n",
            "Epoch 345/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8570 - loss: 0.3833 - val_accuracy: 0.6667 - val_loss: 1.1701\n",
            "Epoch 346/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8726 - loss: 0.3642 - val_accuracy: 0.6632 - val_loss: 1.1795\n",
            "Epoch 347/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8678 - loss: 0.4253 - val_accuracy: 0.6424 - val_loss: 1.2286\n",
            "Epoch 348/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8659 - loss: 0.3946 - val_accuracy: 0.6319 - val_loss: 1.2868\n",
            "Epoch 349/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8955 - loss: 0.3315 - val_accuracy: 0.6562 - val_loss: 1.1705\n",
            "Epoch 350/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8743 - loss: 0.3592 - val_accuracy: 0.6528 - val_loss: 1.3081\n",
            "Epoch 351/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8741 - loss: 0.3455 - val_accuracy: 0.6667 - val_loss: 1.1724\n",
            "Epoch 352/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.8700 - loss: 0.3854 - val_accuracy: 0.6528 - val_loss: 1.1343\n",
            "Epoch 353/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8754 - loss: 0.3269 - val_accuracy: 0.6597 - val_loss: 1.1086\n",
            "Epoch 354/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8927 - loss: 0.3155 - val_accuracy: 0.6701 - val_loss: 1.1121\n",
            "Epoch 355/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8721 - loss: 0.3955 - val_accuracy: 0.6701 - val_loss: 1.1322\n",
            "Epoch 356/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8454 - loss: 0.4301 - val_accuracy: 0.6667 - val_loss: 1.1811\n",
            "Epoch 357/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8726 - loss: 0.3746 - val_accuracy: 0.6736 - val_loss: 1.2175\n",
            "Epoch 358/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8651 - loss: 0.3962 - val_accuracy: 0.6528 - val_loss: 1.1771\n",
            "Epoch 359/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8689 - loss: 0.3919 - val_accuracy: 0.6424 - val_loss: 1.2146\n",
            "Epoch 360/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8758 - loss: 0.3661 - val_accuracy: 0.6667 - val_loss: 1.1109\n",
            "Epoch 361/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8741 - loss: 0.4311 - val_accuracy: 0.6562 - val_loss: 1.1701\n",
            "Epoch 362/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8648 - loss: 0.3693 - val_accuracy: 0.6424 - val_loss: 1.2344\n",
            "Epoch 363/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8924 - loss: 0.3571 - val_accuracy: 0.6528 - val_loss: 1.1941\n",
            "Epoch 364/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8885 - loss: 0.3280 - val_accuracy: 0.6354 - val_loss: 1.2094\n",
            "Epoch 365/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8738 - loss: 0.3790 - val_accuracy: 0.6319 - val_loss: 1.2864\n",
            "Epoch 366/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8862 - loss: 0.3513 - val_accuracy: 0.6319 - val_loss: 1.3180\n",
            "Epoch 367/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8942 - loss: 0.3548 - val_accuracy: 0.6562 - val_loss: 1.3062\n",
            "Epoch 368/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8673 - loss: 0.3634 - val_accuracy: 0.6458 - val_loss: 1.2683\n",
            "Epoch 369/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8592 - loss: 0.4018 - val_accuracy: 0.6319 - val_loss: 1.2709\n",
            "Epoch 370/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8633 - loss: 0.3719 - val_accuracy: 0.6389 - val_loss: 1.2759\n",
            "Epoch 371/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8854 - loss: 0.3539 - val_accuracy: 0.6424 - val_loss: 1.2936\n",
            "Epoch 372/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8906 - loss: 0.3517 - val_accuracy: 0.6354 - val_loss: 1.2654\n",
            "Epoch 373/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8672 - loss: 0.3703 - val_accuracy: 0.6389 - val_loss: 1.2081\n",
            "Epoch 374/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8732 - loss: 0.3555 - val_accuracy: 0.6458 - val_loss: 1.2510\n",
            "Epoch 375/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8903 - loss: 0.3286 - val_accuracy: 0.6354 - val_loss: 1.2297\n",
            "Epoch 376/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8801 - loss: 0.3644 - val_accuracy: 0.6632 - val_loss: 1.2279\n",
            "Epoch 377/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8551 - loss: 0.3908 - val_accuracy: 0.6319 - val_loss: 1.2525\n",
            "Epoch 378/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8890 - loss: 0.3120 - val_accuracy: 0.6597 - val_loss: 1.2209\n",
            "Epoch 379/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8701 - loss: 0.3830 - val_accuracy: 0.6389 - val_loss: 1.3125\n",
            "Epoch 380/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8937 - loss: 0.3015 - val_accuracy: 0.6354 - val_loss: 1.3056\n",
            "Epoch 381/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8902 - loss: 0.3609 - val_accuracy: 0.6458 - val_loss: 1.3065\n",
            "Epoch 382/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8641 - loss: 0.3917 - val_accuracy: 0.6458 - val_loss: 1.3121\n",
            "Epoch 383/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8877 - loss: 0.3581 - val_accuracy: 0.6389 - val_loss: 1.2902\n",
            "Epoch 384/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8746 - loss: 0.3576 - val_accuracy: 0.6354 - val_loss: 1.2772\n",
            "Epoch 385/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8837 - loss: 0.3501 - val_accuracy: 0.6354 - val_loss: 1.2254\n",
            "Epoch 386/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.8650 - loss: 0.3941 - val_accuracy: 0.6562 - val_loss: 1.1885\n",
            "Epoch 387/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8829 - loss: 0.3419 - val_accuracy: 0.6528 - val_loss: 1.2330\n",
            "Epoch 388/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8889 - loss: 0.3090 - val_accuracy: 0.6632 - val_loss: 1.2298\n",
            "Epoch 389/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8770 - loss: 0.3839 - val_accuracy: 0.6458 - val_loss: 1.1931\n",
            "Epoch 390/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8601 - loss: 0.3668 - val_accuracy: 0.6493 - val_loss: 1.2030\n",
            "Epoch 391/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8608 - loss: 0.3924 - val_accuracy: 0.6528 - val_loss: 1.2460\n",
            "Epoch 392/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8861 - loss: 0.3366 - val_accuracy: 0.6285 - val_loss: 1.2603\n",
            "Epoch 393/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8874 - loss: 0.3655 - val_accuracy: 0.6528 - val_loss: 1.2109\n",
            "Epoch 394/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8949 - loss: 0.3135 - val_accuracy: 0.6736 - val_loss: 1.2232\n",
            "Epoch 395/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8944 - loss: 0.3067 - val_accuracy: 0.6354 - val_loss: 1.2300\n",
            "Epoch 396/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.8738 - loss: 0.3304 - val_accuracy: 0.6736 - val_loss: 1.2562\n",
            "Epoch 397/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8783 - loss: 0.3526 - val_accuracy: 0.6389 - val_loss: 1.2593\n",
            "Epoch 398/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8803 - loss: 0.3526 - val_accuracy: 0.6354 - val_loss: 1.2849\n",
            "Epoch 399/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8845 - loss: 0.3473 - val_accuracy: 0.6632 - val_loss: 1.2039\n",
            "Epoch 400/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8722 - loss: 0.3813 - val_accuracy: 0.6528 - val_loss: 1.2470\n",
            "Epoch 401/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8694 - loss: 0.3908 - val_accuracy: 0.6562 - val_loss: 1.1875\n",
            "Epoch 402/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8501 - loss: 0.4243 - val_accuracy: 0.6389 - val_loss: 1.1606\n",
            "Epoch 403/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8666 - loss: 0.3773 - val_accuracy: 0.6632 - val_loss: 1.1925\n",
            "Epoch 404/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8628 - loss: 0.3631 - val_accuracy: 0.6389 - val_loss: 1.2020\n",
            "Epoch 405/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8687 - loss: 0.4295 - val_accuracy: 0.6354 - val_loss: 1.2716\n",
            "Epoch 406/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8676 - loss: 0.3537 - val_accuracy: 0.6389 - val_loss: 1.2414\n",
            "Epoch 407/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8853 - loss: 0.3521 - val_accuracy: 0.6354 - val_loss: 1.2604\n",
            "Epoch 408/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.8628 - loss: 0.3855 - val_accuracy: 0.6562 - val_loss: 1.2212\n",
            "Epoch 409/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.8690 - loss: 0.3842 - val_accuracy: 0.6424 - val_loss: 1.1990\n",
            "Epoch 410/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8594 - loss: 0.4586 - val_accuracy: 0.6736 - val_loss: 1.1319\n",
            "Epoch 411/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8801 - loss: 0.3829 - val_accuracy: 0.6701 - val_loss: 1.1632\n",
            "Epoch 412/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9168 - loss: 0.2888 - val_accuracy: 0.6528 - val_loss: 1.2236\n",
            "Epoch 413/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8922 - loss: 0.2821 - val_accuracy: 0.6667 - val_loss: 1.1812\n",
            "Epoch 414/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8890 - loss: 0.3603 - val_accuracy: 0.6701 - val_loss: 1.1806\n",
            "Epoch 415/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8826 - loss: 0.3641 - val_accuracy: 0.6389 - val_loss: 1.2612\n",
            "Epoch 416/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8764 - loss: 0.3313 - val_accuracy: 0.6389 - val_loss: 1.2453\n",
            "Epoch 417/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8930 - loss: 0.3643 - val_accuracy: 0.6215 - val_loss: 1.2945\n",
            "Epoch 418/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8615 - loss: 0.3672 - val_accuracy: 0.6632 - val_loss: 1.2841\n",
            "Epoch 419/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8527 - loss: 0.4078 - val_accuracy: 0.6285 - val_loss: 1.2630\n",
            "Epoch 420/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8998 - loss: 0.2957 - val_accuracy: 0.6493 - val_loss: 1.1959\n",
            "Epoch 421/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8807 - loss: 0.3234 - val_accuracy: 0.6458 - val_loss: 1.2995\n",
            "Epoch 422/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8783 - loss: 0.3396 - val_accuracy: 0.6424 - val_loss: 1.3143\n",
            "Epoch 423/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8739 - loss: 0.3449 - val_accuracy: 0.6354 - val_loss: 1.2816\n",
            "Epoch 424/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8802 - loss: 0.3580 - val_accuracy: 0.6389 - val_loss: 1.2989\n",
            "Epoch 425/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8691 - loss: 0.3648 - val_accuracy: 0.6597 - val_loss: 1.2835\n",
            "Epoch 426/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9011 - loss: 0.2679 - val_accuracy: 0.6701 - val_loss: 1.2585\n",
            "Epoch 427/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9118 - loss: 0.3038 - val_accuracy: 0.6632 - val_loss: 1.2537\n",
            "Epoch 428/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8934 - loss: 0.3361 - val_accuracy: 0.6424 - val_loss: 1.3463\n",
            "Epoch 429/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8793 - loss: 0.3390 - val_accuracy: 0.6597 - val_loss: 1.2939\n",
            "Epoch 430/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8881 - loss: 0.3079 - val_accuracy: 0.6632 - val_loss: 1.3173\n",
            "Epoch 431/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8915 - loss: 0.3222 - val_accuracy: 0.6493 - val_loss: 1.3434\n",
            "Epoch 432/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8734 - loss: 0.3454 - val_accuracy: 0.6354 - val_loss: 1.3726\n",
            "Epoch 433/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.8632 - loss: 0.3536 - val_accuracy: 0.6424 - val_loss: 1.4327\n",
            "Epoch 434/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9018 - loss: 0.2987 - val_accuracy: 0.6667 - val_loss: 1.3121\n",
            "Epoch 435/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8515 - loss: 0.3551 - val_accuracy: 0.6528 - val_loss: 1.2975\n",
            "Epoch 436/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9121 - loss: 0.2910 - val_accuracy: 0.6597 - val_loss: 1.2628\n",
            "Epoch 437/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8683 - loss: 0.3885 - val_accuracy: 0.6285 - val_loss: 1.2779\n",
            "Epoch 438/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8948 - loss: 0.2885 - val_accuracy: 0.6389 - val_loss: 1.2702\n",
            "Epoch 439/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8903 - loss: 0.3181 - val_accuracy: 0.6285 - val_loss: 1.2510\n",
            "Epoch 440/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8672 - loss: 0.3672 - val_accuracy: 0.6528 - val_loss: 1.3046\n",
            "Epoch 441/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8642 - loss: 0.3809 - val_accuracy: 0.6424 - val_loss: 1.2997\n",
            "Epoch 442/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8954 - loss: 0.3250 - val_accuracy: 0.6319 - val_loss: 1.2387\n",
            "Epoch 443/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8970 - loss: 0.3023 - val_accuracy: 0.6562 - val_loss: 1.2053\n",
            "Epoch 444/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8928 - loss: 0.3134 - val_accuracy: 0.6736 - val_loss: 1.2264\n",
            "Epoch 445/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8716 - loss: 0.3337 - val_accuracy: 0.6632 - val_loss: 1.2211\n",
            "Epoch 446/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8897 - loss: 0.3283 - val_accuracy: 0.6806 - val_loss: 1.1350\n",
            "Epoch 447/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9013 - loss: 0.3566 - val_accuracy: 0.6493 - val_loss: 1.2656\n",
            "Epoch 448/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8498 - loss: 0.3941 - val_accuracy: 0.6771 - val_loss: 1.2803\n",
            "Epoch 449/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9005 - loss: 0.3200 - val_accuracy: 0.6667 - val_loss: 1.2594\n",
            "Epoch 450/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8798 - loss: 0.3543 - val_accuracy: 0.6562 - val_loss: 1.2465\n",
            "Epoch 451/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8793 - loss: 0.3764 - val_accuracy: 0.6493 - val_loss: 1.3149\n",
            "Epoch 452/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8835 - loss: 0.3489 - val_accuracy: 0.6806 - val_loss: 1.1875\n",
            "Epoch 453/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8963 - loss: 0.3628 - val_accuracy: 0.6667 - val_loss: 1.2531\n",
            "Epoch 454/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8884 - loss: 0.3297 - val_accuracy: 0.6493 - val_loss: 1.2164\n",
            "Epoch 455/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8648 - loss: 0.3723 - val_accuracy: 0.6354 - val_loss: 1.2264\n",
            "Epoch 456/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8894 - loss: 0.3189 - val_accuracy: 0.6528 - val_loss: 1.2269\n",
            "Epoch 457/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8810 - loss: 0.3462 - val_accuracy: 0.6562 - val_loss: 1.2635\n",
            "Epoch 458/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8851 - loss: 0.3082 - val_accuracy: 0.6562 - val_loss: 1.2648\n",
            "Epoch 459/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8957 - loss: 0.2951 - val_accuracy: 0.6701 - val_loss: 1.2762\n",
            "Epoch 460/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8929 - loss: 0.3212 - val_accuracy: 0.6597 - val_loss: 1.2481\n",
            "Epoch 461/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8900 - loss: 0.3270 - val_accuracy: 0.6701 - val_loss: 1.1675\n",
            "Epoch 462/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8664 - loss: 0.3775 - val_accuracy: 0.6840 - val_loss: 1.1776\n",
            "Epoch 463/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8779 - loss: 0.3294 - val_accuracy: 0.6632 - val_loss: 1.1721\n",
            "Epoch 464/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8970 - loss: 0.2982 - val_accuracy: 0.6736 - val_loss: 1.1646\n",
            "Epoch 465/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9119 - loss: 0.2966 - val_accuracy: 0.6701 - val_loss: 1.1734\n",
            "Epoch 466/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8977 - loss: 0.3066 - val_accuracy: 0.6840 - val_loss: 1.2262\n",
            "Epoch 467/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8946 - loss: 0.3100 - val_accuracy: 0.6354 - val_loss: 1.2426\n",
            "Epoch 468/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8869 - loss: 0.3380 - val_accuracy: 0.6528 - val_loss: 1.2538\n",
            "Epoch 469/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8882 - loss: 0.3346 - val_accuracy: 0.6528 - val_loss: 1.2824\n",
            "Epoch 470/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8701 - loss: 0.4072 - val_accuracy: 0.6736 - val_loss: 1.1862\n",
            "Epoch 471/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8885 - loss: 0.3695 - val_accuracy: 0.6771 - val_loss: 1.2474\n",
            "Epoch 472/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8814 - loss: 0.3255 - val_accuracy: 0.6632 - val_loss: 1.1393\n",
            "Epoch 473/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8995 - loss: 0.3020 - val_accuracy: 0.6528 - val_loss: 1.1866\n",
            "Epoch 474/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8981 - loss: 0.2922 - val_accuracy: 0.6354 - val_loss: 1.2089\n",
            "Epoch 475/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8867 - loss: 0.3486 - val_accuracy: 0.6424 - val_loss: 1.2149\n",
            "Epoch 476/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8751 - loss: 0.3328 - val_accuracy: 0.6562 - val_loss: 1.2313\n",
            "Epoch 477/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8844 - loss: 0.3247 - val_accuracy: 0.6840 - val_loss: 1.2049\n",
            "Epoch 478/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8859 - loss: 0.3013 - val_accuracy: 0.6736 - val_loss: 1.1537\n",
            "Epoch 479/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8825 - loss: 0.3401 - val_accuracy: 0.6389 - val_loss: 1.2301\n",
            "Epoch 480/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8769 - loss: 0.3381 - val_accuracy: 0.6632 - val_loss: 1.2230\n",
            "Epoch 481/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9029 - loss: 0.2794 - val_accuracy: 0.6771 - val_loss: 1.1692\n",
            "Epoch 482/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8680 - loss: 0.3740 - val_accuracy: 0.6701 - val_loss: 1.2158\n",
            "Epoch 483/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8767 - loss: 0.3657 - val_accuracy: 0.6493 - val_loss: 1.2672\n",
            "Epoch 484/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8820 - loss: 0.3724 - val_accuracy: 0.6736 - val_loss: 1.2440\n",
            "Epoch 485/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8501 - loss: 0.4135 - val_accuracy: 0.6771 - val_loss: 1.2714\n",
            "Epoch 486/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9036 - loss: 0.3041 - val_accuracy: 0.6771 - val_loss: 1.1956\n",
            "Epoch 487/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8882 - loss: 0.3172 - val_accuracy: 0.6597 - val_loss: 1.2057\n",
            "Epoch 488/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8692 - loss: 0.4231 - val_accuracy: 0.6771 - val_loss: 1.2011\n",
            "Epoch 489/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8777 - loss: 0.3562 - val_accuracy: 0.6806 - val_loss: 1.2181\n",
            "Epoch 490/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8866 - loss: 0.3088 - val_accuracy: 0.6597 - val_loss: 1.2010\n",
            "Epoch 491/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8936 - loss: 0.2978 - val_accuracy: 0.6458 - val_loss: 1.1980\n",
            "Epoch 492/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8557 - loss: 0.4069 - val_accuracy: 0.6458 - val_loss: 1.2462\n",
            "Epoch 493/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8796 - loss: 0.3722 - val_accuracy: 0.6562 - val_loss: 1.2399\n",
            "Epoch 494/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8689 - loss: 0.3476 - val_accuracy: 0.6458 - val_loss: 1.3163\n",
            "Epoch 495/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8773 - loss: 0.3728 - val_accuracy: 0.6493 - val_loss: 1.3158\n",
            "Epoch 496/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8843 - loss: 0.3536 - val_accuracy: 0.6597 - val_loss: 1.3296\n",
            "Epoch 497/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8991 - loss: 0.2907 - val_accuracy: 0.6667 - val_loss: 1.2554\n",
            "Epoch 498/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8911 - loss: 0.2964 - val_accuracy: 0.6389 - val_loss: 1.2607\n",
            "Epoch 499/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8893 - loss: 0.3103 - val_accuracy: 0.6597 - val_loss: 1.2855\n",
            "Epoch 500/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8786 - loss: 0.3393 - val_accuracy: 0.6424 - val_loss: 1.2648\n",
            "Epoch 501/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8896 - loss: 0.3230 - val_accuracy: 0.6389 - val_loss: 1.2651\n",
            "Epoch 502/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8562 - loss: 0.4243 - val_accuracy: 0.6458 - val_loss: 1.2420\n",
            "Epoch 503/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8995 - loss: 0.3470 - val_accuracy: 0.6632 - val_loss: 1.2093\n",
            "Epoch 504/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9059 - loss: 0.2798 - val_accuracy: 0.6632 - val_loss: 1.2484\n",
            "Epoch 505/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8819 - loss: 0.3287 - val_accuracy: 0.6701 - val_loss: 1.2339\n",
            "Epoch 506/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8693 - loss: 0.3454 - val_accuracy: 0.6736 - val_loss: 1.1785\n",
            "Epoch 507/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8957 - loss: 0.2955 - val_accuracy: 0.6562 - val_loss: 1.2397\n",
            "Epoch 508/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9048 - loss: 0.2920 - val_accuracy: 0.6458 - val_loss: 1.2150\n",
            "Epoch 509/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9025 - loss: 0.3235 - val_accuracy: 0.6771 - val_loss: 1.1513\n",
            "Epoch 510/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9068 - loss: 0.2740 - val_accuracy: 0.6458 - val_loss: 1.3101\n",
            "Epoch 511/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8792 - loss: 0.3302 - val_accuracy: 0.6458 - val_loss: 1.2893\n",
            "Epoch 512/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8820 - loss: 0.3231 - val_accuracy: 0.6701 - val_loss: 1.2584\n",
            "Epoch 513/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9120 - loss: 0.3008 - val_accuracy: 0.6979 - val_loss: 1.2462\n",
            "Epoch 514/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8943 - loss: 0.3081 - val_accuracy: 0.6667 - val_loss: 1.2864\n",
            "Epoch 515/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9169 - loss: 0.2639 - val_accuracy: 0.6632 - val_loss: 1.3160\n",
            "Epoch 516/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8876 - loss: 0.3400 - val_accuracy: 0.6528 - val_loss: 1.2887\n",
            "Epoch 517/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9026 - loss: 0.3088 - val_accuracy: 0.6424 - val_loss: 1.3210\n",
            "Epoch 518/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8819 - loss: 0.3312 - val_accuracy: 0.6528 - val_loss: 1.3513\n",
            "Epoch 519/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8886 - loss: 0.3338 - val_accuracy: 0.6562 - val_loss: 1.2408\n",
            "Epoch 520/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8794 - loss: 0.3448 - val_accuracy: 0.6771 - val_loss: 1.1878\n",
            "Epoch 521/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9019 - loss: 0.3299 - val_accuracy: 0.6806 - val_loss: 1.2814\n",
            "Epoch 522/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8864 - loss: 0.3277 - val_accuracy: 0.6458 - val_loss: 1.3229\n",
            "Epoch 523/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8951 - loss: 0.3129 - val_accuracy: 0.6285 - val_loss: 1.3292\n",
            "Epoch 524/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8777 - loss: 0.3487 - val_accuracy: 0.6493 - val_loss: 1.2561\n",
            "Epoch 525/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8790 - loss: 0.3439 - val_accuracy: 0.6215 - val_loss: 1.2590\n",
            "Epoch 526/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8897 - loss: 0.2944 - val_accuracy: 0.6319 - val_loss: 1.2883\n",
            "Epoch 527/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8853 - loss: 0.3403 - val_accuracy: 0.6597 - val_loss: 1.2314\n",
            "Epoch 528/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8966 - loss: 0.2948 - val_accuracy: 0.6701 - val_loss: 1.2107\n",
            "Epoch 529/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8772 - loss: 0.3592 - val_accuracy: 0.6597 - val_loss: 1.2401\n",
            "Epoch 530/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8874 - loss: 0.3049 - val_accuracy: 0.6632 - val_loss: 1.2704\n",
            "Epoch 531/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9034 - loss: 0.2671 - val_accuracy: 0.6493 - val_loss: 1.2730\n",
            "Epoch 532/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8874 - loss: 0.3035 - val_accuracy: 0.6458 - val_loss: 1.3562\n",
            "Epoch 533/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8934 - loss: 0.2855 - val_accuracy: 0.6562 - val_loss: 1.2380\n",
            "Epoch 534/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9073 - loss: 0.3047 - val_accuracy: 0.6319 - val_loss: 1.2540\n",
            "Epoch 535/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9078 - loss: 0.3078 - val_accuracy: 0.6562 - val_loss: 1.2964\n",
            "Epoch 536/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8946 - loss: 0.3044 - val_accuracy: 0.6354 - val_loss: 1.2626\n",
            "Epoch 537/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8990 - loss: 0.3436 - val_accuracy: 0.6319 - val_loss: 1.2994\n",
            "Epoch 538/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8988 - loss: 0.3082 - val_accuracy: 0.6389 - val_loss: 1.2244\n",
            "Epoch 539/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8900 - loss: 0.3081 - val_accuracy: 0.6528 - val_loss: 1.2627\n",
            "Epoch 540/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8797 - loss: 0.3489 - val_accuracy: 0.6528 - val_loss: 1.1993\n",
            "Epoch 541/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9060 - loss: 0.3043 - val_accuracy: 0.6632 - val_loss: 1.2239\n",
            "Epoch 542/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8784 - loss: 0.3066 - val_accuracy: 0.6528 - val_loss: 1.2412\n",
            "Epoch 543/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8808 - loss: 0.3403 - val_accuracy: 0.6667 - val_loss: 1.2415\n",
            "Epoch 544/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9021 - loss: 0.2810 - val_accuracy: 0.6389 - val_loss: 1.2319\n",
            "Epoch 545/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8920 - loss: 0.2894 - val_accuracy: 0.6528 - val_loss: 1.2529\n",
            "Epoch 546/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8965 - loss: 0.3138 - val_accuracy: 0.6701 - val_loss: 1.2151\n",
            "Epoch 547/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8904 - loss: 0.2978 - val_accuracy: 0.6458 - val_loss: 1.2417\n",
            "Epoch 548/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8818 - loss: 0.3612 - val_accuracy: 0.6632 - val_loss: 1.2482\n",
            "Epoch 549/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8852 - loss: 0.3663 - val_accuracy: 0.6424 - val_loss: 1.2549\n",
            "Epoch 550/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9071 - loss: 0.2903 - val_accuracy: 0.6528 - val_loss: 1.2421\n",
            "Epoch 551/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8912 - loss: 0.2994 - val_accuracy: 0.6806 - val_loss: 1.2521\n",
            "Epoch 552/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8748 - loss: 0.3754 - val_accuracy: 0.6562 - val_loss: 1.1924\n",
            "Epoch 553/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8883 - loss: 0.3416 - val_accuracy: 0.6458 - val_loss: 1.1806\n",
            "Epoch 554/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8971 - loss: 0.3241 - val_accuracy: 0.6528 - val_loss: 1.1596\n",
            "Epoch 555/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8987 - loss: 0.2852 - val_accuracy: 0.6493 - val_loss: 1.2171\n",
            "Epoch 556/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9030 - loss: 0.3024 - val_accuracy: 0.6875 - val_loss: 1.1669\n",
            "Epoch 557/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8956 - loss: 0.3580 - val_accuracy: 0.6493 - val_loss: 1.1955\n",
            "Epoch 558/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8930 - loss: 0.2947 - val_accuracy: 0.6736 - val_loss: 1.1706\n",
            "Epoch 559/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9112 - loss: 0.2743 - val_accuracy: 0.6840 - val_loss: 1.1995\n",
            "Epoch 560/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9036 - loss: 0.2773 - val_accuracy: 0.6667 - val_loss: 1.2278\n",
            "Epoch 561/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9049 - loss: 0.2792 - val_accuracy: 0.6910 - val_loss: 1.2277\n",
            "Epoch 562/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8804 - loss: 0.3608 - val_accuracy: 0.6736 - val_loss: 1.2072\n",
            "Epoch 563/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8877 - loss: 0.3320 - val_accuracy: 0.6632 - val_loss: 1.1981\n",
            "Epoch 564/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9003 - loss: 0.3001 - val_accuracy: 0.6736 - val_loss: 1.1770\n",
            "Epoch 565/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9029 - loss: 0.3017 - val_accuracy: 0.6389 - val_loss: 1.2443\n",
            "Epoch 566/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8539 - loss: 0.4929 - val_accuracy: 0.6597 - val_loss: 1.1960\n",
            "Epoch 567/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9088 - loss: 0.2749 - val_accuracy: 0.6493 - val_loss: 1.2010\n",
            "Epoch 568/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8985 - loss: 0.2890 - val_accuracy: 0.6597 - val_loss: 1.2434\n",
            "Epoch 569/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9015 - loss: 0.2788 - val_accuracy: 0.6632 - val_loss: 1.2144\n",
            "Epoch 570/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8821 - loss: 0.3655 - val_accuracy: 0.6424 - val_loss: 1.2739\n",
            "Epoch 571/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8622 - loss: 0.3924 - val_accuracy: 0.6701 - val_loss: 1.1833\n",
            "Epoch 572/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9100 - loss: 0.2867 - val_accuracy: 0.6562 - val_loss: 1.2121\n",
            "Epoch 573/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8932 - loss: 0.3238 - val_accuracy: 0.6701 - val_loss: 1.2035\n",
            "Epoch 574/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9141 - loss: 0.2929 - val_accuracy: 0.6771 - val_loss: 1.1727\n",
            "Epoch 575/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8958 - loss: 0.3034 - val_accuracy: 0.6840 - val_loss: 1.1517\n",
            "Epoch 576/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8742 - loss: 0.3573 - val_accuracy: 0.6701 - val_loss: 1.1703\n",
            "Epoch 577/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8795 - loss: 0.3889 - val_accuracy: 0.6528 - val_loss: 1.1755\n",
            "Epoch 578/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8982 - loss: 0.2801 - val_accuracy: 0.6840 - val_loss: 1.1764\n",
            "Epoch 579/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9011 - loss: 0.3150 - val_accuracy: 0.6806 - val_loss: 1.1565\n",
            "Epoch 580/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9174 - loss: 0.2400 - val_accuracy: 0.6840 - val_loss: 1.1533\n",
            "Epoch 581/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9073 - loss: 0.2935 - val_accuracy: 0.6528 - val_loss: 1.2095\n",
            "Epoch 582/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8918 - loss: 0.3212 - val_accuracy: 0.6562 - val_loss: 1.2043\n",
            "Epoch 583/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8902 - loss: 0.3711 - val_accuracy: 0.6875 - val_loss: 1.1858\n",
            "Epoch 584/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9263 - loss: 0.2432 - val_accuracy: 0.6528 - val_loss: 1.2418\n",
            "Epoch 585/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9095 - loss: 0.3012 - val_accuracy: 0.6458 - val_loss: 1.2194\n",
            "Epoch 586/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8909 - loss: 0.2702 - val_accuracy: 0.6632 - val_loss: 1.2472\n",
            "Epoch 587/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8584 - loss: 0.3727 - val_accuracy: 0.6424 - val_loss: 1.2132\n",
            "Epoch 588/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8670 - loss: 0.3347 - val_accuracy: 0.6597 - val_loss: 1.2549\n",
            "Epoch 589/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8820 - loss: 0.3367 - val_accuracy: 0.6319 - val_loss: 1.2421\n",
            "Epoch 590/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8982 - loss: 0.3191 - val_accuracy: 0.6424 - val_loss: 1.2509\n",
            "Epoch 591/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9055 - loss: 0.2654 - val_accuracy: 0.6389 - val_loss: 1.2440\n",
            "Epoch 592/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9232 - loss: 0.2428 - val_accuracy: 0.6389 - val_loss: 1.2027\n",
            "Epoch 593/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8934 - loss: 0.3116 - val_accuracy: 0.6667 - val_loss: 1.1641\n",
            "Epoch 594/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8817 - loss: 0.3379 - val_accuracy: 0.6493 - val_loss: 1.2531\n",
            "Epoch 595/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8839 - loss: 0.3085 - val_accuracy: 0.6736 - val_loss: 1.2331\n",
            "Epoch 596/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9017 - loss: 0.2792 - val_accuracy: 0.6319 - val_loss: 1.2233\n",
            "Epoch 597/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9183 - loss: 0.2637 - val_accuracy: 0.6458 - val_loss: 1.1776\n",
            "Epoch 598/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8944 - loss: 0.3284 - val_accuracy: 0.6319 - val_loss: 1.2852\n",
            "Epoch 599/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9157 - loss: 0.2702 - val_accuracy: 0.6250 - val_loss: 1.2797\n",
            "Epoch 600/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9315 - loss: 0.2366 - val_accuracy: 0.6632 - val_loss: 1.2795\n",
            "Epoch 601/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9037 - loss: 0.3304 - val_accuracy: 0.6528 - val_loss: 1.2973\n",
            "Epoch 602/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8809 - loss: 0.3345 - val_accuracy: 0.6528 - val_loss: 1.2256\n",
            "Epoch 603/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9100 - loss: 0.2394 - val_accuracy: 0.6632 - val_loss: 1.2484\n",
            "Epoch 604/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9022 - loss: 0.2669 - val_accuracy: 0.6493 - val_loss: 1.2630\n",
            "Epoch 605/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8825 - loss: 0.3347 - val_accuracy: 0.6736 - val_loss: 1.2904\n",
            "Epoch 606/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8812 - loss: 0.3405 - val_accuracy: 0.6493 - val_loss: 1.2235\n",
            "Epoch 607/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8930 - loss: 0.2999 - val_accuracy: 0.6528 - val_loss: 1.2205\n",
            "Epoch 608/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9003 - loss: 0.3063 - val_accuracy: 0.6528 - val_loss: 1.2770\n",
            "Epoch 609/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8866 - loss: 0.3262 - val_accuracy: 0.6562 - val_loss: 1.2638\n",
            "Epoch 610/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8860 - loss: 0.3080 - val_accuracy: 0.6354 - val_loss: 1.3262\n",
            "Epoch 611/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8910 - loss: 0.3909 - val_accuracy: 0.6806 - val_loss: 1.2363\n",
            "Epoch 612/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9009 - loss: 0.3193 - val_accuracy: 0.6771 - val_loss: 1.1730\n",
            "Epoch 613/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8930 - loss: 0.2799 - val_accuracy: 0.6528 - val_loss: 1.1813\n",
            "Epoch 614/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9010 - loss: 0.3043 - val_accuracy: 0.6597 - val_loss: 1.2465\n",
            "Epoch 615/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9045 - loss: 0.2793 - val_accuracy: 0.6458 - val_loss: 1.2658\n",
            "Epoch 616/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8908 - loss: 0.2978 - val_accuracy: 0.6493 - val_loss: 1.2741\n",
            "Epoch 617/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8863 - loss: 0.3539 - val_accuracy: 0.6528 - val_loss: 1.3072\n",
            "Epoch 618/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9082 - loss: 0.2617 - val_accuracy: 0.6667 - val_loss: 1.2914\n",
            "Epoch 619/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8924 - loss: 0.3416 - val_accuracy: 0.6285 - val_loss: 1.3107\n",
            "Epoch 620/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.8947 - loss: 0.3357 - val_accuracy: 0.6181 - val_loss: 1.3371\n",
            "Epoch 621/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8854 - loss: 0.3282 - val_accuracy: 0.6181 - val_loss: 1.3218\n",
            "Epoch 622/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8929 - loss: 0.3041 - val_accuracy: 0.6424 - val_loss: 1.3212\n",
            "Epoch 623/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8538 - loss: 0.3569 - val_accuracy: 0.6562 - val_loss: 1.2565\n",
            "Epoch 624/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9025 - loss: 0.2462 - val_accuracy: 0.6424 - val_loss: 1.3263\n",
            "Epoch 625/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9217 - loss: 0.2409 - val_accuracy: 0.6424 - val_loss: 1.3187\n",
            "Epoch 626/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8949 - loss: 0.3102 - val_accuracy: 0.6562 - val_loss: 1.2786\n",
            "Epoch 627/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8894 - loss: 0.3253 - val_accuracy: 0.6528 - val_loss: 1.2937\n",
            "Epoch 628/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8975 - loss: 0.2838 - val_accuracy: 0.6389 - val_loss: 1.2962\n",
            "Epoch 629/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8940 - loss: 0.2720 - val_accuracy: 0.6528 - val_loss: 1.2634\n",
            "Epoch 630/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9185 - loss: 0.2685 - val_accuracy: 0.6597 - val_loss: 1.2176\n",
            "Epoch 631/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8943 - loss: 0.3312 - val_accuracy: 0.6389 - val_loss: 1.2717\n",
            "Epoch 632/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8883 - loss: 0.3362 - val_accuracy: 0.6667 - val_loss: 1.2954\n",
            "Epoch 633/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8871 - loss: 0.3479 - val_accuracy: 0.6389 - val_loss: 1.3224\n",
            "Epoch 634/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8944 - loss: 0.3025 - val_accuracy: 0.6146 - val_loss: 1.2741\n",
            "Epoch 635/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9035 - loss: 0.3211 - val_accuracy: 0.6424 - val_loss: 1.2482\n",
            "Epoch 636/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9288 - loss: 0.2299 - val_accuracy: 0.6319 - val_loss: 1.3093\n",
            "Epoch 637/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8755 - loss: 0.3374 - val_accuracy: 0.6562 - val_loss: 1.2944\n",
            "Epoch 638/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9214 - loss: 0.2218 - val_accuracy: 0.6458 - val_loss: 1.3133\n",
            "Epoch 639/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9237 - loss: 0.2683 - val_accuracy: 0.6458 - val_loss: 1.2850\n",
            "Epoch 640/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9288 - loss: 0.2367 - val_accuracy: 0.6632 - val_loss: 1.2273\n",
            "Epoch 641/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9132 - loss: 0.2441 - val_accuracy: 0.6632 - val_loss: 1.2567\n",
            "Epoch 642/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8837 - loss: 0.3597 - val_accuracy: 0.6632 - val_loss: 1.2820\n",
            "Epoch 643/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8982 - loss: 0.2913 - val_accuracy: 0.6493 - val_loss: 1.2491\n",
            "Epoch 644/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8968 - loss: 0.2957 - val_accuracy: 0.6319 - val_loss: 1.3156\n",
            "Epoch 645/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8975 - loss: 0.3273 - val_accuracy: 0.6667 - val_loss: 1.2309\n",
            "Epoch 646/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8878 - loss: 0.2786 - val_accuracy: 0.6389 - val_loss: 1.1754\n",
            "Epoch 647/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8911 - loss: 0.3171 - val_accuracy: 0.6285 - val_loss: 1.2207\n",
            "Epoch 648/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8888 - loss: 0.2908 - val_accuracy: 0.6528 - val_loss: 1.2122\n",
            "Epoch 649/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9074 - loss: 0.2665 - val_accuracy: 0.6562 - val_loss: 1.1635\n",
            "Epoch 650/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9201 - loss: 0.2887 - val_accuracy: 0.6562 - val_loss: 1.1451\n",
            "Epoch 651/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8983 - loss: 0.2763 - val_accuracy: 0.6701 - val_loss: 1.1871\n",
            "Epoch 652/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8971 - loss: 0.3014 - val_accuracy: 0.6424 - val_loss: 1.2118\n",
            "Epoch 653/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8924 - loss: 0.3028 - val_accuracy: 0.6701 - val_loss: 1.2043\n",
            "Epoch 654/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8974 - loss: 0.3163 - val_accuracy: 0.6424 - val_loss: 1.2082\n",
            "Epoch 655/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9216 - loss: 0.2691 - val_accuracy: 0.6562 - val_loss: 1.2369\n",
            "Epoch 656/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9136 - loss: 0.2521 - val_accuracy: 0.6701 - val_loss: 1.2008\n",
            "Epoch 657/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9023 - loss: 0.3308 - val_accuracy: 0.6632 - val_loss: 1.1957\n",
            "Epoch 658/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9201 - loss: 0.2280 - val_accuracy: 0.6875 - val_loss: 1.2051\n",
            "Epoch 659/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9004 - loss: 0.3142 - val_accuracy: 0.6667 - val_loss: 1.2034\n",
            "Epoch 660/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8856 - loss: 0.2803 - val_accuracy: 0.6667 - val_loss: 1.1500\n",
            "Epoch 661/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9183 - loss: 0.2386 - val_accuracy: 0.6736 - val_loss: 1.1526\n",
            "Epoch 662/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8967 - loss: 0.3185 - val_accuracy: 0.6528 - val_loss: 1.1925\n",
            "Epoch 663/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9046 - loss: 0.2907 - val_accuracy: 0.6528 - val_loss: 1.2170\n",
            "Epoch 664/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9111 - loss: 0.2729 - val_accuracy: 0.6632 - val_loss: 1.2453\n",
            "Epoch 665/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8951 - loss: 0.2872 - val_accuracy: 0.6597 - val_loss: 1.2494\n",
            "Epoch 666/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8755 - loss: 0.3316 - val_accuracy: 0.6806 - val_loss: 1.1888\n",
            "Epoch 667/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8973 - loss: 0.3021 - val_accuracy: 0.6667 - val_loss: 1.2267\n",
            "Epoch 668/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8902 - loss: 0.3100 - val_accuracy: 0.6736 - val_loss: 1.2446\n",
            "Epoch 669/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8837 - loss: 0.3404 - val_accuracy: 0.6632 - val_loss: 1.2846\n",
            "Epoch 670/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9198 - loss: 0.2545 - val_accuracy: 0.6667 - val_loss: 1.2972\n",
            "Epoch 671/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8713 - loss: 0.3235 - val_accuracy: 0.6424 - val_loss: 1.2653\n",
            "Epoch 672/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8985 - loss: 0.3102 - val_accuracy: 0.6389 - val_loss: 1.2471\n",
            "Epoch 673/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9147 - loss: 0.2414 - val_accuracy: 0.6562 - val_loss: 1.2859\n",
            "Epoch 674/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9106 - loss: 0.2622 - val_accuracy: 0.6562 - val_loss: 1.2549\n",
            "Epoch 675/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9288 - loss: 0.2465 - val_accuracy: 0.6424 - val_loss: 1.2625\n",
            "Epoch 676/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9234 - loss: 0.2481 - val_accuracy: 0.6493 - val_loss: 1.2404\n",
            "Epoch 677/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9019 - loss: 0.2773 - val_accuracy: 0.6528 - val_loss: 1.3078\n",
            "Epoch 678/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8964 - loss: 0.2838 - val_accuracy: 0.6562 - val_loss: 1.2930\n",
            "Epoch 679/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9125 - loss: 0.2580 - val_accuracy: 0.6562 - val_loss: 1.3015\n",
            "Epoch 680/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9136 - loss: 0.2512 - val_accuracy: 0.6528 - val_loss: 1.3341\n",
            "Epoch 681/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8777 - loss: 0.3909 - val_accuracy: 0.6562 - val_loss: 1.2692\n",
            "Epoch 682/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8931 - loss: 0.3359 - val_accuracy: 0.6771 - val_loss: 1.1787\n",
            "Epoch 683/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9204 - loss: 0.2273 - val_accuracy: 0.6840 - val_loss: 1.1913\n",
            "Epoch 684/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9111 - loss: 0.2572 - val_accuracy: 0.6424 - val_loss: 1.2384\n",
            "Epoch 685/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9183 - loss: 0.2751 - val_accuracy: 0.6806 - val_loss: 1.1584\n",
            "Epoch 686/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9200 - loss: 0.2483 - val_accuracy: 0.6701 - val_loss: 1.1874\n",
            "Epoch 687/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9114 - loss: 0.2942 - val_accuracy: 0.6597 - val_loss: 1.2647\n",
            "Epoch 688/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9059 - loss: 0.2935 - val_accuracy: 0.6597 - val_loss: 1.3386\n",
            "Epoch 689/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9258 - loss: 0.2381 - val_accuracy: 0.6701 - val_loss: 1.2763\n",
            "Epoch 690/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.8999 - loss: 0.3250 - val_accuracy: 0.6736 - val_loss: 1.2527\n",
            "Epoch 691/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8917 - loss: 0.3237 - val_accuracy: 0.6771 - val_loss: 1.2574\n",
            "Epoch 692/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8758 - loss: 0.3361 - val_accuracy: 0.6701 - val_loss: 1.2671\n",
            "Epoch 693/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8875 - loss: 0.3434 - val_accuracy: 0.6632 - val_loss: 1.2746\n",
            "Epoch 694/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9269 - loss: 0.2567 - val_accuracy: 0.6389 - val_loss: 1.3029\n",
            "Epoch 695/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8953 - loss: 0.3324 - val_accuracy: 0.6562 - val_loss: 1.2264\n",
            "Epoch 696/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9054 - loss: 0.2911 - val_accuracy: 0.6806 - val_loss: 1.2233\n",
            "Epoch 697/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8910 - loss: 0.3244 - val_accuracy: 0.6632 - val_loss: 1.2359\n",
            "Epoch 698/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9178 - loss: 0.2600 - val_accuracy: 0.6493 - val_loss: 1.2143\n",
            "Epoch 699/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9174 - loss: 0.2682 - val_accuracy: 0.6701 - val_loss: 1.1822\n",
            "Epoch 700/700\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9133 - loss: 0.2577 - val_accuracy: 0.6736 - val_loss: 1.1879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7th"
      ],
      "metadata": {
        "id": "WTtNM5E2Kxm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the HyperModel class:1\n",
        "class MLPHyperModel(HyperModel):\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "\n",
        "        # Number of hidden layers\n",
        "        for i in range(hp.Int('num_layers', 1, 6)):\n",
        "            model.add(Dense(\n",
        "                units=hp.Int(f'units_{i}', min_value=130, max_value=780, step=13),\n",
        "                activation=hp.Choice(f'activation_{i}', ['relu', 'elu','tanh','softmax']),\n",
        "                input_dim=13 if i == 0 else None\n",
        "            ))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dropout(rate=hp.Float(f'dropout_{i}', 0.1, 0.9, step=0.1)))\n",
        "\n",
        "        # Output layer\n",
        "        model.add(Dense(8, activation='softmax'))\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-1, sampling='log')),\n",
        "            loss=hp.Choice('loss', ['sparse_categorical_crossentropy', 'categorical_crossentropy']),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model\n"
      ],
      "metadata": {
        "id": "r0NKtR8uK0io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tuner\n",
        "tuner = RandomSearch(\n",
        "    MLPHyperModel(),\n",
        "    objective='val_accuracy',\n",
        "    max_trials=3,\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='mlp_tuning'\n",
        ")"
      ],
      "metadata": {
        "id": "lSwefHOJK0ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(x_train, y_train_sparse, epochs=100,batch_size=32, validation_data=(x_test, y_test_sparse))\n",
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtnOTQ8CK0ON",
        "outputId": "94a4306d-c60e-4e7c-a6e8-66a40249d818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3 Complete [00h 00m 01s]\n",
            "\n",
            "Best val_accuracy So Far: 0.5972222089767456\n",
            "Total elapsed time: 00h 00m 58s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the best hyperparameters\n",
        "best_hyperparameters = tuner.get_best_hyperparameters()[0]"
      ],
      "metadata": {
        "id": "AQOE2vO5K0K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "for hp_name in best_hyperparameters.values.keys():\n",
        "    print(f\"{hp_name}: {best_hyperparameters.get(hp_name)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJdh4NiqK0IF",
        "outputId": "2dab3fd6-c5a3-4bbe-dc42-db705869bec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "num_layers: 2\n",
            "units_0: 741\n",
            "activation_0: tanh\n",
            "dropout_0: 0.2\n",
            "learning_rate: 0.09458731722205639\n",
            "loss: sparse_categorical_crossentropy\n",
            "units_1: 130\n",
            "activation_1: relu\n",
            "dropout_1: 0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8th"
      ],
      "metadata": {
        "id": "Z1Tq2VxfMJrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPHyperModel12(HyperModel):\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "\n",
        "        # Number of hidden layers\n",
        "        for i in range(hp.Int('num_layers', 1,9)):\n",
        "            model.add(Dense(\n",
        "                units=hp.Int(f'units_{i}', min_value=130, max_value=1170, step=13),\n",
        "                activation=hp.Choice(f'activation_{i}', ['relu', 'elu','tanh','softmax']),\n",
        "                input_dim=13 if i == 0 else None\n",
        "            ))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dropout(rate=hp.Float(f'dropout_{i}', 0.1, 0.5, step=0.1)))\n",
        "\n",
        "        # Output layer\n",
        "        model.add(Dense(8, activation='softmax'))\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-1, sampling='log')),\n",
        "            loss=hp.Choice('loss', ['sparse_categorical_crossentropy', 'categorical_crossentropy']),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model"
      ],
      "metadata": {
        "id": "4NBysY-5MM7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tuner\n",
        "tuner = RandomSearch(\n",
        "    MLPHyperModel(),\n",
        "    objective='val_accuracy',\n",
        "    max_trials=3,\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='mlp_tuning'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGCpbMOlMM3w",
        "outputId": "8764178d-de31-47bd-908e-37c87c98796d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading Tuner from my_dir/mlp_tuning/tuner0.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(x_train, y_train_sparse, epochs=100,batch_size=32, validation_data=(x_test, y_test_sparse))\n",
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ],
      "metadata": {
        "id": "Ws_Uz_GgMMsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8-xdz4Y9MMpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y8-jNIAFMMmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN"
      ],
      "metadata": {
        "id": "sHB_OgIx6XvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "# Simple RNN Layer\n",
        "model.add(SimpleRNN(64, input_shape=(13, 1), return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Another Simple RNN Layer\n",
        "model.add(SimpleRNN(64))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Fully connected (Dense) layer\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(8, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "B56qro926ZaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(x_train, y_train_sparse, batch_size=50, epochs=100, validation_data=(x_test, y_test_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaMJcVDz6aPN",
        "outputId": "a1c4bd5a-131b-45eb-9add-0bba5b5981c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 304ms/step - accuracy: 0.1323 - loss: 2.9759 - val_accuracy: 0.2812 - val_loss: 1.9685\n",
            "Epoch 2/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.1984 - loss: 2.5575 - val_accuracy: 0.2986 - val_loss: 1.8695\n",
            "Epoch 3/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2245 - loss: 2.3824 - val_accuracy: 0.3438 - val_loss: 1.7881\n",
            "Epoch 4/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2493 - loss: 2.2464 - val_accuracy: 0.3750 - val_loss: 1.7273\n",
            "Epoch 5/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2284 - loss: 2.2397 - val_accuracy: 0.4062 - val_loss: 1.6878\n",
            "Epoch 6/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2761 - loss: 2.1383 - val_accuracy: 0.4167 - val_loss: 1.6559\n",
            "Epoch 7/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2370 - loss: 2.1390 - val_accuracy: 0.4375 - val_loss: 1.5878\n",
            "Epoch 8/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2911 - loss: 1.9809 - val_accuracy: 0.4549 - val_loss: 1.5852\n",
            "Epoch 9/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2582 - loss: 2.0479 - val_accuracy: 0.4375 - val_loss: 1.5634\n",
            "Epoch 10/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2959 - loss: 1.9698 - val_accuracy: 0.4688 - val_loss: 1.5395\n",
            "Epoch 11/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3157 - loss: 1.9344 - val_accuracy: 0.4410 - val_loss: 1.5839\n",
            "Epoch 12/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3187 - loss: 1.9297 - val_accuracy: 0.3611 - val_loss: 1.6402\n",
            "Epoch 13/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3027 - loss: 1.9482 - val_accuracy: 0.4444 - val_loss: 1.5452\n",
            "Epoch 14/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2960 - loss: 1.9291 - val_accuracy: 0.3993 - val_loss: 1.5771\n",
            "Epoch 15/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3148 - loss: 1.8631 - val_accuracy: 0.4097 - val_loss: 1.6088\n",
            "Epoch 16/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3034 - loss: 1.9547 - val_accuracy: 0.3438 - val_loss: 1.6828\n",
            "Epoch 17/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2798 - loss: 1.9510 - val_accuracy: 0.3993 - val_loss: 1.6036\n",
            "Epoch 18/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2656 - loss: 1.9945 - val_accuracy: 0.3438 - val_loss: 1.7803\n",
            "Epoch 19/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2553 - loss: 2.0752 - val_accuracy: 0.4028 - val_loss: 1.6578\n",
            "Epoch 20/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2637 - loss: 1.9417 - val_accuracy: 0.4132 - val_loss: 1.6272\n",
            "Epoch 21/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.2654 - loss: 1.9244 - val_accuracy: 0.3819 - val_loss: 1.5937\n",
            "Epoch 22/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2983 - loss: 1.8788 - val_accuracy: 0.3958 - val_loss: 1.6557\n",
            "Epoch 23/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2902 - loss: 1.9289 - val_accuracy: 0.4236 - val_loss: 1.6336\n",
            "Epoch 24/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2564 - loss: 1.9633 - val_accuracy: 0.3194 - val_loss: 1.8338\n",
            "Epoch 25/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2353 - loss: 2.0962 - val_accuracy: 0.3299 - val_loss: 1.7873\n",
            "Epoch 26/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2449 - loss: 1.9263 - val_accuracy: 0.3646 - val_loss: 1.7338\n",
            "Epoch 27/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.2285 - loss: 1.9894 - val_accuracy: 0.2917 - val_loss: 1.7955\n",
            "Epoch 28/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2599 - loss: 1.9221 - val_accuracy: 0.3368 - val_loss: 1.6925\n",
            "Epoch 29/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2781 - loss: 1.8430 - val_accuracy: 0.3854 - val_loss: 1.6558\n",
            "Epoch 30/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3136 - loss: 1.8576 - val_accuracy: 0.3993 - val_loss: 1.6066\n",
            "Epoch 31/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3303 - loss: 1.7990 - val_accuracy: 0.3750 - val_loss: 1.6575\n",
            "Epoch 32/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3286 - loss: 1.7763 - val_accuracy: 0.4201 - val_loss: 1.6203\n",
            "Epoch 33/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3168 - loss: 1.8347 - val_accuracy: 0.4167 - val_loss: 1.5913\n",
            "Epoch 34/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3200 - loss: 1.7721 - val_accuracy: 0.4097 - val_loss: 1.6062\n",
            "Epoch 35/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3183 - loss: 1.7577 - val_accuracy: 0.4444 - val_loss: 1.5824\n",
            "Epoch 36/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3441 - loss: 1.7599 - val_accuracy: 0.4236 - val_loss: 1.5607\n",
            "Epoch 37/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3327 - loss: 1.7139 - val_accuracy: 0.4444 - val_loss: 1.5897\n",
            "Epoch 38/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.3463 - loss: 1.7258 - val_accuracy: 0.4340 - val_loss: 1.5952\n",
            "Epoch 39/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.3355 - loss: 1.7139 - val_accuracy: 0.4410 - val_loss: 1.5635\n",
            "Epoch 40/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.3565 - loss: 1.7340 - val_accuracy: 0.4410 - val_loss: 1.5640\n",
            "Epoch 41/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.3459 - loss: 1.7478 - val_accuracy: 0.4236 - val_loss: 1.5581\n",
            "Epoch 42/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.3600 - loss: 1.6712 - val_accuracy: 0.4167 - val_loss: 1.5378\n",
            "Epoch 43/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3611 - loss: 1.6665 - val_accuracy: 0.4410 - val_loss: 1.5478\n",
            "Epoch 44/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3705 - loss: 1.6594 - val_accuracy: 0.4201 - val_loss: 1.5857\n",
            "Epoch 45/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3470 - loss: 1.7029 - val_accuracy: 0.4583 - val_loss: 1.5442\n",
            "Epoch 46/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3575 - loss: 1.6786 - val_accuracy: 0.4375 - val_loss: 1.5282\n",
            "Epoch 47/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3872 - loss: 1.6379 - val_accuracy: 0.4375 - val_loss: 1.5334\n",
            "Epoch 48/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3798 - loss: 1.6177 - val_accuracy: 0.4201 - val_loss: 1.5215\n",
            "Epoch 49/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3571 - loss: 1.7139 - val_accuracy: 0.4062 - val_loss: 1.5403\n",
            "Epoch 50/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3424 - loss: 1.7476 - val_accuracy: 0.3819 - val_loss: 1.6561\n",
            "Epoch 51/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3779 - loss: 1.6674 - val_accuracy: 0.4271 - val_loss: 1.5598\n",
            "Epoch 52/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3420 - loss: 1.6932 - val_accuracy: 0.4410 - val_loss: 1.5263\n",
            "Epoch 53/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3793 - loss: 1.6302 - val_accuracy: 0.4375 - val_loss: 1.5193\n",
            "Epoch 54/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3697 - loss: 1.5924 - val_accuracy: 0.4340 - val_loss: 1.5144\n",
            "Epoch 55/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3666 - loss: 1.6309 - val_accuracy: 0.4479 - val_loss: 1.5025\n",
            "Epoch 56/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3763 - loss: 1.6511 - val_accuracy: 0.4097 - val_loss: 1.5620\n",
            "Epoch 57/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3460 - loss: 1.7102 - val_accuracy: 0.3924 - val_loss: 1.6536\n",
            "Epoch 58/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3457 - loss: 1.6816 - val_accuracy: 0.4271 - val_loss: 1.5344\n",
            "Epoch 59/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4134 - loss: 1.6017 - val_accuracy: 0.4201 - val_loss: 1.5248\n",
            "Epoch 60/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3829 - loss: 1.6495 - val_accuracy: 0.3819 - val_loss: 1.7450\n",
            "Epoch 61/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3598 - loss: 1.6690 - val_accuracy: 0.3889 - val_loss: 1.5648\n",
            "Epoch 62/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3675 - loss: 1.6501 - val_accuracy: 0.4201 - val_loss: 1.5424\n",
            "Epoch 63/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3559 - loss: 1.6640 - val_accuracy: 0.3542 - val_loss: 1.8235\n",
            "Epoch 64/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3290 - loss: 1.6962 - val_accuracy: 0.3819 - val_loss: 1.6946\n",
            "Epoch 65/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3726 - loss: 1.6590 - val_accuracy: 0.4236 - val_loss: 1.5890\n",
            "Epoch 66/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3753 - loss: 1.6346 - val_accuracy: 0.4479 - val_loss: 1.5715\n",
            "Epoch 67/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3780 - loss: 1.6402 - val_accuracy: 0.4410 - val_loss: 1.5517\n",
            "Epoch 68/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3746 - loss: 1.6155 - val_accuracy: 0.4444 - val_loss: 1.5554\n",
            "Epoch 69/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3436 - loss: 1.6505 - val_accuracy: 0.4479 - val_loss: 1.5472\n",
            "Epoch 70/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3936 - loss: 1.5972 - val_accuracy: 0.4410 - val_loss: 1.5318\n",
            "Epoch 71/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3507 - loss: 1.6572 - val_accuracy: 0.4306 - val_loss: 1.6102\n",
            "Epoch 72/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.3817 - loss: 1.5981 - val_accuracy: 0.4306 - val_loss: 1.5963\n",
            "Epoch 73/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3878 - loss: 1.5921 - val_accuracy: 0.4340 - val_loss: 1.5398\n",
            "Epoch 74/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4122 - loss: 1.5825 - val_accuracy: 0.4306 - val_loss: 1.5461\n",
            "Epoch 75/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3604 - loss: 1.5817 - val_accuracy: 0.4201 - val_loss: 1.5326\n",
            "Epoch 76/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3804 - loss: 1.5724 - val_accuracy: 0.4444 - val_loss: 1.5146\n",
            "Epoch 77/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4089 - loss: 1.5592 - val_accuracy: 0.4549 - val_loss: 1.5157\n",
            "Epoch 78/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4325 - loss: 1.5104 - val_accuracy: 0.4757 - val_loss: 1.4835\n",
            "Epoch 79/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4347 - loss: 1.5133 - val_accuracy: 0.4583 - val_loss: 1.4889\n",
            "Epoch 80/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4240 - loss: 1.5217 - val_accuracy: 0.4861 - val_loss: 1.4819\n",
            "Epoch 81/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4361 - loss: 1.5277 - val_accuracy: 0.4896 - val_loss: 1.4700\n",
            "Epoch 82/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4097 - loss: 1.5681 - val_accuracy: 0.4271 - val_loss: 1.5574\n",
            "Epoch 83/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4196 - loss: 1.5416 - val_accuracy: 0.4549 - val_loss: 1.4952\n",
            "Epoch 84/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3995 - loss: 1.5279 - val_accuracy: 0.4792 - val_loss: 1.4636\n",
            "Epoch 85/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4512 - loss: 1.4969 - val_accuracy: 0.4549 - val_loss: 1.4692\n",
            "Epoch 86/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4378 - loss: 1.5509 - val_accuracy: 0.3958 - val_loss: 1.5858\n",
            "Epoch 87/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4286 - loss: 1.5367 - val_accuracy: 0.4514 - val_loss: 1.4886\n",
            "Epoch 88/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4341 - loss: 1.5377 - val_accuracy: 0.4375 - val_loss: 1.4600\n",
            "Epoch 89/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4416 - loss: 1.4887 - val_accuracy: 0.4375 - val_loss: 1.4693\n",
            "Epoch 90/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4505 - loss: 1.5102 - val_accuracy: 0.4410 - val_loss: 1.4562\n",
            "Epoch 91/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3946 - loss: 1.5663 - val_accuracy: 0.4201 - val_loss: 1.4990\n",
            "Epoch 92/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4294 - loss: 1.5220 - val_accuracy: 0.4722 - val_loss: 1.4316\n",
            "Epoch 93/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4578 - loss: 1.4864 - val_accuracy: 0.4618 - val_loss: 1.4232\n",
            "Epoch 94/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4511 - loss: 1.4737 - val_accuracy: 0.4410 - val_loss: 1.4369\n",
            "Epoch 95/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4720 - loss: 1.4351 - val_accuracy: 0.4340 - val_loss: 1.4453\n",
            "Epoch 96/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4381 - loss: 1.5088 - val_accuracy: 0.4444 - val_loss: 1.4353\n",
            "Epoch 97/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4732 - loss: 1.4306 - val_accuracy: 0.4306 - val_loss: 1.4389\n",
            "Epoch 98/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4400 - loss: 1.4665 - val_accuracy: 0.4271 - val_loss: 1.5334\n",
            "Epoch 99/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4243 - loss: 1.5039 - val_accuracy: 0.4201 - val_loss: 1.4607\n",
            "Epoch 100/100\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4389 - loss: 1.5066 - val_accuracy: 0.4201 - val_loss: 1.4522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vQOQT6o36ekl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}